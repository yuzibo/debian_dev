The patch was copied from:

https://github.com/petrpavlu/valgrind-riscv64/commit/2c12bec1ff61cb8ebf2cbeaeafab8ca437a83d8a#diff-aad5bbbffb980be6ce10e65c156b9e7d87580fec4548229f0dfcd01ea830ab98


--- a/configure.ac
+++ b/configure.ac
@@ -286,6 +286,17 @@
         AC_MSG_RESULT([ok (${host_cpu})])
         ARCH_MAX="mips64"
         ;;
+
+     aarch64*)
+       AC_MSG_RESULT([ok (${host_cpu})])
+       ARCH_MAX="arm64"
+       ;;
+
+     riscv64*)
+       AC_MSG_RESULT([ok (${host_cpu})])
+       ARCH_MAX="riscv64"
+       ;;
+
      nanomips)
         AC_MSG_RESULT([ok (${host_cpu})])
         ARCH_MAX="nanomips"
@@ -932,6 +943,17 @@
         fi
         AC_MSG_RESULT([ok (${ARCH_MAX}-${VGCONF_OS})])
         ;;
+     riscv64-linux)
+        VGCONF_ARCH_PRI="riscv64"
+        VGCONF_ARCH_SEC=""
+        VGCONF_PLATFORM_PRI_CAPS="RISCV64_LINUX"
+        VGCONF_PLATFORM_SEC_CAPS=""
+        valt_load_address_pri_norml="0x58000000"
+        valt_load_address_pri_inner="0x38000000"
+        valt_load_address_sec_norml="0xUNSET"
+        valt_load_address_sec_inner="0xUNSET"
+        AC_MSG_RESULT([ok (${ARCH_MAX}-${VGCONF_OS})])
+        ;;
     *)
         VGCONF_ARCH_PRI="unknown"
         VGCONF_ARCH_SEC="unknown"
@@ -984,6 +1006,8 @@
                test x$VGCONF_PLATFORM_PRI_CAPS = xMIPS64_LINUX ) 
 AM_CONDITIONAL(VGCONF_ARCHS_INCLUDE_NANOMIPS,
                test x$VGCONF_PLATFORM_PRI_CAPS = xNANOMIPS_LINUX )
+AM_CONDITIONAL(VGCONF_ARCHS_INCLUDE_RISCV64,
+               test x$VGCONF_PLATFORM_PRI_CAPS = xRISCV64_LINUX )
 
 # Set up VGCONF_PLATFORMS_INCLUDE_<platform>.  Either one or two of these
 # become defined.
@@ -1029,6 +1053,8 @@
                  -o x$VGCONF_PLATFORM_SEC_CAPS = xX86_SOLARIS)
 AM_CONDITIONAL(VGCONF_PLATFORMS_INCLUDE_AMD64_SOLARIS,
                test x$VGCONF_PLATFORM_PRI_CAPS = xAMD64_SOLARIS)
+AM_CONDITIONAL(VGCONF_PLATFORMS_INCLUDE_RISCV64_LINUX,
+               test x$VGCONF_PLATFORM_PRI_CAPS = xRISCV64_LINUX)
 
 
 # Similarly, set up VGCONF_OS_IS_<os>.  Exactly one of these becomes defined.
@@ -1045,6 +1071,7 @@
                  -o x$VGCONF_PLATFORM_PRI_CAPS = xS390X_LINUX \
                  -o x$VGCONF_PLATFORM_PRI_CAPS = xMIPS32_LINUX \
                  -o x$VGCONF_PLATFORM_PRI_CAPS = xMIPS64_LINUX \
+                 -o x$VGCONF_PLATFORM_PRI_CAPS = xRISCV64_LINUX) \
                  -o x$VGCONF_PLATFORM_PRI_CAPS = xNANOMIPS_LINUX)
 AM_CONDITIONAL(VGCONF_OS_IS_FREEBSD,
                test x$VGCONF_PLATFORM_PRI_CAPS = xX86_FREEBSD \
@@ -4672,7 +4699,8 @@
        -o x$VGCONF_PLATFORM_PRI_CAPS = xPPC64_LINUX \
        -o x$VGCONF_PLATFORM_PRI_CAPS = xARM64_LINUX \
        -o x$VGCONF_PLATFORM_PRI_CAPS = xMIPS64_LINUX \
-       -o x$VGCONF_PLATFORM_PRI_CAPS = xS390X_LINUX ; then
+       -o x$VGCONF_PLATFORM_PRI_CAPS = xS390X_LINUX \
+       -o x$VGCONF_PLATFORM_PRI_CAPS = xRISCV64_LINUX ; then
   mflag_primary=$FLAG_M64
 elif test x$VGCONF_PLATFORM_PRI_CAPS = xX86_DARWIN ; then
   mflag_primary="$FLAG_M32 -arch i386"
@@ -5145,6 +5173,7 @@
    memcheck/tests/amd64-linux/Makefile
    memcheck/tests/arm64-linux/Makefile
    memcheck/tests/x86-linux/Makefile
+   memcheck/tests/riscv64-linux/Makefile
    memcheck/tests/amd64-solaris/Makefile
    memcheck/tests/x86-solaris/Makefile
    memcheck/tests/amd64-freebsd/Makefile
@@ -5189,6 +5218,7 @@
    none/tests/mips32/Makefile
    none/tests/mips64/Makefile
    none/tests/nanomips/Makefile
+   none/tests/riscv64/Makefile
    none/tests/linux/Makefile
    none/tests/darwin/Makefile
    none/tests/solaris/Makefile
--- a/Makefile.all.am
+++ b/Makefile.all.am
@@ -290,6 +290,11 @@
 				$(AM_CFLAGS_PSO_BASE)
 AM_CCASFLAGS_MIPS64_LINUX  = @FLAG_M64@ -g
 
+AM_FLAG_M3264_RISCV64_LINUX = @FLAG_M64@
+AM_CFLAGS_RISCV64_LINUX     = @FLAG_M64@ $(AM_CFLAGS_BASE)
+AM_CFLAGS_PSO_RISCV64_LINUX = @FLAG_M64@ $(AM_CFLAGS_BASE) $(AM_CFLAGS_PSO_BASE)
+AM_CCASFLAGS_RISCV64_LINUX  = @FLAG_M64@ -g
+
 AM_FLAG_M3264_X86_SOLARIS   = @FLAG_M32@
 AM_CFLAGS_X86_SOLARIS       = @FLAG_M32@ @PREFERRED_STACK_BOUNDARY_2@ \
 				$(AM_CFLAGS_BASE) -fomit-frame-pointer @SOLARIS_UNDEF_LARGESOURCE@
@@ -350,6 +355,7 @@
 PRELOAD_LDFLAGS_MIPS32_LINUX   = $(PRELOAD_LDFLAGS_COMMON_LINUX) @FLAG_M32@
 PRELOAD_LDFLAGS_NANOMIPS_LINUX = $(PRELOAD_LDFLAGS_COMMON_LINUX) @FLAG_M32@
 PRELOAD_LDFLAGS_MIPS64_LINUX   = $(PRELOAD_LDFLAGS_COMMON_LINUX) @FLAG_M64@
+PRELOAD_LDFLAGS_RISCV64_LINUX = $(PRELOAD_LDFLAGS_COMMON_LINUX) @FLAG_M64@
 PRELOAD_LDFLAGS_X86_SOLARIS    = $(PRELOAD_LDFLAGS_COMMON_SOLARIS) @FLAG_M32@
 PRELOAD_LDFLAGS_AMD64_SOLARIS  = $(PRELOAD_LDFLAGS_COMMON_SOLARIS) @FLAG_M64@
 
--- a/Makefile.tool.am
+++ b/Makefile.tool.am
@@ -92,6 +92,9 @@
 	-static -nodefaultlibs -nostartfiles -u __start @FLAG_NO_BUILD_ID@ \
 	@FLAG_M64@
 
+TOOL_LDFLAGS_RISCV64_LINUX = \
+	$(TOOL_LDFLAGS_COMMON_LINUX) @FLAG_M64@
+
 TOOL_LDFLAGS_X86_SOLARIS = \
 	$(TOOL_LDFLAGS_COMMON_SOLARIS) @FLAG_M32@
 
@@ -160,6 +163,9 @@
 LIBREPLACEMALLOC_MIPS64_LINUX = \
 	$(top_builddir)/coregrind/libreplacemalloc_toolpreload-mips64-linux.a
 
+LIBREPLACEMALLOC_RISCV64_LINUX = \
+	$(top_builddir)/coregrind/libreplacemalloc_toolpreload-riscv64-linux.a
+
 LIBREPLACEMALLOC_X86_SOLARIS = \
 	$(top_builddir)/coregrind/libreplacemalloc_toolpreload-x86-solaris.a
 
@@ -232,6 +238,11 @@
 	$(LIBREPLACEMALLOC_MIPS64_LINUX) \
 	-Wl,--no-whole-archive
 
+LIBREPLACEMALLOC_LDFLAGS_RISCV64_LINUX = \
+	-Wl,--whole-archive \
+	$(LIBREPLACEMALLOC_RISCV64_LINUX) \
+	-Wl,--no-whole-archive
+
 LIBREPLACEMALLOC_LDFLAGS_X86_SOLARIS = \
 	-Wl,--whole-archive \
 	$(LIBREPLACEMALLOC_X86_SOLARIS) \
--- a/Makefile.vex.am
+++ b/Makefile.vex.am
@@ -26,6 +26,7 @@
 	pub/libvex_guest_s390x.h \
 	pub/libvex_guest_mips32.h \
 	pub/libvex_guest_mips64.h \
+        pub/libvex_guest_riscv64.h \
 	pub/libvex_s390x_common.h \
 	pub/libvex_ir.h \
 	pub/libvex_trc_values.h \
@@ -49,6 +50,7 @@
 	priv/guest_mips_defs.h \
 	priv/mips_defs.h \
 	priv/guest_nanomips_defs.h \
+	priv/guest_riscv64_defs.h \
 	priv/host_generic_regs.h \
 	priv/host_generic_simd64.h \
 	priv/host_generic_simd128.h \
@@ -64,7 +66,8 @@
 	priv/s390_defs.h \
 	priv/host_mips_defs.h \
 	priv/host_nanomips_defs.h \
-	priv/common_nanomips_defs.h
+	priv/common_nanomips_defs.h \
+	priv/host_riscv64_defs.h
 
 BUILT_SOURCES = pub/libvex_guest_offsets.h
 CLEANFILES    = pub/libvex_guest_offsets.h
@@ -93,7 +96,8 @@
 			    pub/libvex_guest_arm64.h \
 			    pub/libvex_guest_s390x.h \
 			    pub/libvex_guest_mips32.h \
-			    pub/libvex_guest_mips64.h
+			    pub/libvex_guest_mips64.h \
+		            pub/libvex_guest_riscv64.h
 	rm -f auxprogs/genoffsets.s
 	$(mkdir_p) auxprogs pub
 	$(CC) $(CFLAGS_FOR_GENOFFSETS) \
@@ -149,6 +153,8 @@
 	priv/guest_mips_helpers.c \
 	priv/guest_mipsdsp_toIR.c \
 	priv/guest_mips_toIR.c \
+	priv/guest_riscv64_helpers.c \
+	priv/guest_riscv64_toIR.c \
 	priv/guest_nanomips_helpers.c \
 	priv/guest_nanomips_toIR.c \
 	priv/host_generic_regs.c \
@@ -174,7 +180,9 @@
 	priv/host_mips_defs.c \
 	priv/host_nanomips_defs.c \
 	priv/host_mips_isel.c \
-	priv/host_nanomips_isel.c
+	priv/host_nanomips_isel.c \
+	priv/host_riscv64_defs.c \
+	priv/host_riscv64_isel.c
 
 LIBVEXMULTIARCH_SOURCES = priv/multiarch_main_main.c
 
--- a/VEX/auxprogs/genoffsets.c
+++ b/VEX/auxprogs/genoffsets.c
@@ -1,4 +1,4 @@
-
+u
 /*--------------------------------------------------------------------*/
 /*--- begin                                           genoffsets.c ---*/
 /*--------------------------------------------------------------------*/
@@ -53,6 +53,7 @@
 #include "../pub/libvex_guest_s390x.h"
 #include "../pub/libvex_guest_mips32.h"
 #include "../pub/libvex_guest_mips64.h"
+#include "../pub/libvex_guest_riscv64.h"
 
 #define VG_STRINGIFZ(__str)  #__str
 #define VG_STRINGIFY(__str)  VG_STRINGIFZ(__str)
@@ -262,6 +263,41 @@
    GENOFFSET(MIPS64,mips64,PC);
    GENOFFSET(MIPS64,mips64,HI);
    GENOFFSET(MIPS64,mips64,LO);
+
+   // riscv64
+   GENOFFSET(RISCV64,riscv64,x0);
+   GENOFFSET(RISCV64,riscv64,x1);
+   GENOFFSET(RISCV64,riscv64,x2);
+   GENOFFSET(RISCV64,riscv64,x3);
+   GENOFFSET(RISCV64,riscv64,x4);
+   GENOFFSET(RISCV64,riscv64,x5);
+   GENOFFSET(RISCV64,riscv64,x6);
+   GENOFFSET(RISCV64,riscv64,x7);
+   GENOFFSET(RISCV64,riscv64,x8);
+   GENOFFSET(RISCV64,riscv64,x9);
+   GENOFFSET(RISCV64,riscv64,x10);
+   GENOFFSET(RISCV64,riscv64,x11);
+   GENOFFSET(RISCV64,riscv64,x12);
+   GENOFFSET(RISCV64,riscv64,x13);
+   GENOFFSET(RISCV64,riscv64,x14);
+   GENOFFSET(RISCV64,riscv64,x15);
+   GENOFFSET(RISCV64,riscv64,x16);
+   GENOFFSET(RISCV64,riscv64,x17);
+   GENOFFSET(RISCV64,riscv64,x18);
+   GENOFFSET(RISCV64,riscv64,x19);
+   GENOFFSET(RISCV64,riscv64,x20);
+   GENOFFSET(RISCV64,riscv64,x21);
+   GENOFFSET(RISCV64,riscv64,x22);
+   GENOFFSET(RISCV64,riscv64,x23);
+   GENOFFSET(RISCV64,riscv64,x24);
+   GENOFFSET(RISCV64,riscv64,x25);
+   GENOFFSET(RISCV64,riscv64,x26);
+   GENOFFSET(RISCV64,riscv64,x27);
+   GENOFFSET(RISCV64,riscv64,x28);
+   GENOFFSET(RISCV64,riscv64,x29);
+   GENOFFSET(RISCV64,riscv64,x30);
+   GENOFFSET(RISCV64,riscv64,x31);
+   GENOFFSET(RISCV64,riscv64,pc);
 }
 
 /*--------------------------------------------------------------------*/
--- /dev/null
+++ b/VEX/priv/guest_riscv64_defs.h
@@ -0,0 +1,124 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                   guest_riscv64_defs.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+
+   Neither the names of the U.S. Department of Energy nor the
+   University of California nor the names of its contributors may be
+   used to endorse or promote products derived from this software
+   without prior written permission.
+*/
+
+/* Only to be used within the guest_riscv64_* files. */
+
+#ifndef __VEX_GUEST_RISCV64_DEFS_H
+#define __VEX_GUEST_RISCV64_DEFS_H
+
+#include "libvex_basictypes.h"
+
+#include "guest_generic_bb_to_IR.h"
+
+/*------------------------------------------------------------*/
+/*--- riscv64 to IR conversion                             ---*/
+/*------------------------------------------------------------*/
+
+/* Convert one riscv64 insn to IR. See the type DisOneInstrFn in
+   guest_generic_bb_to_IR.h. */
+DisResult disInstr_RISCV64(IRSB*              irbb,
+                           const UChar*       guest_code,
+                           Long               delta,
+                           Addr               guest_IP,
+                           VexArch            guest_arch,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo*  abiinfo,
+                           VexEndness         host_endness,
+                           Bool               sigill_diag);
+
+/* Used by the optimiser to specialise calls to helpers. */
+IRExpr* guest_riscv64_spechelper(const HChar* function_name,
+                                 IRExpr**     args,
+                                 IRStmt**     precedingStmts,
+                                 Int          n_precedingStmts);
+
+/* Describes to the optimiser which part of the guest state require precise
+   memory exceptions. This is logically part of the guest state description. */
+Bool guest_riscv64_state_requires_precise_mem_exns(
+   Int minoff, Int maxoff, VexRegisterUpdates pxControl);
+
+extern VexGuestLayout riscv64guest_layout;
+
+/*------------------------------------------------------------*/
+/*--- riscv64 guest helpers                                ---*/
+/*------------------------------------------------------------*/
+
+/* --- CLEAN HELPERS --- */
+
+/* Calculate resulting flags of a specified floating-point operation. Returns
+   a 32-bit value where bits 4:0 contain the fflags in the RISC-V native
+   format (NV DZ OF UF NX) and remaining upper bits are zero. */
+UInt riscv64g_calculate_fflags_fsqrt_s(Float a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fsqrt_d(Double a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_s_d(Double a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_w_d(Double a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_wu_d(Double a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_l_d(Double a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_lu_d(Double a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_d_l(ULong a1, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fcvt_d_lu(ULong a1, UInt rm_RISCV);
+
+UInt riscv64g_calculate_fflags_fadd_s(Float a1, Float a2, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fmul_s(Float a1, Float a2, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fdiv_s(Float a1, Float a2, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fadd_d(Double a1, Double a2, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fmul_d(Double a1, Double a2, UInt rm_RISCV);
+UInt riscv64g_calculate_fflags_fdiv_d(Double a1, Double a2, UInt rm_RISCV);
+
+UInt riscv64g_calculate_fflags_fmin_s(Float a1, Float a2);
+UInt riscv64g_calculate_fflags_fmax_s(Float a1, Float a2);
+UInt riscv64g_calculate_fflags_fmin_d(Double a1, Double a2);
+UInt riscv64g_calculate_fflags_fmax_d(Double a1, Double a2);
+UInt riscv64g_calculate_fflags_feq_d(Double a1, Double a2);
+UInt riscv64g_calculate_fflags_flt_d(Double a1, Double a2);
+UInt riscv64g_calculate_fflags_fle_d(Double a1, Double a2);
+
+UInt riscv64g_calculate_fflags_fmadd_s(Float a1,
+                                       Float a2,
+                                       Float a3,
+                                       UInt  rm_RISCV);
+UInt riscv64g_calculate_fflags_fmadd_d(Double a1,
+                                       Double a2,
+                                       Double a3,
+                                       UInt   rm_RISCV);
+
+/* Calculate floating-point class. Returns a 64-bit value where bits 9:0
+   contains the properties in the RISC-V FCLASS-instruction format and remaining
+   upper bits are zero. */
+ULong riscv64g_calculate_fclass_d(Double a1);
+
+#endif /* ndef __VEX_GUEST_RISCV64_DEFS_H */
+
+/*--------------------------------------------------------------------*/
+/*--- end                                     guest_riscv64_defs.h ---*/
+/*--------------------------------------------------------------------*/
--- /dev/null
+++ b/VEX/priv/guest_riscv64_helpers.c
@@ -0,0 +1,427 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                guest_riscv64_helpers.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "libvex_guest_riscv64.h"
+
+#include "guest_riscv64_defs.h"
+#include "main_util.h"
+
+/* This file contains helper functions for riscv64 guest code. Calls to these
+   functions are generated by the back end. These calls are of course in the
+   host machine code and this file will be compiled to host machine code, so
+   that all makes sense.
+
+   Only change the signatures of these helper functions very carefully. If you
+   change the signature here, you'll have to change the parameters passed to it
+   in the IR calls constructed by guest_riscv64_toIR.c.
+
+   The convention used is that all functions called from generated code are
+   named riscv64g_<something>, and any function whose name lacks that prefix is
+   not called from generated code. Note that some LibVEX_* functions can however
+   be called by VEX's client, but that is not the same as calling them from
+   VEX-generated code.
+*/
+
+#if defined(__riscv) && (__riscv_xlen == 64)
+/* clang-format off */
+#define CALCULATE_FFLAGS_UNARY64_F(inst)                                       \
+   do {                                                                        \
+      UInt res;                                                                \
+      __asm__ __volatile__(                                                    \
+         "csrr t0, fcsr\n\t"                                                   \
+         "csrw frm, %[rm]\n\t"                                                 \
+         "csrw fflags, zero\n\t"                                               \
+         inst " ft0, %[a1]\n\t"                                                \
+         "csrr %[res], fflags\n\t"                                             \
+         "csrw fcsr, t0\n\t"                                                   \
+         : [res] "=r"(res)                                                     \
+         : [a1] "f"(a1), [rm] "r"(rm_RISCV)                                    \
+         : "t0", "ft0");                                                       \
+      return res;                                                              \
+   } while (0)
+#define CALCULATE_FFLAGS_UNARY64_IF(inst)                                      \
+   do {                                                                        \
+      UInt res;                                                                \
+      __asm__ __volatile__(                                                    \
+         "csrr t0, fcsr\n\t"                                                   \
+         "csrw frm, %[rm]\n\t"                                                 \
+         "csrw fflags, zero\n\t"                                               \
+         inst " t1, %[a1]\n\t"                                                 \
+         "csrr %[res], fflags\n\t"                                             \
+         "csrw fcsr, t0\n\t"                                                   \
+         : [res] "=r"(res)                                                     \
+         : [a1] "f"(a1), [rm] "r"(rm_RISCV)                                    \
+         : "t0", "t1");                                                        \
+      return res;                                                              \
+   } while (0)
+#define CALCULATE_FFLAGS_UNARY64_FI(inst)                                      \
+   do {                                                                        \
+      UInt res;                                                                \
+      __asm__ __volatile__(                                                    \
+         "csrr t0, fcsr\n\t"                                                   \
+         "csrw frm, %[rm]\n\t"                                                 \
+         "csrw fflags, zero\n\t"                                               \
+         inst " ft0, %[a1]\n\t"                                                \
+         "csrr %[res], fflags\n\t"                                             \
+         "csrw fcsr, t0\n\t"                                                   \
+         : [res] "=r"(res)                                                     \
+         : [a1] "r"(a1), [rm] "r"(rm_RISCV)                                    \
+         : "t0", "ft0");                                                       \
+      return res;                                                              \
+   } while (0)
+/* clang-format on */
+#else
+/* No simulated version is currently implemented. */
+#define CALCULATE_FFLAGS_UNARY64_F(inst)                                       \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#define CALCULATE_FFLAGS_UNARY64_IF(inst)                                      \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#define CALCULATE_FFLAGS_UNARY64_FI(inst)                                      \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#endif
+
+/* CALLED FROM GENERATED CODE: CLEAN HELPERS */
+UInt riscv64g_calculate_fflags_fsqrt_s(Float a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_F("fsqrt.s");
+}
+UInt riscv64g_calculate_fflags_fsqrt_d(Double a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_F("fsqrt.d");
+}
+UInt riscv64g_calculate_fflags_fcvt_s_d(Double a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_F("fcvt.s.d");
+}
+UInt riscv64g_calculate_fflags_fcvt_w_d(Double a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_IF("fcvt.w.d");
+}
+UInt riscv64g_calculate_fflags_fcvt_wu_d(Double a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_IF("fcvt.wu.d");
+}
+UInt riscv64g_calculate_fflags_fcvt_l_d(Double a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_IF("fcvt.l.d");
+}
+UInt riscv64g_calculate_fflags_fcvt_lu_d(Double a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_IF("fcvt.lu.d");
+}
+UInt riscv64g_calculate_fflags_fcvt_d_l(ULong a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_FI("fcvt.d.l");
+}
+UInt riscv64g_calculate_fflags_fcvt_d_lu(ULong a1, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_UNARY64_FI("fcvt.d.lu");
+}
+
+#if defined(__riscv) && (__riscv_xlen == 64)
+/* clang-format off */
+#define CALCULATE_FFLAGS_BINARY64(inst)                                        \
+   do {                                                                        \
+      UInt res;                                                                \
+      __asm__ __volatile__(                                                    \
+         "csrr t0, fcsr\n\t"                                                   \
+         "csrw frm, %[rm]\n\t"                                                 \
+         "csrw fflags, zero\n\t"                                               \
+         inst " %[a1], %[a1], %[a2]\n\t"                                       \
+         "csrr %[res], fflags\n\t"                                             \
+         "csrw fcsr, t0\n\t"                                                   \
+         : [res] "=r"(res)                                                     \
+         : [a1] "f"(a1), [a2] "f"(a2), [rm] "r"(rm_RISCV)                      \
+         : "t0");                                                              \
+      return res;                                                              \
+   } while (0)
+#define CALCULATE_FFLAGS_BINARY64_IFF(inst)                                    \
+   do {                                                                        \
+      UInt res;                                                                \
+      __asm__ __volatile__(                                                    \
+         "csrr t0, fcsr\n\t"                                                   \
+         "csrw frm, %[rm]\n\t"                                                 \
+         "csrw fflags, zero\n\t"                                               \
+         inst " t1, %[a1], %[a2]\n\t"                                          \
+         "csrr %[res], fflags\n\t"                                             \
+         "csrw fcsr, t0\n\t"                                                   \
+         : [res] "=r"(res)                                                     \
+         : [a1] "f"(a1), [a2] "f"(a2), [rm] "r"(rm_RISCV)                      \
+         : "t0", "t1");                                                        \
+      return res;                                                              \
+   } while (0)
+/* clang-format on */
+#else
+/* No simulated version is currently implemented. */
+#define CALCULATE_FFLAGS_BINARY64(inst)                                        \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#define CALCULATE_FFLAGS_BINARY64_IFF(inst)                                    \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#endif
+
+/* CALLED FROM GENERATED CODE: CLEAN HELPERS */
+UInt riscv64g_calculate_fflags_fadd_s(Float a1, Float a2, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_BINARY64("fadd.s");
+}
+UInt riscv64g_calculate_fflags_fmul_s(Float a1, Float a2, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_BINARY64("fmul.s");
+}
+UInt riscv64g_calculate_fflags_fdiv_s(Float a1, Float a2, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_BINARY64("fdiv.s");
+}
+UInt riscv64g_calculate_fflags_fadd_d(Double a1, Double a2, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_BINARY64("fadd.d");
+}
+UInt riscv64g_calculate_fflags_fmul_d(Double a1, Double a2, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_BINARY64("fmul.d");
+}
+UInt riscv64g_calculate_fflags_fdiv_d(Double a1, Double a2, UInt rm_RISCV)
+{
+   CALCULATE_FFLAGS_BINARY64("fdiv.d");
+}
+UInt riscv64g_calculate_fflags_fmin_s(Float a1, Float a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64("fmin.s");
+}
+UInt riscv64g_calculate_fflags_fmax_s(Float a1, Float a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64("fmax.s");
+}
+UInt riscv64g_calculate_fflags_fmin_d(Double a1, Double a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64("fmin.d");
+}
+UInt riscv64g_calculate_fflags_fmax_d(Double a1, Double a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64("fmax.d");
+}
+UInt riscv64g_calculate_fflags_feq_d(Double a1, Double a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64_IFF("feq.d");
+}
+UInt riscv64g_calculate_fflags_flt_d(Double a1, Double a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64_IFF("flt.d");
+}
+UInt riscv64g_calculate_fflags_fle_d(Double a1, Double a2)
+{
+   UInt rm_RISCV = 0; /* unused */
+   CALCULATE_FFLAGS_BINARY64_IFF("fle.d");
+}
+
+#if defined(__riscv) && (__riscv_xlen == 64)
+/* clang-format off */
+#define CALCULATE_FFLAGS_TERNARY64(inst)                                       \
+   do {                                                                        \
+      UInt res;                                                                \
+      __asm__ __volatile__(                                                    \
+         "csrr t0, fcsr\n\t"                                                   \
+         "csrw frm, %[rm]\n\t"                                                 \
+         "csrw fflags, zero\n\t"                                               \
+         inst " %[a1], %[a1], %[a2], %[a3]\n\t"                                \
+         "csrr %[res], fflags\n\t"                                             \
+         "csrw fcsr, t0\n\t"                                                   \
+         : [res] "=r"(res)                                                     \
+         : [a1] "f"(a1), [a2] "f"(a2), [a3] "f"(a3), [rm] "r"(rm_RISCV)        \
+         : "t0");                                                              \
+      return res;                                                              \
+   } while (0)
+/* clang-format on */
+#else
+/* No simulated version is currently implemented. */
+#define CALCULATE_FFLAGS_TERNARY64(inst)                                       \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#endif
+
+/* CALLED FROM GENERATED CODE: CLEAN HELPER */
+UInt riscv64g_calculate_fflags_fmadd_s(Float a1,
+                                       Float a2,
+                                       Float a3,
+                                       UInt  rm_RISCV)
+{
+   CALCULATE_FFLAGS_TERNARY64("fmadd.s");
+}
+UInt riscv64g_calculate_fflags_fmadd_d(Double a1,
+                                       Double a2,
+                                       Double a3,
+                                       UInt   rm_RISCV)
+{
+   CALCULATE_FFLAGS_TERNARY64("fmadd.d");
+}
+
+#if defined(__riscv) && (__riscv_xlen == 64)
+/* clang-format off */
+#define CALCULATE_FCLASS(inst)                                                 \
+   do {                                                                        \
+      ULong res;                                                               \
+      __asm__ __volatile__(                                                    \
+         inst " %[res], %[a1]\n\t"                                             \
+         : [res] "=r"(res)                                                     \
+         : [a1] "f"(a1));                                                      \
+      return res;                                                              \
+   } while (0)
+/* clang-format on */
+#else
+/* No simulated version is currently implemented. */
+#define CALCULATE_FCLASS(inst)                                                 \
+   do {                                                                        \
+      return 0;                                                                \
+   } while (0)
+#endif
+
+/* CALLED FROM GENERATED CODE: CLEAN HELPER */
+ULong riscv64g_calculate_fclass_d(Double a1) { CALCULATE_FCLASS("fclass.d"); }
+
+/*------------------------------------------------------------*/
+/*--- Flag-helpers translation-time function specialisers. ---*/
+/*--- These help iropt specialise calls the above run-time ---*/
+/*--- flags functions.                                     ---*/
+/*------------------------------------------------------------*/
+
+IRExpr* guest_riscv64_spechelper(const HChar* function_name,
+                                 IRExpr**     args,
+                                 IRStmt**     precedingStmts,
+                                 Int          n_precedingStmts)
+{
+   return NULL;
+}
+
+/*------------------------------------------------------------*/
+/*--- Helpers for dealing with, and describing, guest      ---*/
+/*--- state as a whole.                                    ---*/
+/*------------------------------------------------------------*/
+
+/* Initialise the entire riscv64 guest state. */
+/* VISIBLE TO LIBVEX CLIENT */
+void LibVEX_GuestRISCV64_initialise(/*OUT*/ VexGuestRISCV64State* vex_state)
+{
+   vex_bzero(vex_state, sizeof(*vex_state));
+}
+
+/* Figure out if any part of the guest state contained in minoff .. maxoff
+   requires precise memory exceptions. If in doubt return True (but this
+   generates significantly slower code).
+
+   By default we enforce precise exns for guest x2 (sp), x8 (fp) and pc only.
+   These are the minimum needed to extract correct stack backtraces from riscv64
+   code.
+
+   Only x2 (sp) is needed in mode VexRegUpdSpAtMemAccess.
+*/
+Bool guest_riscv64_state_requires_precise_mem_exns(Int                minoff,
+                                                   Int                maxoff,
+                                                   VexRegisterUpdates pxControl)
+{
+   Int fp_min = offsetof(VexGuestRISCV64State, guest_x8);
+   Int fp_max = fp_min + 8 - 1;
+   Int sp_min = offsetof(VexGuestRISCV64State, guest_x2);
+   Int sp_max = sp_min + 8 - 1;
+   Int pc_min = offsetof(VexGuestRISCV64State, guest_pc);
+   Int pc_max = pc_min + 8 - 1;
+
+   if (maxoff < sp_min || minoff > sp_max) {
+      /* No overlap with sp. */
+      if (pxControl == VexRegUpdSpAtMemAccess)
+         return False; /* We only need to check stack pointer. */
+   } else
+      return True;
+
+   if (maxoff < fp_min || minoff > fp_max) {
+      /* No overlap with fp. */
+   } else
+      return True;
+
+   if (maxoff < pc_min || minoff > pc_max) {
+      /* No overlap with pc. */
+   } else
+      return True;
+
+   return False;
+}
+
+#define ALWAYSDEFD(field)                                                      \
+   {                                                                           \
+      offsetof(VexGuestRISCV64State, field),                                   \
+         (sizeof((VexGuestRISCV64State*)0)->field)                             \
+   }
+
+VexGuestLayout riscv64guest_layout = {
+   /* Total size of the guest state, in bytes. */
+   .total_sizeB = sizeof(VexGuestRISCV64State),
+
+   /* Describe the stack pointer. */
+   .offset_SP = offsetof(VexGuestRISCV64State, guest_x2),
+   .sizeof_SP = 8,
+
+   /* Describe the frame pointer. */
+   .offset_FP = offsetof(VexGuestRISCV64State, guest_x8),
+   .sizeof_FP = 8,
+
+   /* Describe the instruction pointer. */
+   .offset_IP = offsetof(VexGuestRISCV64State, guest_pc),
+   .sizeof_IP = 8,
+
+   /* Describe any sections to be regarded by Memcheck as 'always-defined'. */
+   .n_alwaysDefd = 6,
+
+   .alwaysDefd = {
+      /* 0 */ ALWAYSDEFD(guest_x0),
+      /* 1 */ ALWAYSDEFD(guest_pc),
+      /* 2 */ ALWAYSDEFD(guest_EMNOTE),
+      /* 3 */ ALWAYSDEFD(guest_CMSTART),
+      /* 4 */ ALWAYSDEFD(guest_CMLEN),
+      /* 5 */ ALWAYSDEFD(guest_NRADDR),
+   },
+};
+
+/*--------------------------------------------------------------------*/
+/*--- end                                  guest_riscv64_helpers.c ---*/
+/*--------------------------------------------------------------------*/
--- /dev/null
+++ b/VEX/priv/guest_riscv64_toIR.c
@@ -0,0 +1,3294 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                   guest_riscv64_toIR.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+/* Translates riscv64 code to IR. */
+
+/* "Special" instructions.
+
+   This instruction decoder can decode four special instructions which mean
+   nothing natively (are no-ops as far as regs/mem are concerned) but have
+   meaning for supporting Valgrind. A special instruction is flagged by
+   a 16-byte preamble:
+
+      00365613 00d65613 03365613 03d65613
+      (srli a2, a2, 3;   srli a2, a2, 13
+       srli a2, a2, 51;  srli a2, a2, 61)
+
+   Following that, one of the following 4 are allowed (standard interpretation
+   in parentheses):
+
+      00a56533 (or a0, a0, a0)   a3 = client_request ( a4 )
+      00b5e5b3 (or a1, a1, a1)   a3 = guest_NRADDR
+      00c66633 (or a2, a2, a2)   branch-and-link-to-noredir t0
+      00d6e6b3 (or a3, a3, a3)   IR injection
+
+   Any other bytes following the 16-byte preamble are illegal and constitute
+   a failure in instruction decoding. This all assumes that the preamble will
+   never occur except in specific code fragments designed for Valgrind to catch.
+*/
+
+#include "libvex_guest_riscv64.h"
+
+#include "guest_riscv64_defs.h"
+#include "main_globals.h"
+#include "main_util.h"
+
+/*------------------------------------------------------------*/
+/*--- Debugging output                                     ---*/
+/*------------------------------------------------------------*/
+
+#define DIP(format, args...)                                                   \
+   do {                                                                        \
+      if (vex_traceflags & VEX_TRACE_FE)                                       \
+         vex_printf(format, ##args);                                           \
+   } while (0)
+
+#define DIS(buf, format, args...)                                              \
+   do {                                                                        \
+      if (vex_traceflags & VEX_TRACE_FE)                                       \
+         vex_sprintf(buf, format, ##args);                                     \
+   } while (0)
+
+/*------------------------------------------------------------*/
+/*--- Helper bits and pieces for deconstructing the        ---*/
+/*--- riscv64 insn stream.                                 ---*/
+/*------------------------------------------------------------*/
+
+/* Do a little-endian load of a 32-bit word, regardless of the endianness of the
+   underlying host. */
+static inline UInt getUIntLittleEndianly(const UChar* p)
+{
+   UInt w = 0;
+   w      = (w << 8) | p[3];
+   w      = (w << 8) | p[2];
+   w      = (w << 8) | p[1];
+   w      = (w << 8) | p[0];
+   return w;
+}
+
+/* Do read of an instruction, which can be 16-bit (compressed) or 32-bit in
+   size. */
+static inline UInt getInsn(const UChar* p)
+{
+   Bool is_compressed = (p[0] & 0x3) != 0x3;
+   UInt w             = 0;
+   if (!is_compressed) {
+      w = (w << 8) | p[3];
+      w = (w << 8) | p[2];
+   }
+   w = (w << 8) | p[1];
+   w = (w << 8) | p[0];
+   return w;
+}
+
+/* Produce _uint[_bMax:_bMin]. */
+#define SLICE_UInt(_uint, _bMax, _bMin)                                        \
+   ((((UInt)(_uint)) >> (_bMin)) &                                             \
+    (UInt)((1ULL << ((_bMax) - (_bMin) + 1)) - 1ULL))
+
+/*------------------------------------------------------------*/
+/*--- Helpers for constructing IR.                         ---*/
+/*------------------------------------------------------------*/
+
+/* Create an expression to produce a 64-bit constant. */
+static IRExpr* mkU64(ULong i) { return IRExpr_Const(IRConst_U64(i)); }
+
+/* Create an expression to produce a 32-bit constant. */
+static IRExpr* mkU32(UInt i) { return IRExpr_Const(IRConst_U32(i)); }
+
+/* Create an expression to produce an 8-bit constant. */
+static IRExpr* mkU8(UInt i)
+{
+   vassert(i < 256);
+   return IRExpr_Const(IRConst_U8((UChar)i));
+}
+
+/* Create an expression to read a temporary. */
+static IRExpr* mkexpr(IRTemp tmp) { return IRExpr_RdTmp(tmp); }
+
+/* Create an unary-operation expression. */
+static IRExpr* unop(IROp op, IRExpr* a) { return IRExpr_Unop(op, a); }
+
+/* Create a binary-operation expression. */
+static IRExpr* binop(IROp op, IRExpr* a1, IRExpr* a2)
+{
+   return IRExpr_Binop(op, a1, a2);
+}
+
+/* Create a ternary-operation expression. */
+static IRExpr* triop(IROp op, IRExpr* a1, IRExpr* a2, IRExpr* a3)
+{
+   return IRExpr_Triop(op, a1, a2, a3);
+}
+
+/* Create a quaternary-operation expression. */
+static IRExpr* qop(IROp op, IRExpr* a1, IRExpr* a2, IRExpr* a3, IRExpr* a4)
+{
+   return IRExpr_Qop(op, a1, a2, a3, a4);
+}
+
+/* Create an expression to load a value from memory (in the little-endian
+   order). */
+static IRExpr* loadLE(IRType ty, IRExpr* addr)
+{
+   return IRExpr_Load(Iend_LE, ty, addr);
+}
+
+/* Add a statement to the list held by irsb. */
+static void stmt(/*MOD*/ IRSB* irsb, IRStmt* st) { addStmtToIRSB(irsb, st); }
+
+/* Add a statement to assign a value to a temporary. */
+static void assign(/*MOD*/ IRSB* irsb, IRTemp dst, IRExpr* e)
+{
+   stmt(irsb, IRStmt_WrTmp(dst, e));
+}
+
+/* Generate a statement to store a value in memory (in the little-endian
+   order). */
+static void storeLE(/*MOD*/ IRSB* irsb, IRExpr* addr, IRExpr* data)
+{
+   stmt(irsb, IRStmt_Store(Iend_LE, addr, data));
+}
+
+/* Generate a new temporary of the given type. */
+static IRTemp newTemp(/*MOD*/ IRSB* irsb, IRType ty)
+{
+   vassert(isPlausibleIRType(ty));
+   return newIRTemp(irsb->tyenv, ty);
+}
+
+/* Sign-extend a 32/64-bit integer expression to 64 bits. */
+static IRExpr* widenSto64(IRType srcTy, IRExpr* e)
+{
+   switch (srcTy) {
+   case Ity_I64:
+      return e;
+   case Ity_I32:
+      return unop(Iop_32Sto64, e);
+   default:
+      vpanic("widenSto64(riscv64)");
+   }
+}
+
+/* Narrow a 64-bit integer expression to 32/64 bits. */
+static IRExpr* narrowFrom64(IRType dstTy, IRExpr* e)
+{
+   switch (dstTy) {
+   case Ity_I64:
+      return e;
+   case Ity_I32:
+      return unop(Iop_64to32, e);
+   default:
+      vpanic("narrowFrom64(riscv64)");
+   }
+}
+
+/*------------------------------------------------------------*/
+/*--- Offsets of various parts of the riscv64 guest state  ---*/
+/*------------------------------------------------------------*/
+
+#define OFFB_X0  offsetof(VexGuestRISCV64State, guest_x0)
+#define OFFB_X1  offsetof(VexGuestRISCV64State, guest_x1)
+#define OFFB_X2  offsetof(VexGuestRISCV64State, guest_x2)
+#define OFFB_X3  offsetof(VexGuestRISCV64State, guest_x3)
+#define OFFB_X4  offsetof(VexGuestRISCV64State, guest_x4)
+#define OFFB_X5  offsetof(VexGuestRISCV64State, guest_x5)
+#define OFFB_X6  offsetof(VexGuestRISCV64State, guest_x6)
+#define OFFB_X7  offsetof(VexGuestRISCV64State, guest_x7)
+#define OFFB_X8  offsetof(VexGuestRISCV64State, guest_x8)
+#define OFFB_X9  offsetof(VexGuestRISCV64State, guest_x9)
+#define OFFB_X10 offsetof(VexGuestRISCV64State, guest_x10)
+#define OFFB_X11 offsetof(VexGuestRISCV64State, guest_x11)
+#define OFFB_X12 offsetof(VexGuestRISCV64State, guest_x12)
+#define OFFB_X13 offsetof(VexGuestRISCV64State, guest_x13)
+#define OFFB_X14 offsetof(VexGuestRISCV64State, guest_x14)
+#define OFFB_X15 offsetof(VexGuestRISCV64State, guest_x15)
+#define OFFB_X16 offsetof(VexGuestRISCV64State, guest_x16)
+#define OFFB_X17 offsetof(VexGuestRISCV64State, guest_x17)
+#define OFFB_X18 offsetof(VexGuestRISCV64State, guest_x18)
+#define OFFB_X19 offsetof(VexGuestRISCV64State, guest_x19)
+#define OFFB_X20 offsetof(VexGuestRISCV64State, guest_x20)
+#define OFFB_X21 offsetof(VexGuestRISCV64State, guest_x21)
+#define OFFB_X22 offsetof(VexGuestRISCV64State, guest_x22)
+#define OFFB_X23 offsetof(VexGuestRISCV64State, guest_x23)
+#define OFFB_X24 offsetof(VexGuestRISCV64State, guest_x24)
+#define OFFB_X25 offsetof(VexGuestRISCV64State, guest_x25)
+#define OFFB_X26 offsetof(VexGuestRISCV64State, guest_x26)
+#define OFFB_X27 offsetof(VexGuestRISCV64State, guest_x27)
+#define OFFB_X28 offsetof(VexGuestRISCV64State, guest_x28)
+#define OFFB_X29 offsetof(VexGuestRISCV64State, guest_x29)
+#define OFFB_X30 offsetof(VexGuestRISCV64State, guest_x30)
+#define OFFB_X31 offsetof(VexGuestRISCV64State, guest_x31)
+#define OFFB_PC  offsetof(VexGuestRISCV64State, guest_pc)
+
+#define OFFB_F0   offsetof(VexGuestRISCV64State, guest_f0)
+#define OFFB_F1   offsetof(VexGuestRISCV64State, guest_f1)
+#define OFFB_F2   offsetof(VexGuestRISCV64State, guest_f2)
+#define OFFB_F3   offsetof(VexGuestRISCV64State, guest_f3)
+#define OFFB_F4   offsetof(VexGuestRISCV64State, guest_f4)
+#define OFFB_F5   offsetof(VexGuestRISCV64State, guest_f5)
+#define OFFB_F6   offsetof(VexGuestRISCV64State, guest_f6)
+#define OFFB_F7   offsetof(VexGuestRISCV64State, guest_f7)
+#define OFFB_F8   offsetof(VexGuestRISCV64State, guest_f8)
+#define OFFB_F9   offsetof(VexGuestRISCV64State, guest_f9)
+#define OFFB_F10  offsetof(VexGuestRISCV64State, guest_f10)
+#define OFFB_F11  offsetof(VexGuestRISCV64State, guest_f11)
+#define OFFB_F12  offsetof(VexGuestRISCV64State, guest_f12)
+#define OFFB_F13  offsetof(VexGuestRISCV64State, guest_f13)
+#define OFFB_F14  offsetof(VexGuestRISCV64State, guest_f14)
+#define OFFB_F15  offsetof(VexGuestRISCV64State, guest_f15)
+#define OFFB_F16  offsetof(VexGuestRISCV64State, guest_f16)
+#define OFFB_F17  offsetof(VexGuestRISCV64State, guest_f17)
+#define OFFB_F18  offsetof(VexGuestRISCV64State, guest_f18)
+#define OFFB_F19  offsetof(VexGuestRISCV64State, guest_f19)
+#define OFFB_F20  offsetof(VexGuestRISCV64State, guest_f20)
+#define OFFB_F21  offsetof(VexGuestRISCV64State, guest_f21)
+#define OFFB_F22  offsetof(VexGuestRISCV64State, guest_f22)
+#define OFFB_F23  offsetof(VexGuestRISCV64State, guest_f23)
+#define OFFB_F24  offsetof(VexGuestRISCV64State, guest_f24)
+#define OFFB_F25  offsetof(VexGuestRISCV64State, guest_f25)
+#define OFFB_F26  offsetof(VexGuestRISCV64State, guest_f26)
+#define OFFB_F27  offsetof(VexGuestRISCV64State, guest_f27)
+#define OFFB_F28  offsetof(VexGuestRISCV64State, guest_f28)
+#define OFFB_F29  offsetof(VexGuestRISCV64State, guest_f29)
+#define OFFB_F30  offsetof(VexGuestRISCV64State, guest_f30)
+#define OFFB_F31  offsetof(VexGuestRISCV64State, guest_f31)
+#define OFFB_FCSR offsetof(VexGuestRISCV64State, guest_fcsr)
+
+#define OFFB_EMNOTE  offsetof(VexGuestRISCV64State, guest_EMNOTE)
+#define OFFB_CMSTART offsetof(VexGuestRISCV64State, guest_CMSTART)
+#define OFFB_CMLEN   offsetof(VexGuestRISCV64State, guest_CMLEN)
+#define OFFB_NRADDR  offsetof(VexGuestRISCV64State, guest_NRADDR)
+
+#define OFFB_LLSC_SIZE offsetof(VexGuestRISCV64State, guest_LLSC_SIZE)
+#define OFFB_LLSC_ADDR offsetof(VexGuestRISCV64State, guest_LLSC_ADDR)
+#define OFFB_LLSC_DATA offsetof(VexGuestRISCV64State, guest_LLSC_DATA)
+
+/*------------------------------------------------------------*/
+/*--- Integer registers                                    ---*/
+/*------------------------------------------------------------*/
+
+static Int offsetIReg64(UInt iregNo)
+{
+   switch (iregNo) {
+   case 0:
+      return OFFB_X0;
+   case 1:
+      return OFFB_X1;
+   case 2:
+      return OFFB_X2;
+   case 3:
+      return OFFB_X3;
+   case 4:
+      return OFFB_X4;
+   case 5:
+      return OFFB_X5;
+   case 6:
+      return OFFB_X6;
+   case 7:
+      return OFFB_X7;
+   case 8:
+      return OFFB_X8;
+   case 9:
+      return OFFB_X9;
+   case 10:
+      return OFFB_X10;
+   case 11:
+      return OFFB_X11;
+   case 12:
+      return OFFB_X12;
+   case 13:
+      return OFFB_X13;
+   case 14:
+      return OFFB_X14;
+   case 15:
+      return OFFB_X15;
+   case 16:
+      return OFFB_X16;
+   case 17:
+      return OFFB_X17;
+   case 18:
+      return OFFB_X18;
+   case 19:
+      return OFFB_X19;
+   case 20:
+      return OFFB_X20;
+   case 21:
+      return OFFB_X21;
+   case 22:
+      return OFFB_X22;
+   case 23:
+      return OFFB_X23;
+   case 24:
+      return OFFB_X24;
+   case 25:
+      return OFFB_X25;
+   case 26:
+      return OFFB_X26;
+   case 27:
+      return OFFB_X27;
+   case 28:
+      return OFFB_X28;
+   case 29:
+      return OFFB_X29;
+   case 30:
+      return OFFB_X30;
+   case 31:
+      return OFFB_X31;
+   default:
+      vassert(0);
+   }
+}
+
+/* Obtain ABI name of a register. */
+static const HChar* nameIReg(UInt iregNo)
+{
+   vassert(iregNo < 32);
+   static const HChar* names[32] = {
+      "zero", "ra", "sp", "gp", "tp",  "t0",  "t1", "t2", "s0", "s1", "a0",
+      "a1",   "a2", "a3", "a4", "a5",  "a6",  "a7", "s2", "s3", "s4", "s5",
+      "s6",   "s7", "s8", "s9", "s10", "s11", "t3", "t4", "t5", "t6"};
+   return names[iregNo];
+}
+
+/* Read a 64-bit value from a guest integer register. */
+static IRExpr* getIReg64(UInt iregNo)
+{
+   vassert(iregNo < 32);
+   return IRExpr_Get(offsetIReg64(iregNo), Ity_I64);
+}
+
+/* Write a 64-bit value into a guest integer register. */
+static void putIReg64(/*OUT*/ IRSB* irsb, UInt iregNo, /*IN*/ IRExpr* e)
+{
+   vassert(iregNo > 0 && iregNo < 32);
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I64);
+   stmt(irsb, IRStmt_Put(offsetIReg64(iregNo), e));
+}
+
+/* Read a 32-bit value from a guest integer register. */
+static IRExpr* getIReg32(UInt iregNo)
+{
+   vassert(iregNo < 32);
+   return unop(Iop_64to32, IRExpr_Get(offsetIReg64(iregNo), Ity_I64));
+}
+
+/* Write a 32-bit value into a guest integer register. */
+static void putIReg32(/*OUT*/ IRSB* irsb, UInt iregNo, /*IN*/ IRExpr* e)
+{
+   vassert(iregNo > 0 && iregNo < 32);
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I32);
+   stmt(irsb, IRStmt_Put(offsetIReg64(iregNo), unop(Iop_32Sto64, e)));
+}
+
+/* Write an address into the guest pc. */
+static void putPC(/*OUT*/ IRSB* irsb, /*IN*/ IRExpr* e)
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I64);
+   stmt(irsb, IRStmt_Put(OFFB_PC, e));
+}
+
+/*------------------------------------------------------------*/
+/*--- Floating-point registers                             ---*/
+/*------------------------------------------------------------*/
+
+static Int offsetFReg(UInt fregNo)
+{
+   switch (fregNo) {
+   case 0:
+      return OFFB_F0;
+   case 1:
+      return OFFB_F1;
+   case 2:
+      return OFFB_F2;
+   case 3:
+      return OFFB_F3;
+   case 4:
+      return OFFB_F4;
+   case 5:
+      return OFFB_F5;
+   case 6:
+      return OFFB_F6;
+   case 7:
+      return OFFB_F7;
+   case 8:
+      return OFFB_F8;
+   case 9:
+      return OFFB_F9;
+   case 10:
+      return OFFB_F10;
+   case 11:
+      return OFFB_F11;
+   case 12:
+      return OFFB_F12;
+   case 13:
+      return OFFB_F13;
+   case 14:
+      return OFFB_F14;
+   case 15:
+      return OFFB_F15;
+   case 16:
+      return OFFB_F16;
+   case 17:
+      return OFFB_F17;
+   case 18:
+      return OFFB_F18;
+   case 19:
+      return OFFB_F19;
+   case 20:
+      return OFFB_F20;
+   case 21:
+      return OFFB_F21;
+   case 22:
+      return OFFB_F22;
+   case 23:
+      return OFFB_F23;
+   case 24:
+      return OFFB_F24;
+   case 25:
+      return OFFB_F25;
+   case 26:
+      return OFFB_F26;
+   case 27:
+      return OFFB_F27;
+   case 28:
+      return OFFB_F28;
+   case 29:
+      return OFFB_F29;
+   case 30:
+      return OFFB_F30;
+   case 31:
+      return OFFB_F31;
+   default:
+      vassert(0);
+   }
+}
+
+/* Obtain ABI name of a register. */
+static const HChar* nameFReg(UInt fregNo)
+{
+   vassert(fregNo < 32);
+   static const HChar* names[32] = {
+      "ft0", "ft1", "ft2",  "ft3",  "ft4", "ft5", "ft6",  "ft7",
+      "fs0", "fs1", "fa0",  "fa1",  "fa2", "fa3", "fa4",  "fa5",
+      "fa6", "fa7", "fs2",  "fs3",  "fs4", "fs5", "fs6",  "fs7",
+      "fs8", "fs9", "fs10", "fs11", "ft8", "ft9", "ft10", "ft11"};
+   return names[fregNo];
+}
+
+/* Read a 64-bit value from a guest floating-point register. */
+static IRExpr* getFReg64(UInt fregNo)
+{
+   vassert(fregNo < 32);
+   return IRExpr_Get(offsetFReg(fregNo), Ity_F64);
+}
+
+/* Write a 64-bit value into a guest floating-point register. */
+static void putFReg64(/*OUT*/ IRSB* irsb, UInt fregNo, /*IN*/ IRExpr* e)
+{
+   vassert(fregNo < 32);
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_F64);
+   stmt(irsb, IRStmt_Put(offsetFReg(fregNo), e));
+}
+
+/* Read a 32-bit value from a guest floating-point register. */
+static IRExpr* getFReg32(UInt fregNo)
+{
+   vassert(fregNo < 32);
+   /* Note that the following access depends on the host being little-endian
+      which is checked in disInstr_RISCV64(). */
+   /* TODO Check that the value is correctly NaN-boxed. If not then return
+      the 32-bit canonical qNaN, as mandated by the RISC-V ISA. */
+   return IRExpr_Get(offsetFReg(fregNo), Ity_F32);
+}
+
+/* Write a 32-bit value into a guest floating-point register. */
+static void putFReg32(/*OUT*/ IRSB* irsb, UInt fregNo, /*IN*/ IRExpr* e)
+{
+   vassert(fregNo < 32);
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_F32);
+   /* Note that the following access depends on the host being little-endian
+      which is checked in disInstr_RISCV64(). */
+   Int offset = offsetFReg(fregNo);
+   stmt(irsb, IRStmt_Put(offset, e));
+   /* Write 1's in the upper bits of the target 64-bit register to create
+      a NaN-boxed value, as mandated by the RISC-V ISA. */
+   stmt(irsb, IRStmt_Put(offset + 4, mkU32(0xffffffff)));
+   /* TODO Check that this works with Memcheck. */
+}
+
+/* Read a 32-bit value from the fcsr. */
+static IRExpr* getFCSR(void) { return IRExpr_Get(OFFB_FCSR, Ity_I32); }
+
+/* Write a 32-bit value into the fcsr. */
+static void putFCSR(/*OUT*/ IRSB* irsb, /*IN*/ IRExpr* e)
+{
+   vassert(typeOfIRExpr(irsb->tyenv, e) == Ity_I32);
+   stmt(irsb, IRStmt_Put(OFFB_FCSR, e));
+}
+
+/* Generate IR to get hold of the rounding mode in both RISC-V and IR
+   formats. A floating-point operation can use either a static rounding mode
+   encoded in the instruction, or a dynamic rounding mode held in fcsr. Bind the
+   final result to the passed temporaries (which are allocated by the function).
+ */
+static void mk_get_rounding_mode(/*MOD*/ IRSB*   irsb,
+                                 /*OUT*/ IRTemp* rm_RISCV,
+                                 /*OUT*/ IRTemp* rm_IR,
+                                 UInt            inst_rm_RISCV)
+{
+   /*
+      rounding mode                | RISC-V |  IR
+      --------------------------------------------
+      to nearest, ties to even     |   000  | 0000
+      to zero                      |   001  | 0011
+      to +infinity                 |   010  | 0010
+      to -infinity                 |   011  | 0001
+      to nearest, ties away from 0 |   100  | 0100
+      invalid                      |   101  | 1000
+      invalid                      |   110  | 1000
+      dynamic                      |   111  | 1000
+
+      The 'dynamic' value selects the mode from fcsr. Its value is valid when
+      encoded in the instruction but naturally invalid when found in fcsr.
+
+      Static mode is known at the decode time and can be directly expressed by
+      a respective rounding mode IR constant.
+
+      Dynamic mode requires a runtime mapping from the RISC-V to the IR mode.
+      It can be implemented using the following transformation:
+         t0 = fcsr_rm_RISCV - 20
+         t1 = t0 >> 2
+         t2 = fcsr_rm_RISCV + 3
+         t3 = t2 ^ 3
+         rm_IR = t1 & t3
+   */
+   *rm_RISCV = newTemp(irsb, Ity_I32);
+   *rm_IR    = newTemp(irsb, Ity_I32);
+   switch (inst_rm_RISCV) {
+   case 0b000:
+      assign(irsb, *rm_RISCV, mkU32(0));
+      assign(irsb, *rm_IR, mkU32(Irrm_NEAREST));
+      break;
+   case 0b001:
+      assign(irsb, *rm_RISCV, mkU32(1));
+      assign(irsb, *rm_IR, mkU32(Irrm_ZERO));
+      break;
+   case 0b010:
+      assign(irsb, *rm_RISCV, mkU32(2));
+      assign(irsb, *rm_IR, mkU32(Irrm_PosINF));
+      break;
+   case 0b011:
+      assign(irsb, *rm_RISCV, mkU32(3));
+      assign(irsb, *rm_IR, mkU32(Irrm_NegINF));
+      break;
+   case 0b100:
+      assign(irsb, *rm_RISCV, mkU32(4));
+      assign(irsb, *rm_IR, mkU32(Irrm_NEAREST_TIE_AWAY_0));
+      break;
+   case 0b101:
+      assign(irsb, *rm_RISCV, mkU32(5));
+      assign(irsb, *rm_IR, mkU32(Irrm_INVALID));
+      break;
+   case 0b110:
+      assign(irsb, *rm_RISCV, mkU32(6));
+      assign(irsb, *rm_IR, mkU32(Irrm_INVALID));
+      break;
+   case 0b111: {
+      assign(irsb, *rm_RISCV,
+             binop(Iop_And32, binop(Iop_Shr32, getFCSR(), mkU8(5)), mkU32(7)));
+      IRTemp t0 = newTemp(irsb, Ity_I32);
+      assign(irsb, t0, binop(Iop_Sub32, mkexpr(*rm_RISCV), mkU32(20)));
+      IRTemp t1 = newTemp(irsb, Ity_I32);
+      assign(irsb, t1, binop(Iop_Shr32, mkexpr(t0), mkU8(2)));
+      IRTemp t2 = newTemp(irsb, Ity_I32);
+      assign(irsb, t2, binop(Iop_Add32, mkexpr(*rm_RISCV), mkU32(3)));
+      IRTemp t3 = newTemp(irsb, Ity_I32);
+      assign(irsb, t3, binop(Iop_Xor32, mkexpr(t2), mkU32(3)));
+      assign(irsb, *rm_IR, binop(Iop_And32, mkexpr(t1), mkexpr(t3)));
+      break;
+   }
+   default:
+      vassert(0);
+   }
+}
+
+/*------------------------------------------------------------*/
+/*--- Name helpers                                         ---*/
+/*------------------------------------------------------------*/
+
+/* Obtain an acquire/release atomic-instruction suffix. */
+static const HChar* nameAqRlSuffix(UInt aqrl)
+{
+   switch (aqrl) {
+   case 0b00:
+      return "";
+   case 0b01:
+      return ".rl";
+   case 0b10:
+      return ".aq";
+   case 0b11:
+      return ".aqrl";
+   default:
+      vpanic("nameAqRlSuffix(riscv64)");
+   }
+}
+
+/* Obtain a control/status register name. */
+static const HChar* nameCSR(UInt csr)
+{
+   switch (csr) {
+   case 0x001:
+      return "fflags";
+   case 0x002:
+      return "frm";
+   case 0x003:
+      return "fcsr";
+   default:
+      vpanic("nameCSR(riscv64)");
+   }
+}
+
+/* Obtain a floating-point rounding-mode operand string. */
+static const HChar* nameRMOperand(UInt rm)
+{
+   switch (rm) {
+   case 0b000:
+      return ", rne";
+   case 0b001:
+      return ", rtz";
+   case 0b010:
+      return ", rdn";
+   case 0b011:
+      return ", rup";
+   case 0b100:
+      return ", rmm";
+   case 0b101:
+      return ", <invalid>";
+   case 0b110:
+      return ", <invalid>";
+   case 0b111:
+      return ""; /* dyn */
+   default:
+      vpanic("nameRMOperand(riscv64)");
+   }
+}
+
+/*------------------------------------------------------------*/
+/*--- Disassemble a single instruction                     ---*/
+/*------------------------------------------------------------*/
+
+/* A macro to fish bits out of 'insn' which is a local variable to all
+   disassembly functions. */
+#define INSN(_bMax, _bMin) SLICE_UInt(insn, (_bMax), (_bMin))
+
+static Bool dis_RV64C(/*MB_OUT*/ DisResult* dres,
+                      /*OUT*/ IRSB*         irsb,
+                      UInt                  insn,
+                      Addr                  guest_pc_curr_instr,
+                      Bool                  sigill_diag)
+{
+   vassert(INSN(1, 0) == 0b00 || INSN(1, 0) == 0b01 || INSN(1, 0) == 0b10);
+
+   /* ---- RV64C compressed instruction set, quadrant 0 ----- */
+
+   /* ------------- c.addi4spn rd, nzuimm[9:2] -------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b000) {
+      UInt rd = INSN(4, 2) + 8;
+      UInt nzuimm9_2 =
+         INSN(10, 7) << 4 | INSN(12, 11) << 2 | INSN(5, 5) << 1 | INSN(6, 6);
+      if (nzuimm9_2 == 0) {
+         /* Invalid C.ADDI4SPN, fall through. */
+      } else {
+         ULong uimm = nzuimm9_2 << 2;
+         putIReg64(irsb, rd,
+                   binop(Iop_Add64, getIReg64(2 /*x2/sp*/), mkU64(uimm)));
+         DIP("c.addi4spn %s, %llu\n", nameIReg(rd), uimm);
+         return True;
+      }
+   }
+
+   /* -------------- c.fld rd, uimm[7:3](rs1) --------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b001) {
+      UInt  rd      = INSN(4, 2) + 8;
+      UInt  rs1     = INSN(9, 7) + 8;
+      UInt  uimm7_3 = INSN(6, 5) << 3 | INSN(12, 10);
+      ULong uimm    = uimm7_3 << 3;
+      putFReg64(irsb, rd,
+                loadLE(Ity_F64, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm))));
+      DIP("c.fld %s, %llu(%s)\n", nameFReg(rd), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* --------------- c.lw rd, uimm[6:2](rs1) --------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b010) {
+      UInt  rd      = INSN(4, 2) + 8;
+      UInt  rs1     = INSN(9, 7) + 8;
+      UInt  uimm6_2 = INSN(5, 5) << 4 | INSN(12, 10) << 1 | INSN(6, 6);
+      ULong uimm    = uimm6_2 << 2;
+      putIReg64(
+         irsb, rd,
+         unop(Iop_32Sto64,
+              loadLE(Ity_I32, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)))));
+      DIP("c.lw %s, %llu(%s)\n", nameIReg(rd), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* --------------- c.ld rd, uimm[7:3](rs1) --------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b011) {
+      UInt  rd      = INSN(4, 2) + 8;
+      UInt  rs1     = INSN(9, 7) + 8;
+      UInt  uimm7_3 = INSN(6, 5) << 3 | INSN(12, 10);
+      ULong uimm    = uimm7_3 << 3;
+      putIReg64(irsb, rd,
+                loadLE(Ity_I64, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm))));
+      DIP("c.ld %s, %llu(%s)\n", nameIReg(rd), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------------- c.fsd rs2, uimm[7:3](rs1) -------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b101) {
+      UInt  rs1     = INSN(9, 7) + 8;
+      UInt  rs2     = INSN(4, 2) + 8;
+      UInt  uimm7_3 = INSN(6, 5) << 3 | INSN(12, 10);
+      ULong uimm    = uimm7_3 << 3;
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)),
+              getFReg64(rs2));
+      DIP("c.fsd %s, %llu(%s)\n", nameFReg(rs2), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------------- c.sw rs2, uimm[6:2](rs1) --------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b110) {
+      UInt  rs1     = INSN(9, 7) + 8;
+      UInt  rs2     = INSN(4, 2) + 8;
+      UInt  uimm6_2 = INSN(5, 5) << 4 | INSN(12, 10) << 1 | INSN(6, 6);
+      ULong uimm    = uimm6_2 << 2;
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)),
+              unop(Iop_64to32, getIReg64(rs2)));
+      DIP("c.sw %s, %llu(%s)\n", nameIReg(rs2), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------------- c.sd rs2, uimm[7:3](rs1) --------------- */
+   if (INSN(1, 0) == 0b00 && INSN(15, 13) == 0b111) {
+      UInt  rs1     = INSN(9, 7) + 8;
+      UInt  rs2     = INSN(4, 2) + 8;
+      UInt  uimm7_3 = INSN(6, 5) << 3 | INSN(12, 10);
+      ULong uimm    = uimm7_3 << 3;
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)),
+              getIReg64(rs2));
+      DIP("c.sd %s, %llu(%s)\n", nameIReg(rs2), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* ---- RV64C compressed instruction set, quadrant 1 ----- */
+
+   /* ------------------------ c.nop ------------------------ */
+   if (INSN(15, 0) == 0b0000000000000001) {
+      DIP("c.nop\n");
+      return True;
+   }
+
+   /* -------------- c.addi rd_rs1, nzimm[5:0] -------------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 13) == 0b000) {
+      UInt rd_rs1   = INSN(11, 7);
+      UInt nzimm5_0 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (rd_rs1 == 0 || nzimm5_0 == 0) {
+         /* Invalid C.ADDI, fall through. */
+      } else {
+         ULong simm = vex_sx_to_64(nzimm5_0, 6);
+         putIReg64(irsb, rd_rs1,
+                   binop(Iop_Add64, getIReg64(rd_rs1), mkU64(simm)));
+         DIP("c.addi %s, %lld\n", nameIReg(rd_rs1), (Long)simm);
+         return True;
+      }
+   }
+
+   /* -------------- c.addiw rd_rs1, imm[5:0] --------------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 13) == 0b001) {
+      UInt rd_rs1 = INSN(11, 7);
+      UInt imm5_0 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (rd_rs1 == 0) {
+         /* Invalid C.ADDIW, fall through. */
+      } else {
+         UInt simm = (UInt)vex_sx_to_64(imm5_0, 6);
+         putIReg32(irsb, rd_rs1,
+                   binop(Iop_Add32, getIReg32(rd_rs1), mkU32(simm)));
+         DIP("c.addiw %s, %d\n", nameIReg(rd_rs1), (Int)simm);
+         return True;
+      }
+   }
+
+   /* ------------------ c.li rd, imm[5:0] ------------------ */
+   if (INSN(1, 0) == 0b01 && INSN(15, 13) == 0b010) {
+      UInt rd     = INSN(11, 7);
+      UInt imm5_0 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (rd == 0) {
+         /* Invalid C.LI, fall through. */
+      } else {
+         ULong simm = vex_sx_to_64(imm5_0, 6);
+         putIReg64(irsb, rd, mkU64(simm));
+         DIP("c.li %s, %lld\n", nameIReg(rd), (Long)simm);
+         return True;
+      }
+   }
+
+   /* ---------------- c.addi16sp nzimm[9:4] ---------------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 13) == 0b011) {
+      UInt rd_rs1   = INSN(11, 7);
+      UInt nzimm9_4 = INSN(12, 12) << 5 | INSN(4, 3) << 3 | INSN(5, 5) << 2 |
+                      INSN(2, 2) << 1 | INSN(6, 6);
+      if (rd_rs1 != 2 || nzimm9_4 == 0) {
+         /* Invalid C.ADDI16SP, fall through. */
+      } else {
+         ULong simm = vex_sx_to_64(nzimm9_4 << 4, 10);
+         putIReg64(irsb, rd_rs1,
+                   binop(Iop_Add64, getIReg64(rd_rs1), mkU64(simm)));
+         DIP("c.addi16sp %lld\n", (Long)simm);
+         return True;
+      }
+   }
+
+   /* --------------- c.lui rd, nzimm[17:12] ---------------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 13) == 0b011) {
+      UInt rd         = INSN(11, 7);
+      UInt nzimm17_12 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (rd == 0 || rd == 2 || nzimm17_12 == 0) {
+         /* Invalid C.LUI, fall through. */
+      } else {
+         putIReg64(irsb, rd, mkU64(vex_sx_to_64(nzimm17_12 << 12, 18)));
+         DIP("c.lui %s, 0x%x\n", nameIReg(rd), nzimm17_12);
+         return True;
+      }
+   }
+
+   /* ---------- c.{srli,srai} rd_rs1, nzuimm[5:0] ---------- */
+   if (INSN(1, 0) == 0b01 && INSN(11, 11) == 0b0 && INSN(15, 13) == 0b100) {
+      Bool is_log    = INSN(10, 10) == 0b0;
+      UInt rd_rs1    = INSN(9, 7) + 8;
+      UInt nzuimm5_0 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (nzuimm5_0 == 0) {
+         /* Invalid C.{SRLI,SRAI}, fall through. */
+      } else {
+         putIReg64(irsb, rd_rs1,
+                   binop(is_log ? Iop_Shr64 : Iop_Sar64, getIReg64(rd_rs1),
+                         mkU8(nzuimm5_0)));
+         DIP("c.%s %s, %u\n", is_log ? "srli" : "srai", nameIReg(rd_rs1),
+             nzuimm5_0);
+         return True;
+      }
+   }
+
+   /* --------------- c.andi rd_rs1, imm[5:0] --------------- */
+   if (INSN(1, 0) == 0b01 && INSN(11, 10) == 0b10 && INSN(15, 13) == 0b100) {
+      UInt rd_rs1 = INSN(9, 7) + 8;
+      UInt imm5_0 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (rd_rs1 == 0) {
+         /* Invalid C.ANDI, fall through. */
+      } else {
+         ULong simm = vex_sx_to_64(imm5_0, 6);
+         putIReg64(irsb, rd_rs1,
+                   binop(Iop_And64, getIReg64(rd_rs1), mkU64(simm)));
+         DIP("c.andi %s, 0x%llx\n", nameIReg(rd_rs1), simm);
+         return True;
+      }
+   }
+
+   /* ----------- c.{sub,xor,or,and} rd_rs1, rs2 ----------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 10) == 0b100011) {
+      UInt         funct2 = INSN(6, 5);
+      UInt         rd_rs1 = INSN(9, 7) + 8;
+      UInt         rs2    = INSN(4, 2) + 8;
+      const HChar* name;
+      IROp         op;
+      switch (funct2) {
+      case 0b00:
+         name = "sub";
+         op   = Iop_Sub64;
+         break;
+      case 0b01:
+         name = "xor";
+         op   = Iop_Xor64;
+         break;
+      case 0b10:
+         name = "or";
+         op   = Iop_Or64;
+         break;
+      case 0b11:
+         name = "and";
+         op   = Iop_And64;
+         break;
+      default:
+         vassert(0);
+      }
+      putIReg64(irsb, rd_rs1, binop(op, getIReg64(rd_rs1), getIReg64(rs2)));
+      DIP("c.%s %s, %s\n", name, nameIReg(rd_rs1), nameIReg(rs2));
+      return True;
+   }
+
+   /* -------------- c.{subw,addw} rd_rs1, rs2 -------------- */
+   if (INSN(1, 0) == 0b01 && INSN(6, 6) == 0b0 && INSN(15, 10) == 0b100111) {
+      Bool is_sub = INSN(5, 5) == 0b0;
+      UInt rd_rs1 = INSN(9, 7) + 8;
+      UInt rs2    = INSN(4, 2) + 8;
+      putIReg32(irsb, rd_rs1,
+                binop(is_sub ? Iop_Sub32 : Iop_Add32, getIReg32(rd_rs1),
+                      getIReg32(rs2)));
+      DIP("c.%s %s, %s\n", is_sub ? "subw" : "addw", nameIReg(rd_rs1),
+          nameIReg(rs2));
+      return True;
+   }
+
+   /* -------------------- c.j imm[11:1] -------------------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 13) == 0b101) {
+      UInt imm11_1 = INSN(12, 12) << 10 | INSN(8, 8) << 9 | INSN(10, 9) << 7 |
+                     INSN(6, 6) << 6 | INSN(7, 7) << 5 | INSN(2, 2) << 4 |
+                     INSN(11, 11) << 3 | INSN(5, 3);
+      ULong simm   = vex_sx_to_64(imm11_1 << 1, 12);
+      ULong dst_pc = guest_pc_curr_instr + simm;
+      putPC(irsb, mkU64(dst_pc));
+      dres->whatNext    = Dis_StopHere;
+      dres->jk_StopHere = Ijk_Boring;
+      DIP("c.j 0x%llx\n", dst_pc);
+      return True;
+   }
+
+   /* ------------- c.{beqz,bnez} rs1, imm[8:1] ------------- */
+   if (INSN(1, 0) == 0b01 && INSN(15, 14) == 0b11) {
+      Bool is_eq  = INSN(13, 13) == 0b0;
+      UInt rs1    = INSN(9, 7) + 8;
+      UInt imm8_1 = INSN(12, 12) << 7 | INSN(6, 5) << 5 | INSN(2, 2) << 4 |
+                    INSN(11, 10) << 2 | INSN(4, 3);
+      ULong simm   = vex_sx_to_64(imm8_1 << 1, 9);
+      ULong dst_pc = guest_pc_curr_instr + simm;
+      stmt(irsb, IRStmt_Exit(binop(is_eq ? Iop_CmpEQ64 : Iop_CmpNE64,
+                                   getIReg64(rs1), mkU64(0)),
+                             Ijk_Boring, IRConst_U64(dst_pc), OFFB_PC));
+      putPC(irsb, mkU64(guest_pc_curr_instr + 2));
+      dres->whatNext    = Dis_StopHere;
+      dres->jk_StopHere = Ijk_Boring;
+      DIP("c.%s %s, 0x%llx\n", is_eq ? "beqz" : "bnez", nameIReg(rs1), dst_pc);
+      return True;
+   }
+
+   /* ---- RV64C compressed instruction set, quadrant 2 ----- */
+
+   /* ------------- c.slli rd_rs1, nzuimm[5:0] -------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b000) {
+      UInt rd_rs1    = INSN(11, 7);
+      UInt nzuimm5_0 = INSN(12, 12) << 5 | INSN(6, 2);
+      if (rd_rs1 == 0 || nzuimm5_0 == 0) {
+         /* Invalid C.SLLI, fall through. */
+      } else {
+         putIReg64(irsb, rd_rs1,
+                   binop(Iop_Shl64, getIReg64(rd_rs1), mkU8(nzuimm5_0)));
+         DIP("c.slli %s, %u\n", nameIReg(rd_rs1), nzuimm5_0);
+         return True;
+      }
+   }
+
+   /* -------------- c.fldsp rd, uimm[8:3](x2) -------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b001) {
+      UInt  rd      = INSN(11, 7);
+      UInt  rs1     = 2; /* base=x2/sp */
+      UInt  uimm8_3 = INSN(4, 2) << 3 | INSN(12, 12) << 2 | INSN(6, 5);
+      ULong uimm    = uimm8_3 << 3;
+      putFReg64(irsb, rd,
+                loadLE(Ity_F64, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm))));
+      DIP("c.fldsp %s, %llu(%s)\n", nameFReg(rd), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------------- c.lwsp rd, uimm[7:2](x2) --------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b010) {
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = 2; /* base=x2/sp */
+      UInt uimm7_2 = INSN(3, 2) << 4 | INSN(12, 12) << 3 | INSN(6, 4);
+      if (rd == 0) {
+         /* Invalid C.LWSP, fall through. */
+      } else {
+         ULong uimm = uimm7_2 << 2;
+         putIReg64(irsb, rd,
+                   unop(Iop_32Sto64,
+                        loadLE(Ity_I32,
+                               binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)))));
+         DIP("c.lwsp %s, %llu(%s)\n", nameIReg(rd), uimm, nameIReg(rs1));
+         return True;
+      }
+   }
+
+   /* -------------- c.ldsp rd, uimm[8:3](x2) --------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b011) {
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = 2; /* base=x2/sp */
+      UInt uimm8_3 = INSN(4, 2) << 3 | INSN(12, 12) << 2 | INSN(6, 5);
+      if (rd == 0) {
+         /* Invalid C.LDSP, fall through. */
+      } else {
+         ULong uimm = uimm8_3 << 3;
+         putIReg64(
+            irsb, rd,
+            loadLE(Ity_I64, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm))));
+         DIP("c.ldsp %s, %llu(%s)\n", nameIReg(rd), uimm, nameIReg(rs1));
+         return True;
+      }
+   }
+
+   /* ---------------------- c.jr rs1 ----------------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 12) == 0b1000) {
+      UInt rs1 = INSN(11, 7);
+      UInt rs2 = INSN(6, 2);
+      if (rs1 == 0 || rs2 != 0) {
+         /* Invalid C.JR, fall through. */
+      } else {
+         putPC(irsb, getIReg64(rs1));
+         dres->whatNext = Dis_StopHere;
+         if (rs1 == 1 /*x1/ra*/) {
+            dres->jk_StopHere = Ijk_Ret;
+            DIP("c.ret\n");
+         } else {
+            dres->jk_StopHere = Ijk_Boring;
+            DIP("c.jr %s\n", nameIReg(rs1));
+         }
+         return True;
+      }
+   }
+
+   /* -------------------- c.mv rd, rs2 --------------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 12) == 0b1000) {
+      UInt rd  = INSN(11, 7);
+      UInt rs2 = INSN(6, 2);
+      if (rd == 0 || rs2 == 0) {
+         /* Invalid C.MV, fall through. */
+      } else {
+         putIReg64(irsb, rd, getIReg64(rs2));
+         DIP("c.mv %s, %s\n", nameIReg(rd), nameIReg(rs2));
+         return True;
+      }
+   }
+
+   /* --------------------- c.jalr rs1 ---------------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 12) == 0b1001) {
+      UInt rs1 = INSN(11, 7);
+      UInt rs2 = INSN(6, 2);
+      if (rs1 == 0 || rs2 != 0) {
+         /* Invalid C.JALR, fall through. */
+      } else {
+         putIReg64(irsb, 1 /*x1/ra*/, mkU64(guest_pc_curr_instr + 2));
+         putPC(irsb, getIReg64(rs1));
+         dres->whatNext    = Dis_StopHere;
+         dres->jk_StopHere = Ijk_Call;
+         DIP("c.jalr %s\n", nameIReg(rs1));
+         return True;
+      }
+   }
+
+   /* ------------------ c.add rd_rs1, rs2 ------------------ */
+   if (INSN(1, 0) == 0b10 && INSN(15, 12) == 0b1001) {
+      UInt rd_rs1 = INSN(11, 7);
+      UInt rs2    = INSN(6, 2);
+      if (rd_rs1 == 0 || rs2 == 0) {
+         /* Invalid C.ADD, fall through. */
+      } else {
+         putIReg64(irsb, rd_rs1,
+                   binop(Iop_Add64, getIReg64(rd_rs1), getIReg64(rs2)));
+         DIP("c.add %s, %s\n", nameIReg(rd_rs1), nameIReg(rs2));
+         return True;
+      }
+   }
+
+   /* ------------- c.fsdsp rs2, uimm[8:3](x2) -------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b101) {
+      UInt  rs1     = 2; /* base=x2/sp */
+      UInt  rs2     = INSN(6, 2);
+      UInt  uimm8_3 = INSN(9, 7) << 3 | INSN(12, 10);
+      ULong uimm    = uimm8_3 << 3;
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)),
+              getFReg64(rs2));
+      DIP("c.fsdsp %s, %llu(%s)\n", nameFReg(rs2), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------------- c.swsp rs2, uimm[7:2](x2) -------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b110) {
+      UInt  rs1     = 2; /* base=x2/sp */
+      UInt  rs2     = INSN(6, 2);
+      UInt  uimm7_2 = INSN(8, 7) << 4 | INSN(12, 9);
+      ULong uimm    = uimm7_2 << 2;
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)),
+              unop(Iop_64to32, getIReg64(rs2)));
+      DIP("c.swsp %s, %llu(%s)\n", nameIReg(rs2), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------------- c.sdsp rs2, uimm[8:3](x2) -------------- */
+   if (INSN(1, 0) == 0b10 && INSN(15, 13) == 0b111) {
+      UInt  rs1     = 2; /* base=x2/sp */
+      UInt  rs2     = INSN(6, 2);
+      UInt  uimm8_3 = INSN(9, 7) << 3 | INSN(12, 10);
+      ULong uimm    = uimm8_3 << 3;
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(uimm)),
+              getIReg64(rs2));
+      DIP("c.sdsp %s, %llu(%s)\n", nameIReg(rs2), uimm, nameIReg(rs1));
+      return True;
+   }
+
+   if (sigill_diag)
+      vex_printf("RISCV64 front end: compressed\n");
+   return False;
+}
+
+static Bool dis_RV64I(/*MB_OUT*/ DisResult* dres,
+                      /*OUT*/ IRSB*         irsb,
+                      UInt                  insn,
+                      Addr                  guest_pc_curr_instr)
+{
+   /* ------------- RV64I base instruction set -------------- */
+
+   /* ----------------- lui rd, imm[31:12] ------------------ */
+   if (INSN(6, 0) == 0b0110111) {
+      UInt rd       = INSN(11, 7);
+      UInt imm31_12 = INSN(31, 12);
+      if (rd != 0)
+         putIReg64(irsb, rd, mkU64(vex_sx_to_64(imm31_12 << 12, 32)));
+      DIP("lui %s, 0x%x\n", nameIReg(rd), imm31_12);
+      return True;
+   }
+
+   /* ---------------- auipc rd, imm[31:12] ----------------- */
+   if (INSN(6, 0) == 0b0010111) {
+      UInt rd       = INSN(11, 7);
+      UInt imm31_12 = INSN(31, 12);
+      if (rd != 0)
+         putIReg64(
+            irsb, rd,
+            mkU64(guest_pc_curr_instr + vex_sx_to_64(imm31_12 << 12, 32)));
+      DIP("auipc %s, 0x%x\n", nameIReg(rd), imm31_12);
+      return True;
+   }
+
+   /* ------------------ jal rd, imm[20:1] ------------------ */
+   if (INSN(6, 0) == 0b1101111) {
+      UInt rd      = INSN(11, 7);
+      UInt imm20_1 = INSN(31, 31) << 19 | INSN(19, 12) << 11 |
+                     INSN(20, 20) << 10 | INSN(30, 21);
+      ULong simm   = vex_sx_to_64(imm20_1 << 1, 21);
+      ULong dst_pc = guest_pc_curr_instr + simm;
+      if (rd != 0)
+         putIReg64(irsb, rd, mkU64(guest_pc_curr_instr + 4));
+      putPC(irsb, mkU64(dst_pc));
+      dres->whatNext = Dis_StopHere;
+      if (rd != 0) {
+         dres->jk_StopHere = Ijk_Call;
+         DIP("jal %s, 0x%llx\n", nameIReg(rd), dst_pc);
+      } else {
+         dres->jk_StopHere = Ijk_Boring;
+         DIP("j 0x%llx\n", dst_pc);
+      }
+      return True;
+   }
+
+   /* --------------- jalr rd, imm[11:0](rs1) --------------- */
+   if (INSN(6, 0) == 0b1100111 && INSN(14, 12) == 0b000) {
+      UInt   rd      = INSN(11, 7);
+      UInt   rs1     = INSN(19, 15);
+      UInt   imm11_0 = INSN(31, 20);
+      ULong  simm    = vex_sx_to_64(imm11_0, 12);
+      IRTemp dst_pc  = newTemp(irsb, Ity_I64);
+      assign(irsb, dst_pc, binop(Iop_Add64, getIReg64(rs1), mkU64(simm)));
+      if (rd != 0)
+         putIReg64(irsb, rd, mkU64(guest_pc_curr_instr + 4));
+      putPC(irsb, mkexpr(dst_pc));
+      dres->whatNext = Dis_StopHere;
+      if (rd == 0) {
+         if (rs1 == 1 /*x1/ra*/ && simm == 0) {
+            dres->jk_StopHere = Ijk_Ret;
+            DIP("ret\n");
+         } else {
+            dres->jk_StopHere = Ijk_Boring;
+            DIP("jr %lld(%s)\n", (Long)simm, nameIReg(rs1));
+         }
+      } else {
+         dres->jk_StopHere = Ijk_Call;
+         DIP("jalr %s, %lld(%s)\n", nameIReg(rd), (Long)simm, nameIReg(rs1));
+      }
+      return True;
+   }
+
+   /* ------------ {beq,bne} rs1, rs2, imm[12:1] ------------ */
+   /* ------------ {blt,bge} rs1, rs2, imm[12:1] ------------ */
+   /* ----------- {bltu,bgeu} rs1, rs2, imm[12:1] ----------- */
+   if (INSN(6, 0) == 0b1100011) {
+      UInt funct3  = INSN(14, 12);
+      UInt rs1     = INSN(19, 15);
+      UInt rs2     = INSN(24, 20);
+      UInt imm12_1 = INSN(31, 31) << 11 | INSN(7, 7) << 10 | INSN(30, 25) << 4 |
+                     INSN(11, 8);
+      if (funct3 == 0b010 || funct3 == 0b011) {
+         /* Invalid B<x>, fall through. */
+      } else {
+         ULong        simm   = vex_sx_to_64(imm12_1 << 1, 13);
+         ULong        dst_pc = guest_pc_curr_instr + simm;
+         const HChar* name;
+         IRExpr*      cond;
+         switch (funct3) {
+         case 0b000:
+            name = "beq";
+            cond = binop(Iop_CmpEQ64, getIReg64(rs1), getIReg64(rs2));
+            break;
+         case 0b001:
+            name = "bne";
+            cond = binop(Iop_CmpNE64, getIReg64(rs1), getIReg64(rs2));
+            break;
+         case 0b100:
+            name = "blt";
+            cond = binop(Iop_CmpLT64S, getIReg64(rs1), getIReg64(rs2));
+            break;
+         case 0b101:
+            name = "bge";
+            cond = binop(Iop_CmpLE64S, getIReg64(rs2), getIReg64(rs1));
+            break;
+         case 0b110:
+            name = "bltu";
+            cond = binop(Iop_CmpLT64U, getIReg64(rs1), getIReg64(rs2));
+            break;
+         case 0b111:
+            name = "bgeu";
+            cond = binop(Iop_CmpLE64U, getIReg64(rs2), getIReg64(rs1));
+            break;
+         default:
+            vassert(0);
+         }
+         stmt(irsb,
+              IRStmt_Exit(cond, Ijk_Boring, IRConst_U64(dst_pc), OFFB_PC));
+         putPC(irsb, mkU64(guest_pc_curr_instr + 4));
+         dres->whatNext    = Dis_StopHere;
+         dres->jk_StopHere = Ijk_Boring;
+         DIP("%s %s, %s, 0x%llx\n", name, nameIReg(rs1), nameIReg(rs2), dst_pc);
+         return True;
+      }
+   }
+
+   /* ---------- {lb,lh,lw,ld} rd, imm[11:0](rs1) ----------- */
+   /* ---------- {lbu,lhu,lwu} rd, imm[11:0](rs1) ----------- */
+   if (INSN(6, 0) == 0b0000011) {
+      UInt funct3  = INSN(14, 12);
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt imm11_0 = INSN(31, 20);
+      if (funct3 == 0b111) {
+         /* Invalid L<x>, fall through. */
+      } else {
+         ULong simm = vex_sx_to_64(imm11_0, 12);
+         if (rd != 0) {
+            IRExpr* ea = binop(Iop_Add64, getIReg64(rs1), mkU64(simm));
+            IRExpr* expr;
+            switch (funct3) {
+            case 0b000:
+               expr = unop(Iop_8Sto64, loadLE(Ity_I8, ea));
+               break;
+            case 0b001:
+               expr = unop(Iop_16Sto64, loadLE(Ity_I16, ea));
+               break;
+            case 0b010:
+               expr = unop(Iop_32Sto64, loadLE(Ity_I32, ea));
+               break;
+            case 0b011:
+               expr = loadLE(Ity_I64, ea);
+               break;
+            case 0b100:
+               expr = unop(Iop_8Uto64, loadLE(Ity_I8, ea));
+               break;
+            case 0b101:
+               expr = unop(Iop_16Uto64, loadLE(Ity_I16, ea));
+               break;
+            case 0b110:
+               expr = unop(Iop_32Uto64, loadLE(Ity_I32, ea));
+               break;
+            default:
+               vassert(0);
+            }
+            putIReg64(irsb, rd, expr);
+         }
+         const HChar* name;
+         switch (funct3) {
+         case 0b000:
+            name = "lb";
+            break;
+         case 0b001:
+            name = "lh";
+            break;
+         case 0b010:
+            name = "lw";
+            break;
+         case 0b011:
+            name = "ld";
+            break;
+         case 0b100:
+            name = "lbu";
+            break;
+         case 0b101:
+            name = "lhu";
+            break;
+         case 0b110:
+            name = "lwu";
+            break;
+         default:
+            vassert(0);
+         }
+         DIP("%s %s, %lld(%s)\n", name, nameIReg(rd), (Long)simm,
+             nameIReg(rs1));
+         return True;
+      }
+   }
+
+   /* ---------- {sb,sh,sw,sd} rs2, imm[11:0](rs1) ---------- */
+   if (INSN(6, 0) == 0b0100011) {
+      UInt funct3  = INSN(14, 12);
+      UInt rs1     = INSN(19, 15);
+      UInt rs2     = INSN(24, 20);
+      UInt imm11_0 = INSN(31, 25) << 5 | INSN(11, 7);
+      if (funct3 == 0b100 || funct3 == 0b101 || funct3 == 0b110 ||
+          funct3 == 0b111) {
+         /* Invalid S<x>, fall through. */
+      } else {
+         ULong        simm = vex_sx_to_64(imm11_0, 12);
+         IRExpr*      ea   = binop(Iop_Add64, getIReg64(rs1), mkU64(simm));
+         const HChar* name;
+         IRExpr*      expr;
+         switch (funct3) {
+         case 0b000:
+            name = "sb";
+            expr = unop(Iop_64to8, getIReg64(rs2));
+            break;
+         case 0b001:
+            name = "sh";
+            expr = unop(Iop_64to16, getIReg64(rs2));
+            break;
+         case 0b010:
+            name = "sw";
+            expr = unop(Iop_64to32, getIReg64(rs2));
+            break;
+         case 0b011:
+            name = "sd";
+            expr = getIReg64(rs2);
+            break;
+         default:
+            vassert(0);
+         }
+         storeLE(irsb, ea, expr);
+         DIP("%s %s, %lld(%s)\n", name, nameIReg(rs2), (Long)simm,
+             nameIReg(rs1));
+         return True;
+      }
+   }
+
+   /* -------- {addi,slti,sltiu} rd, rs1, imm[11:0] --------- */
+   /* --------- {xori,ori,andi} rd, rs1, imm[11:0] ---------- */
+   if (INSN(6, 0) == 0b0010011) {
+      UInt funct3  = INSN(14, 12);
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt imm11_0 = INSN(31, 20);
+      if (funct3 == 0b001 || funct3 == 0b101) {
+         /* Invalid <x>I, fall through. */
+      } else {
+         ULong simm = vex_sx_to_64(imm11_0, 12);
+         if (rd != 0) {
+            IRExpr* expr;
+            switch (funct3) {
+            case 0b000:
+               expr = binop(Iop_Add64, getIReg64(rs1), mkU64(simm));
+               break;
+            case 0b010:
+               expr = unop(Iop_1Uto64,
+                           binop(Iop_CmpLT64S, getIReg64(rs1), mkU64(simm)));
+               break;
+            case 0b011:
+               /* Note that the comparison itself is unsigned but the immediate
+                  is sign-extended. */
+               expr = unop(Iop_1Uto64,
+                           binop(Iop_CmpLT64U, getIReg64(rs1), mkU64(simm)));
+               break;
+            case 0b100:
+               expr = binop(Iop_Xor64, getIReg64(rs1), mkU64(simm));
+               break;
+            case 0b110:
+               expr = binop(Iop_Or64, getIReg64(rs1), mkU64(simm));
+               break;
+            case 0b111:
+               expr = binop(Iop_And64, getIReg64(rs1), mkU64(simm));
+               break;
+            default:
+               vassert(0);
+            }
+            putIReg64(irsb, rd, expr);
+         }
+         const HChar* name;
+         switch (funct3) {
+         case 0b000:
+            name = "addi";
+            break;
+         case 0b010:
+            name = "slti";
+            break;
+         case 0b011:
+            name = "sltiu";
+            break;
+         case 0b100:
+            name = "xori";
+            break;
+         case 0b110:
+            name = "ori";
+            break;
+         case 0b111:
+            name = "andi";
+            break;
+         default:
+            vassert(0);
+         }
+         DIP("%s %s, %s, %lld\n", name, nameIReg(rd), nameIReg(rs1),
+             (Long)simm);
+         return True;
+      }
+   }
+
+   /* --------------- slli rd, rs1, uimm[5:0] --------------- */
+   if (INSN(6, 0) == 0b0010011 && INSN(14, 12) == 0b001 &&
+       INSN(31, 26) == 0b000000) {
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt uimm5_0 = INSN(25, 20);
+      if (rd != 0)
+         putIReg64(irsb, rd, binop(Iop_Shl64, getIReg64(rs1), mkU8(uimm5_0)));
+      DIP("slli %s, %s, %u\n", nameIReg(rd), nameIReg(rs1), uimm5_0);
+      return True;
+   }
+
+   /* ----------- {srli,srai} rd, rs1, uimm[5:0] ----------=- */
+   if (INSN(6, 0) == 0b0010011 && INSN(14, 12) == 0b101 &&
+       INSN(29, 26) == 0b0000 && INSN(31, 31) == 0b0) {
+      Bool is_log  = INSN(30, 30) == 0b0;
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt uimm5_0 = INSN(25, 20);
+      if (rd != 0)
+         putIReg64(irsb, rd,
+                   binop(is_log ? Iop_Shr64 : Iop_Sar64, getIReg64(rs1),
+                         mkU8(uimm5_0)));
+      DIP("%s %s, %s, %u\n", is_log ? "srli" : "srai", nameIReg(rd),
+          nameIReg(rs1), uimm5_0);
+      return True;
+   }
+
+   /* --------------- {add,sub} rd, rs1, rs2 ---------------- */
+   /* ------------- {sll,srl,sra} rd, rs1, rs2 -------------- */
+   /* --------------- {slt,sltu} rd, rs1, rs2 --------------- */
+   /* -------------- {xor,or,and} rd, rs1, rs2 -------------- */
+   if (INSN(6, 0) == 0b0110011 && INSN(29, 25) == 0b00000 &&
+       INSN(31, 31) == 0b0) {
+      UInt funct3  = INSN(14, 12);
+      Bool is_base = INSN(30, 30) == 0b0;
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt rs2     = INSN(24, 20);
+      if (!is_base && funct3 != 0b000 && funct3 != 0b101) {
+         /* Invalid <x>, fall through. */
+      } else {
+         if (rd != 0) {
+            IRExpr* expr;
+            switch (funct3) {
+            case 0b000: /* sll */
+               expr = binop(is_base ? Iop_Add64 : Iop_Sub64, getIReg64(rs1),
+                            getIReg64(rs2));
+               break;
+            case 0b001:
+               expr = binop(Iop_Shl64, getIReg64(rs1),
+                            unop(Iop_64to8, getIReg64(rs2)));
+               break;
+            case 0b010:
+               expr = unop(Iop_1Uto64,
+                           binop(Iop_CmpLT64S, getIReg64(rs1), getIReg64(rs2)));
+               break;
+            case 0b011:
+               expr = unop(Iop_1Uto64,
+                           binop(Iop_CmpLT64U, getIReg64(rs1), getIReg64(rs2)));
+               break;
+            case 0b100:
+               expr = binop(Iop_Xor64, getIReg64(rs1), getIReg64(rs2));
+               break;
+            case 0b101:
+               expr = binop(is_base ? Iop_Shr64 : Iop_Sar64, getIReg64(rs1),
+                            unop(Iop_64to8, getIReg64(rs2)));
+               break;
+            case 0b110:
+               expr = binop(Iop_Or64, getIReg64(rs1), getIReg64(rs2));
+               break;
+            case 0b111:
+               expr = binop(Iop_And64, getIReg64(rs1), getIReg64(rs2));
+               break;
+            default:
+               vassert(0);
+            }
+            putIReg64(irsb, rd, expr);
+         }
+         const HChar* name;
+         switch (funct3) {
+         case 0b000:
+            name = is_base ? "add" : "sub";
+            break;
+         case 0b001:
+            name = "sll";
+            break;
+         case 0b010:
+            name = "slt";
+            break;
+         case 0b011:
+            name = "sltu";
+            break;
+         case 0b100:
+            name = "xor";
+            break;
+         case 0b101:
+            name = is_base ? "srl" : "sra";
+            break;
+         case 0b110:
+            name = "or";
+            break;
+         case 0b111:
+            name = "and";
+            break;
+         default:
+            vassert(0);
+         }
+         DIP("%s %s, %s, %s\n", name, nameIReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+         return True;
+      }
+   }
+
+   /* ------------------------ fence ------------------------ */
+   if (INSN(19, 0) == 0b00000000000000001111 && INSN(31, 28) == 0b0000) {
+      UInt succ = INSN(23, 20);
+      UInt pred = INSN(27, 24);
+      stmt(irsb, IRStmt_MBE(Imbe_Fence));
+      if (pred == 0b1111 && succ == 0b1111)
+         DIP("fence\n");
+      else
+         DIP("fence %s%s%s%s,%s%s%s%s\n", (pred & 0x8) ? "i" : "",
+             (pred & 0x4) ? "o" : "", (pred & 0x2) ? "r" : "",
+             (pred & 0x1) ? "w" : "", (succ & 0x8) ? "i" : "",
+             (succ & 0x4) ? "o" : "", (succ & 0x2) ? "r" : "",
+             (succ & 0x1) ? "w" : "");
+      return True;
+   }
+
+   /* ------------------------ ecall ------------------------ */
+   if (INSN(31, 0) == 0b00000000000000000000000001110011) {
+      putPC(irsb, mkU64(guest_pc_curr_instr + 4));
+      dres->whatNext    = Dis_StopHere;
+      dres->jk_StopHere = Ijk_Sys_syscall;
+      DIP("ecall\n");
+      return True;
+   }
+
+   /* -------------- addiw rd, rs1, imm[11:0] --------------- */
+   if (INSN(6, 0) == 0b0011011 && INSN(14, 12) == 0b000) {
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt imm11_0 = INSN(31, 20);
+      UInt simm    = (UInt)vex_sx_to_64(imm11_0, 12);
+      if (rd != 0)
+         putIReg32(irsb, rd, binop(Iop_Add32, getIReg32(rs1), mkU32(simm)));
+      DIP("addiw %s, %s, %d\n", nameIReg(rd), nameIReg(rs1), (Int)simm);
+      return True;
+   }
+
+   /* -------------- slliw rd, rs1, uimm[4:0] --------------- */
+   if (INSN(6, 0) == 0b0011011 && INSN(14, 12) == 0b001 &&
+       INSN(31, 25) == 0b0000000) {
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt uimm4_0 = INSN(24, 20);
+      if (rd != 0)
+         putIReg32(irsb, rd, binop(Iop_Shl32, getIReg32(rs1), mkU8(uimm4_0)));
+      DIP("slliw %s, %s, %u\n", nameIReg(rd), nameIReg(rs1), uimm4_0);
+      return True;
+   }
+
+   /* ---------- {srliw,sraiw} rd, rs1, uimm[4:0] ----------- */
+   if (INSN(6, 0) == 0b0011011 && INSN(14, 12) == 0b101 &&
+       INSN(29, 25) == 0b00000 && INSN(31, 31) == 0b0) {
+      Bool is_log  = INSN(30, 30) == 0b0;
+      UInt rd      = INSN(11, 7);
+      UInt rs1     = INSN(19, 15);
+      UInt uimm4_0 = INSN(24, 20);
+      if (rd != 0)
+         putIReg32(irsb, rd,
+                   binop(is_log ? Iop_Shr32 : Iop_Sar32, getIReg32(rs1),
+                         mkU8(uimm4_0)));
+      DIP("%s %s, %s, %u\n", is_log ? "srliw" : "sraiw", nameIReg(rd),
+          nameIReg(rs1), uimm4_0);
+      return True;
+   }
+
+   /* -------------- {addw,subw} rd, rs1, rs2 --------------- */
+   if (INSN(6, 0) == 0b0111011 && INSN(14, 12) == 0b000 &&
+       INSN(29, 25) == 0b00000 && INSN(31, 31) == 0b0) {
+      Bool is_add = INSN(30, 30) == 0b0;
+      UInt rd     = INSN(11, 7);
+      UInt rs1    = INSN(19, 15);
+      UInt rs2    = INSN(24, 20);
+      if (rd != 0)
+         putIReg32(irsb, rd,
+                   binop(is_add ? Iop_Add32 : Iop_Sub32, getIReg32(rs1),
+                         getIReg32(rs2)));
+      DIP("%s %s, %s, %s\n", is_add ? "addw" : "subw", nameIReg(rd),
+          nameIReg(rs1), nameIReg(rs2));
+      return True;
+   }
+
+   /* ------------------ sllw rd, rs1, rs2 ------------------ */
+   if (INSN(6, 0) == 0b0111011 && INSN(14, 12) == 0b001 &&
+       INSN(31, 25) == 0b0000000) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rd != 0)
+         putIReg32(
+            irsb, rd,
+            binop(Iop_Shl32, getIReg32(rs1), unop(Iop_64to8, getIReg64(rs2))));
+      DIP("sllw %s, %s, %s\n", nameIReg(rd), nameIReg(rs1), nameIReg(rs2));
+      return True;
+   }
+
+   /* -------------- {srlw,sraw} rd, rs1, rs2 --------------- */
+   if (INSN(6, 0) == 0b0111011 && INSN(14, 12) == 0b101 &&
+       INSN(29, 25) == 0b00000 && INSN(31, 31) == 0b0) {
+      Bool is_log = INSN(30, 30) == 0b0;
+      UInt rd     = INSN(11, 7);
+      UInt rs1    = INSN(19, 15);
+      UInt rs2    = INSN(24, 20);
+      if (rd != 0)
+         putIReg32(irsb, rd,
+                   binop(is_log ? Iop_Shr32 : Iop_Sar32, getIReg32(rs1),
+                         unop(Iop_64to8, getIReg64(rs2))));
+      DIP("%s %s, %s, %s\n", is_log ? "srlw" : "sraw", nameIReg(rd),
+          nameIReg(rs1), nameIReg(rs2));
+      return True;
+   }
+
+   return False;
+}
+
+static Bool dis_RV64M(/*MB_OUT*/ DisResult* dres,
+                      /*OUT*/ IRSB*         irsb,
+                      UInt                  insn)
+{
+   /* -------------- RV64M standard extension --------------- */
+
+   /* -------- {mul,mulh,mulhsu,mulhu} rd, rs1, rs2 --------- */
+   /* --------------- {div,divu} rd, rs1, rs2 --------------- */
+   /* --------------- {rem,remu} rd, rs1, rs2 --------------- */
+   if (INSN(6, 0) == 0b0110011 && INSN(31, 25) == 0b0000001) {
+      UInt rd     = INSN(11, 7);
+      UInt funct3 = INSN(14, 12);
+      UInt rs1    = INSN(19, 15);
+      UInt rs2    = INSN(24, 20);
+      /* TODO Handle mulhsu. */
+      if (funct3 == 0b010) {
+         /* Invalid {MUL,DIV,REM}<x>, fall through. */
+      } else {
+         if (rd != 0) {
+            IRExpr* expr;
+            switch (funct3) {
+            case 0b000:
+               expr = binop(Iop_Mul64, getIReg64(rs1), getIReg64(rs2));
+               break;
+            case 0b001:
+               expr = unop(Iop_128HIto64,
+                           binop(Iop_MullS64, getIReg64(rs1), getIReg64(rs2)));
+               break;
+            case 0b010:
+               /* TODO */
+               vassert(0);
+               break;
+            case 0b011:
+               expr = unop(Iop_128HIto64,
+                           binop(Iop_MullU64, getIReg64(rs1), getIReg64(rs2)));
+               break;
+            case 0b100:
+               expr = binop(Iop_DivS64, getIReg64(rs1), getIReg64(rs2));
+               break;
+            case 0b101:
+               expr = binop(Iop_DivU64, getIReg64(rs1), getIReg64(rs2));
+               break;
+            case 0b110:
+               expr =
+                  unop(Iop_128HIto64, binop(Iop_DivModS64to64, getIReg64(rs1),
+                                            getIReg64(rs2)));
+               break;
+            case 0b111:
+               expr =
+                  unop(Iop_128HIto64, binop(Iop_DivModU64to64, getIReg64(rs1),
+                                            getIReg64(rs2)));
+               break;
+            default:
+               vassert(0);
+            }
+            putIReg64(irsb, rd, expr);
+         }
+         const HChar* name;
+         switch (funct3) {
+         case 0b000:
+            name = "mul";
+            break;
+         case 0b001:
+            name = "mulh";
+            break;
+         case 0b010:
+            name = "mulhsu";
+            break;
+         case 0b011:
+            name = "mulhu";
+            break;
+         case 0b100:
+            name = "div";
+            break;
+         case 0b101:
+            name = "divu";
+            break;
+         case 0b110:
+            name = "rem";
+            break;
+         case 0b111:
+            name = "remu";
+            break;
+         default:
+            vassert(0);
+         }
+         DIP("%s %s, %s, %s\n", name, nameIReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+         return True;
+      }
+   }
+
+   /* ------------------ mulw rd, rs1, rs2 ------------------ */
+   /* -------------- {divw,divuw} rd, rs1, rs2 -------------- */
+   /* -------------- {remw,remuw} rd, rs1, rs2 -------------- */
+   if (INSN(6, 0) == 0b0111011 && INSN(31, 25) == 0b0000001) {
+      UInt rd     = INSN(11, 7);
+      UInt funct3 = INSN(14, 12);
+      UInt rs1    = INSN(19, 15);
+      UInt rs2    = INSN(24, 20);
+      if (funct3 == 0b001 || funct3 == 0b010 || funct3 == 0b011) {
+         /* Invalid {MUL,DIV,REM}<x>W, fall through. */
+      } else {
+         if (rd != 0) {
+            IRExpr* expr;
+            switch (funct3) {
+            case 0b000:
+               expr = binop(Iop_Mul32, getIReg32(rs1), getIReg32(rs2));
+               break;
+            case 0b100:
+               expr = binop(Iop_DivS32, getIReg32(rs1), getIReg32(rs2));
+               break;
+            case 0b101:
+               expr = binop(Iop_DivU32, getIReg32(rs1), getIReg32(rs2));
+               break;
+            case 0b110:
+               expr = unop(Iop_64HIto32, binop(Iop_DivModS32to32,
+                                               getIReg32(rs1), getIReg32(rs2)));
+               break;
+            case 0b111:
+               expr = unop(Iop_64HIto32, binop(Iop_DivModU32to32,
+                                               getIReg32(rs1), getIReg32(rs2)));
+               break;
+            default:
+               vassert(0);
+            }
+            putIReg32(irsb, rd, expr);
+         }
+         const HChar* name;
+         switch (funct3) {
+         case 0b000:
+            name = "mulw";
+            break;
+         case 0b100:
+            name = "divw";
+            break;
+         case 0b101:
+            name = "divuw";
+            break;
+         case 0b110:
+            name = "remw";
+            break;
+         case 0b111:
+            name = "remuw";
+            break;
+         default:
+            vassert(0);
+         }
+         DIP("%s %s, %s, %s\n", name, nameIReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+         return True;
+      }
+   }
+
+   return False;
+}
+
+static Bool dis_RV64A(/*MB_OUT*/ DisResult* dres,
+                      /*OUT*/ IRSB*         irsb,
+                      UInt                  insn,
+                      Addr                  guest_pc_curr_instr,
+                      const VexAbiInfo*     abiinfo)
+{
+   /* -------------- RV64A standard extension --------------- */
+
+   /* ----------------- lr.{w,d} rd, (rs1) ------------------ */
+   if (INSN(6, 0) == 0b0101111 && INSN(14, 13) == 0b01 &&
+       INSN(24, 20) == 0b00000 && INSN(31, 27) == 0b00010) {
+      UInt rd    = INSN(11, 7);
+      Bool is_32 = INSN(12, 12) == 0b0;
+      UInt rs1   = INSN(19, 15);
+      UInt aqrl  = INSN(26, 25);
+
+      if (aqrl & 0x1)
+         stmt(irsb, IRStmt_MBE(Imbe_Fence));
+
+      IRType ty = is_32 ? Ity_I32 : Ity_I64;
+      if (abiinfo->guest__use_fallback_LLSC) {
+         /* Get address of the load. */
+         IRTemp ea = newTemp(irsb, Ity_I64);
+         assign(irsb, ea, getIReg64(rs1));
+
+         /* Load the value. */
+         IRTemp res = newTemp(irsb, Ity_I64);
+         assign(irsb, res, widenSto64(ty, loadLE(ty, mkexpr(ea))));
+
+         /* Set up the LLSC fallback data. */
+         stmt(irsb, IRStmt_Put(OFFB_LLSC_DATA, mkexpr(res)));
+         stmt(irsb, IRStmt_Put(OFFB_LLSC_ADDR, mkexpr(ea)));
+         stmt(irsb, IRStmt_Put(OFFB_LLSC_SIZE, mkU64(4)));
+
+         /* Write the result to the destination register. */
+         if (rd != 0)
+            putIReg64(irsb, rd, mkexpr(res));
+      } else {
+         /* TODO Rework the non-fallback mode by recognizing common LR+SC
+            sequences and simulating them as one. */
+         IRTemp res = newTemp(irsb, ty);
+         stmt(irsb, IRStmt_LLSC(Iend_LE, res, getIReg64(rs1), NULL /*LL*/));
+         if (rd != 0)
+            putIReg64(irsb, rd, widenSto64(ty, mkexpr(res)));
+      }
+
+      if (aqrl & 0x2)
+         stmt(irsb, IRStmt_MBE(Imbe_Fence));
+
+      DIP("lr.%s%s %s, (%s)%s\n", is_32 ? "w" : "d", nameAqRlSuffix(aqrl),
+          nameIReg(rd), nameIReg(rs1),
+          abiinfo->guest__use_fallback_LLSC ? " (fallback implementation)"
+                                            : "");
+      return True;
+   }
+
+   /* --------------- sc.{w,d} rd, rs2, (rs1) --------------- */
+   if (INSN(6, 0) == 0b0101111 && INSN(14, 13) == 0b01 &&
+       INSN(31, 27) == 0b00011) {
+      UInt rd    = INSN(11, 7);
+      Bool is_32 = INSN(12, 12) == 0b0;
+      UInt rs1   = INSN(19, 15);
+      UInt rs2   = INSN(24, 20);
+      UInt aqrl  = INSN(26, 25);
+
+      if (aqrl & 0x1)
+         stmt(irsb, IRStmt_MBE(Imbe_Fence));
+
+      IRType ty = is_32 ? Ity_I32 : Ity_I64;
+      if (abiinfo->guest__use_fallback_LLSC) {
+         /* Get address of the load. */
+         IRTemp ea = newTemp(irsb, Ity_I64);
+         assign(irsb, ea, getIReg64(rs1));
+
+         /* Get the continuation address. */
+         IRConst* nia = IRConst_U64(guest_pc_curr_instr + 4);
+
+         /* Mark the SC initially as failed. */
+         if (rd != 0)
+            putIReg64(irsb, rd, mkU64(1));
+
+         /* Set that no transaction is in progress. */
+         IRTemp size = newTemp(irsb, Ity_I64);
+         assign(irsb, size, IRExpr_Get(OFFB_LLSC_SIZE, Ity_I64));
+         stmt(irsb,
+              IRStmt_Put(OFFB_LLSC_SIZE, mkU64(0) /* "no transaction" */));
+
+         /* Fail if no or wrong-size transaction. */
+         stmt(irsb, IRStmt_Exit(binop(Iop_CmpNE64, mkexpr(size), mkU64(4)),
+                                Ijk_Boring, nia, OFFB_PC));
+
+         /* Fail if the address doesn't match the LL address. */
+         stmt(irsb, IRStmt_Exit(binop(Iop_CmpNE64, mkexpr(ea),
+                                      IRExpr_Get(OFFB_LLSC_ADDR, Ity_I64)),
+                                Ijk_Boring, nia, OFFB_PC));
+
+         /* Fail if the data doesn't match the LL data. */
+         IRTemp data = newTemp(irsb, Ity_I64);
+         assign(irsb, data, IRExpr_Get(OFFB_LLSC_DATA, Ity_I64));
+         stmt(irsb, IRStmt_Exit(binop(Iop_CmpNE64,
+                                      widenSto64(ty, loadLE(ty, mkexpr(ea))),
+                                      mkexpr(data)),
+                                Ijk_Boring, nia, OFFB_PC));
+
+         /* Try to CAS the new value in. */
+         IRTemp old  = newTemp(irsb, ty);
+         IRTemp expd = newTemp(irsb, ty);
+         assign(irsb, expd, narrowFrom64(ty, mkexpr(data)));
+         stmt(irsb, IRStmt_CAS(mkIRCAS(
+                       /*oldHi*/ IRTemp_INVALID, old, Iend_LE, mkexpr(ea),
+                       /*expdHi*/ NULL, mkexpr(expd),
+                       /*dataHi*/ NULL, narrowFrom64(ty, getIReg64(rs2)))));
+
+         /* Fail if the CAS failed (old != expd). */
+         stmt(irsb, IRStmt_Exit(binop(is_32 ? Iop_CmpNE32 : Iop_CmpNE64,
+                                      mkexpr(old), mkexpr(expd)),
+                                Ijk_Boring, nia, OFFB_PC));
+
+         /* Otherwise mark the operation as successful. */
+         if (rd != 0)
+            putIReg64(irsb, rd, mkU64(0));
+      } else {
+         IRTemp res = newTemp(irsb, Ity_I1);
+         stmt(irsb, IRStmt_LLSC(Iend_LE, res, getIReg64(rs1),
+                                narrowFrom64(ty, getIReg64(rs2))));
+         /* IR semantics: res is 1 if store succeeds, 0 if it fails. Need to set
+            rd to 1 on failure, 0 on success. */
+         if (rd != 0)
+            putIReg64(
+               irsb, rd,
+               binop(Iop_Xor64, unop(Iop_1Uto64, mkexpr(res)), mkU64(1)));
+      }
+
+      if (aqrl & 0x2)
+         stmt(irsb, IRStmt_MBE(Imbe_Fence));
+
+      DIP("sc.%s%s %s, %s, (%s)%s\n", is_32 ? "w" : "d", nameAqRlSuffix(aqrl),
+          nameIReg(rd), nameIReg(rs2), nameIReg(rs1),
+          abiinfo->guest__use_fallback_LLSC ? " (fallback implementation)"
+                                            : "");
+      return True;
+   }
+
+   /* --------- amo{swap,add}.{w,d} rd, rs2, (rs1) ---------- */
+   /* -------- amo{xor,and,or}.{w,d} rd, rs2, (rs1) --------- */
+   /* ---------- amo{min,max}.{w,d} rd, rs2, (rs1) ---------- */
+   /* --------- amo{minu,maxu}.{w,d} rd, rs2, (rs1) --------- */
+   if (INSN(6, 0) == 0b0101111 && INSN(14, 13) == 0b01) {
+      UInt rd     = INSN(11, 7);
+      Bool is_32  = INSN(12, 12) == 0b0;
+      UInt rs1    = INSN(19, 15);
+      UInt rs2    = INSN(24, 20);
+      UInt aqrl   = INSN(26, 25);
+      UInt funct5 = INSN(31, 27);
+      if ((funct5 & 0b00010) || funct5 == 0b00101 || funct5 == 0b01001 ||
+          funct5 == 0b01101 || funct5 == 0b10001 || funct5 == 0b10101 ||
+          funct5 == 0b11001 || funct5 == 0b11101) {
+         /* Invalid AMO<x>, fall through. */
+      } else {
+         if (aqrl & 0x1)
+            stmt(irsb, IRStmt_MBE(Imbe_Fence));
+
+         IRTemp addr = newTemp(irsb, Ity_I64);
+         assign(irsb, addr, getIReg64(rs1));
+
+         IRType ty   = is_32 ? Ity_I32 : Ity_I64;
+         IRTemp orig = newTemp(irsb, ty);
+         assign(irsb, orig, loadLE(ty, mkexpr(addr)));
+         IRExpr* lhs = mkexpr(orig);
+         IRExpr* rhs = narrowFrom64(ty, getIReg64(rs2));
+
+         /* Perform the operation. */
+         const HChar* name;
+         IRExpr*      res;
+         switch (funct5) {
+         case 0b00001:
+            name = "amoswap";
+            res  = rhs;
+            break;
+         case 0b00000:
+            name = "amoadd";
+            res  = binop(is_32 ? Iop_Add32 : Iop_Add64, lhs, rhs);
+            break;
+         case 0b00100:
+            name = "amoxor";
+            res  = binop(is_32 ? Iop_Xor32 : Iop_Xor64, lhs, rhs);
+            break;
+         case 0b01100:
+            name = "amoand";
+            res  = binop(is_32 ? Iop_And32 : Iop_And64, lhs, rhs);
+            break;
+         case 0b01000:
+            name = "amoor";
+            res  = binop(is_32 ? Iop_Or32 : Iop_Or64, lhs, rhs);
+            break;
+         case 0b10000:
+            name = "amomin";
+            res  = IRExpr_ITE(
+                binop(is_32 ? Iop_CmpLT32S : Iop_CmpLT64S, lhs, rhs), lhs, rhs);
+            break;
+         case 0b10100:
+            name = "amomax";
+            res  = IRExpr_ITE(
+                binop(is_32 ? Iop_CmpLT32S : Iop_CmpLT64S, lhs, rhs), rhs, lhs);
+            break;
+         case 0b11000:
+            name = "amominu";
+            res  = IRExpr_ITE(
+                binop(is_32 ? Iop_CmpLT32U : Iop_CmpLT64U, lhs, rhs), lhs, rhs);
+            break;
+         case 0b11100:
+            name = "amomaxu";
+            res  = IRExpr_ITE(
+                binop(is_32 ? Iop_CmpLT32U : Iop_CmpLT64U, lhs, rhs), rhs, lhs);
+            break;
+         default:
+            vassert(0);
+         }
+
+         /* Store the result back if the original value remains unchanged in
+            memory. */
+         IRTemp old = newTemp(irsb, ty);
+         stmt(irsb, IRStmt_CAS(mkIRCAS(/*oldHi*/ IRTemp_INVALID, old, Iend_LE,
+                                       mkexpr(addr),
+                                       /*expdHi*/ NULL, mkexpr(orig),
+                                       /*dataHi*/ NULL, res)));
+
+         if (aqrl & 0x2)
+            stmt(irsb, IRStmt_MBE(Imbe_Fence));
+
+         /* Retry if the CAS failed (i.e. when old != orig). */
+         stmt(irsb, IRStmt_Exit(binop(is_32 ? Iop_CasCmpNE32 : Iop_CasCmpNE64,
+                                      mkexpr(old), mkexpr(orig)),
+                                Ijk_Boring, IRConst_U64(guest_pc_curr_instr),
+                                OFFB_PC));
+         /* Otherwise we succeeded. */
+         if (rd != 0)
+            putIReg64(irsb, rd, widenSto64(ty, mkexpr(old)));
+
+         DIP("%s.%s%s %s, %s, (%s)\n", name, is_32 ? "w" : "d",
+             nameAqRlSuffix(aqrl), nameIReg(rd), nameIReg(rs2), nameIReg(rs1));
+         return True;
+      }
+   }
+
+   return False;
+}
+
+static Bool dis_RV64F(/*MB_OUT*/ DisResult* dres,
+                      /*OUT*/ IRSB*         irsb,
+                      UInt                  insn)
+{
+   /* -------------- RV64F standard extension --------------- */
+
+   /* --------------- flw rd, imm[11:0](rs1) ---------------- */
+   if (INSN(6, 0) == 0b0000111 && INSN(14, 12) == 0b010) {
+      UInt  rd      = INSN(11, 7);
+      UInt  rs1     = INSN(19, 15);
+      UInt  imm11_0 = INSN(31, 20);
+      ULong simm    = vex_sx_to_64(imm11_0, 12);
+      putFReg32(irsb, rd,
+                loadLE(Ity_F32, binop(Iop_Add64, getIReg64(rs1), mkU64(simm))));
+      DIP("flw %s, %lld(%s)\n", nameFReg(rd), (Long)simm, nameIReg(rs1));
+      return True;
+   }
+
+   /* --------------- fsw rs2, imm[11:0](rs1) --------------- */
+   if (INSN(6, 0) == 0b0100111 && INSN(14, 12) == 0b010) {
+      UInt  rs1     = INSN(19, 15);
+      UInt  rs2     = INSN(24, 20);
+      UInt  imm11_0 = INSN(31, 25) << 5 | INSN(11, 7);
+      ULong simm    = vex_sx_to_64(imm11_0, 12);
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(simm)),
+              getFReg32(rs2));
+      DIP("fsw %s, %lld(%s)\n", nameFReg(rs2), (Long)simm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------- f{madd,msub}.s rd, rs1, rs2, rs3, rm --------- */
+   /* ------- f{nmsub,nmadd}.s rd, rs1, rs2, rs3, rm -------- */
+   if (INSN(1, 0) == 0b11 && INSN(6, 4) == 0b100 && INSN(26, 25) == 0b00) {
+      UInt   opcode = INSN(6, 0);
+      UInt   rd     = INSN(11, 7);
+      UInt   rm     = INSN(14, 12);
+      UInt   rs1    = INSN(19, 15);
+      UInt   rs2    = INSN(24, 20);
+      UInt   rs3    = INSN(31, 27);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      const HChar* name;
+      IRTemp       a1 = newTemp(irsb, Ity_F32);
+      IRTemp       a2 = newTemp(irsb, Ity_F32);
+      IRTemp       a3 = newTemp(irsb, Ity_F32);
+      switch (opcode) {
+      case 0b1000011:
+         name = "fmadd";
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, getFReg32(rs2));
+         assign(irsb, a3, getFReg32(rs3));
+         break;
+      case 0b1000111:
+         name = "fmsub";
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, getFReg32(rs2));
+         assign(irsb, a3, unop(Iop_NegF32, getFReg32(rs3)));
+         break;
+      case 0b1001011:
+         name = "fnmsub";
+         assign(irsb, a1, unop(Iop_NegF32, getFReg32(rs1)));
+         assign(irsb, a2, getFReg32(rs2));
+         assign(irsb, a3, getFReg32(rs3));
+         break;
+      case 0b1001111:
+         name = "fnmadd";
+         assign(irsb, a1, unop(Iop_NegF32, getFReg32(rs1)));
+         assign(irsb, a2, getFReg32(rs2));
+         assign(irsb, a3, unop(Iop_NegF32, getFReg32(rs3)));
+         break;
+      default:
+         vassert(0);
+      }
+      putFReg32(
+         irsb, rd,
+         qop(Iop_MAddF32, mkexpr(rm_IR), mkexpr(a1), mkexpr(a2), mkexpr(a3)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(Ity_I32, 0 /*regparms*/,
+                                        "riscv64g_calculate_fflags_fmadd_s",
+                                        riscv64g_calculate_fflags_fmadd_s,
+                                        mkIRExprVec_4(mkexpr(a1), mkexpr(a2),
+                                                      mkexpr(a3),
+                                                      mkexpr(rm_RISCV)))));
+      DIP("%s.s %s, %s, %s, %s%s\n", name, nameFReg(rd), nameFReg(rs1),
+          nameFReg(rs2), nameFReg(rs3), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ------------ f{add,sub}.s rd, rs1, rs2, rm ------------ */
+   /* ------------ f{mul,div}.s rd, rs1, rs2, rm ------------ */
+   if (INSN(6, 0) == 0b1010011 && INSN(26, 25) == 0b00 &&
+       INSN(31, 29) == 0b000) {
+      UInt   rd     = INSN(11, 7);
+      UInt   rm     = INSN(14, 12);
+      UInt   rs1    = INSN(19, 15);
+      UInt   rs2    = INSN(24, 20);
+      UInt   funct7 = INSN(31, 25);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      const HChar* name;
+      IROp         op;
+      IRTemp       a1 = newTemp(irsb, Ity_F32);
+      IRTemp       a2 = newTemp(irsb, Ity_F32);
+      const HChar* helper_name;
+      void*        helper_addr;
+      switch (funct7) {
+      case 0b0000000:
+         name = "fadd";
+         op   = Iop_AddF32;
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, getFReg32(rs2));
+         helper_name = "riscv64g_calculate_fflags_fadd_s";
+         helper_addr = riscv64g_calculate_fflags_fadd_s;
+         break;
+      case 0b0000100:
+         name = "fsub";
+         op   = Iop_AddF32;
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, unop(Iop_NegF32, getFReg32(rs2)));
+         helper_name = "riscv64g_calculate_fflags_fadd_s";
+         helper_addr = riscv64g_calculate_fflags_fadd_s;
+         break;
+      case 0b0001000:
+         name = "fmul";
+         op   = Iop_MulF32;
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, getFReg32(rs2));
+         helper_name = "riscv64g_calculate_fflags_fmul_s";
+         helper_addr = riscv64g_calculate_fflags_fmul_s;
+         break;
+      case 0b0001100:
+         name = "fdiv";
+         op   = Iop_DivF32;
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, getFReg32(rs2));
+         helper_name = "riscv64g_calculate_fflags_fdiv_s";
+         helper_addr = riscv64g_calculate_fflags_fdiv_s;
+         break;
+      default:
+         vassert(0);
+      }
+      putFReg32(irsb, rd, triop(op, mkexpr(rm_IR), mkexpr(a1), mkexpr(a2)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(Ity_I32, 0 /*regparms*/, helper_name,
+                                        helper_addr,
+                                        mkIRExprVec_3(mkexpr(a1), mkexpr(a2),
+                                                      mkexpr(rm_RISCV)))));
+      DIP("%s.s %s, %s, %s%s\n", name, nameFReg(rd), nameFReg(rs1),
+          nameFReg(rs2), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ----------------- fsqrt.s rd, rs1, rm ----------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 20) == 0b00000 &&
+       INSN(31, 25) == 0b0101100) {
+      UInt   rd  = INSN(11, 7);
+      UInt   rm  = INSN(14, 12);
+      UInt   rs1 = INSN(19, 15);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      IRTemp a1 = newTemp(irsb, Ity_F32);
+      assign(irsb, a1, getFReg32(rs1));
+      putFReg32(irsb, rd, binop(Iop_SqrtF32, mkexpr(rm_IR), mkexpr(a1)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(
+                             Ity_I32, 0 /*regparms*/,
+                             "riscv64g_calculate_fflags_fsqrt_s",
+                             riscv64g_calculate_fflags_fsqrt_s,
+                             mkIRExprVec_2(mkexpr(a1), mkexpr(rm_RISCV)))));
+      DIP("fsqrt.s %s, %s%s\n", nameFReg(rd), nameFReg(rs1), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ---------------- fsgnj.s rd, rs1, rs2 ----------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b000 &&
+       INSN(31, 25) == 0b0010000) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rs1 == rs2) {
+         putFReg32(irsb, rd, getFReg32(rs1));
+         DIP("fmv.s %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      } else {
+         putFReg32(
+            irsb, rd,
+            unop(Iop_ReinterpI32asF32,
+                 binop(
+                    Iop_Or32,
+                    binop(Iop_And32, unop(Iop_ReinterpF32asI32, getFReg32(rs1)),
+                          mkU32(0x7fffffff)),
+                    binop(Iop_And32, unop(Iop_ReinterpF32asI32, getFReg32(rs2)),
+                          mkU32(0x80000000)))));
+         DIP("fsgnj.s %s, %s, %s\n", nameFReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+      }
+      return True;
+   }
+
+   /* ---------------- fsgnjn.s rd, rs1, rs2 ---------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b001 &&
+       INSN(31, 25) == 0b0010000) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rs1 == rs2) {
+         putFReg32(irsb, rd, unop(Iop_NegF32, getFReg32(rs1)));
+         DIP("fneg.s %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      } else {
+         putFReg32(irsb, rd,
+                   unop(Iop_ReinterpI32asF32,
+                        binop(Iop_Or32,
+                              binop(Iop_And32,
+                                    unop(Iop_ReinterpF32asI32, getFReg32(rs1)),
+                                    mkU32(0x7fffffff)),
+                              binop(Iop_And32,
+                                    unop(Iop_ReinterpF32asI32,
+                                         unop(Iop_NegF32, getFReg32(rs2))),
+                                    mkU32(0x80000000)))));
+         DIP("fsgnjn.s %s, %s, %s\n", nameFReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+      }
+      return True;
+   }
+
+   /* ---------------- fsgnjx.s rd, rs1, rs2 ---------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b010 &&
+       INSN(31, 25) == 0b0010000) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rs1 == rs2) {
+         putFReg32(irsb, rd, unop(Iop_AbsF32, getFReg32(rs1)));
+         DIP("fabs.s %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      } else {
+         putFReg32(
+            irsb, rd,
+            unop(Iop_ReinterpI32asF32,
+                 binop(Iop_Xor32, unop(Iop_ReinterpF32asI32, getFReg32(rs1)),
+                       binop(Iop_And32,
+                             unop(Iop_ReinterpF32asI32, getFReg32(rs2)),
+                             mkU32(0x80000000)))));
+         DIP("fsgnjx.s %s, %s, %s\n", nameFReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+      }
+      return True;
+   }
+
+   /* -------------- f{min,max}.s rd, rs1, rs2 -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(31, 25) == 0b0010100) {
+      UInt rd  = INSN(11, 7);
+      UInt rm  = INSN(14, 12);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rm != 0b000 && rm != 0b001) {
+         /* Invalid F{MIN,MAX}.S, fall through. */
+      } else {
+         const HChar* name;
+         IROp         op;
+         const HChar* helper_name;
+         void*        helper_addr;
+         switch (rm) {
+         case 0b000:
+            name        = "fmin";
+            op          = Iop_MinNumF32;
+            helper_name = "riscv64g_calculate_fflags_fmin_s";
+            helper_addr = riscv64g_calculate_fflags_fmin_s;
+            break;
+         case 0b001:
+            name        = "fmax";
+            op          = Iop_MaxNumF32;
+            helper_name = "riscv64g_calculate_fflags_fmax_s";
+            helper_addr = riscv64g_calculate_fflags_fmax_s;
+            break;
+         default:
+            vassert(0);
+         }
+         IRTemp a1 = newTemp(irsb, Ity_F32);
+         IRTemp a2 = newTemp(irsb, Ity_F32);
+         assign(irsb, a1, getFReg32(rs1));
+         assign(irsb, a2, getFReg32(rs2));
+         putFReg32(irsb, rd, binop(op, mkexpr(a1), mkexpr(a2)));
+         putFCSR(irsb,
+                 binop(Iop_Or32, getFCSR(),
+                       mkIRExprCCall(Ity_I32, 0 /*regparms*/, helper_name,
+                                     helper_addr,
+                                     mkIRExprVec_2(mkexpr(a1), mkexpr(a2)))));
+         DIP("%s.s %s, %s, %s\n", name, nameFReg(rd), nameFReg(rs1),
+             nameFReg(rs2));
+         return True;
+      }
+   }
+
+   return False;
+}
+
+static Bool dis_RV64D(/*MB_OUT*/ DisResult* dres,
+                      /*OUT*/ IRSB*         irsb,
+                      UInt                  insn)
+{
+   /* -------------- RV64D standard extension --------------- */
+
+   /* --------------- fld rd, imm[11:0](rs1) ---------------- */
+   if (INSN(6, 0) == 0b0000111 && INSN(14, 12) == 0b011) {
+      UInt  rd      = INSN(11, 7);
+      UInt  rs1     = INSN(19, 15);
+      UInt  imm11_0 = INSN(31, 20);
+      ULong simm    = vex_sx_to_64(imm11_0, 12);
+      putFReg64(irsb, rd,
+                loadLE(Ity_F64, binop(Iop_Add64, getIReg64(rs1), mkU64(simm))));
+      DIP("fld %s, %lld(%s)\n", nameFReg(rd), (Long)simm, nameIReg(rs1));
+      return True;
+   }
+
+   /* --------------- fsd rs2, imm[11:0](rs1) --------------- */
+   if (INSN(6, 0) == 0b0100111 && INSN(14, 12) == 0b011) {
+      UInt  rs1     = INSN(19, 15);
+      UInt  rs2     = INSN(24, 20);
+      UInt  imm11_0 = INSN(31, 25) << 5 | INSN(11, 7);
+      ULong simm    = vex_sx_to_64(imm11_0, 12);
+      storeLE(irsb, binop(Iop_Add64, getIReg64(rs1), mkU64(simm)),
+              getFReg64(rs2));
+      DIP("fsd %s, %lld(%s)\n", nameFReg(rs2), (Long)simm, nameIReg(rs1));
+      return True;
+   }
+
+   /* -------- f{madd,msub}.d rd, rs1, rs2, rs3, rm --------- */
+   /* ------- f{nmsub,nmadd}.d rd, rs1, rs2, rs3, rm -------- */
+   if (INSN(1, 0) == 0b11 && INSN(6, 4) == 0b100 && INSN(26, 25) == 0b01) {
+      UInt   opcode = INSN(6, 0);
+      UInt   rd     = INSN(11, 7);
+      UInt   rm     = INSN(14, 12);
+      UInt   rs1    = INSN(19, 15);
+      UInt   rs2    = INSN(24, 20);
+      UInt   rs3    = INSN(31, 27);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      const HChar* name;
+      IRTemp       a1 = newTemp(irsb, Ity_F64);
+      IRTemp       a2 = newTemp(irsb, Ity_F64);
+      IRTemp       a3 = newTemp(irsb, Ity_F64);
+      switch (opcode) {
+      case 0b1000011:
+         name = "fmadd";
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         assign(irsb, a3, getFReg64(rs3));
+         break;
+      case 0b1000111:
+         name = "fmsub";
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         assign(irsb, a3, unop(Iop_NegF64, getFReg64(rs3)));
+         break;
+      case 0b1001011:
+         name = "fnmsub";
+         assign(irsb, a1, unop(Iop_NegF64, getFReg64(rs1)));
+         assign(irsb, a2, getFReg64(rs2));
+         assign(irsb, a3, getFReg64(rs3));
+         break;
+      case 0b1001111:
+         name = "fnmadd";
+         assign(irsb, a1, unop(Iop_NegF64, getFReg64(rs1)));
+         assign(irsb, a2, getFReg64(rs2));
+         assign(irsb, a3, unop(Iop_NegF64, getFReg64(rs3)));
+         break;
+      default:
+         vassert(0);
+      }
+      putFReg64(
+         irsb, rd,
+         qop(Iop_MAddF64, mkexpr(rm_IR), mkexpr(a1), mkexpr(a2), mkexpr(a3)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(Ity_I32, 0 /*regparms*/,
+                                        "riscv64g_calculate_fflags_fmadd_d",
+                                        riscv64g_calculate_fflags_fmadd_d,
+                                        mkIRExprVec_4(mkexpr(a1), mkexpr(a2),
+                                                      mkexpr(a3),
+                                                      mkexpr(rm_RISCV)))));
+      DIP("%s.d %s, %s, %s, %s%s\n", name, nameFReg(rd), nameFReg(rs1),
+          nameFReg(rs2), nameFReg(rs3), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ------------ f{add,sub}.d rd, rs1, rs2, rm ------------ */
+   /* ------------ f{mul,div}.d rd, rs1, rs2, rm ------------ */
+   if (INSN(6, 0) == 0b1010011 && INSN(26, 25) == 0b01 &&
+       INSN(31, 29) == 0b000) {
+      UInt   rd     = INSN(11, 7);
+      UInt   rm     = INSN(14, 12);
+      UInt   rs1    = INSN(19, 15);
+      UInt   rs2    = INSN(24, 20);
+      UInt   funct7 = INSN(31, 25);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      const HChar* name;
+      IROp         op;
+      IRTemp       a1 = newTemp(irsb, Ity_F64);
+      IRTemp       a2 = newTemp(irsb, Ity_F64);
+      const HChar* helper_name;
+      void*        helper_addr;
+      switch (funct7) {
+      case 0b0000001:
+         name = "fadd";
+         op   = Iop_AddF64;
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         helper_name = "riscv64g_calculate_fflags_fadd_d";
+         helper_addr = riscv64g_calculate_fflags_fadd_d;
+         break;
+      case 0b0000101:
+         name = "fsub";
+         op   = Iop_AddF64;
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, unop(Iop_NegF64, getFReg64(rs2)));
+         helper_name = "riscv64g_calculate_fflags_fadd_d";
+         helper_addr = riscv64g_calculate_fflags_fadd_d;
+         break;
+      case 0b0001001:
+         name = "fmul";
+         op   = Iop_MulF64;
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         helper_name = "riscv64g_calculate_fflags_fmul_d";
+         helper_addr = riscv64g_calculate_fflags_fmul_d;
+         break;
+      case 0b0001101:
+         name = "fdiv";
+         op   = Iop_DivF64;
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         helper_name = "riscv64g_calculate_fflags_fdiv_d";
+         helper_addr = riscv64g_calculate_fflags_fdiv_d;
+         break;
+      default:
+         vassert(0);
+      }
+      putFReg64(irsb, rd, triop(op, mkexpr(rm_IR), mkexpr(a1), mkexpr(a2)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(Ity_I32, 0 /*regparms*/, helper_name,
+                                        helper_addr,
+                                        mkIRExprVec_3(mkexpr(a1), mkexpr(a2),
+                                                      mkexpr(rm_RISCV)))));
+      DIP("%s.d %s, %s, %s%s\n", name, nameFReg(rd), nameFReg(rs1),
+          nameFReg(rs2), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ----------------- fsqrt.d rd, rs1, rm ----------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 20) == 0b00000 &&
+       INSN(31, 25) == 0b0101101) {
+      UInt   rd  = INSN(11, 7);
+      UInt   rm  = INSN(14, 12);
+      UInt   rs1 = INSN(19, 15);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      IRTemp a1 = newTemp(irsb, Ity_F64);
+      assign(irsb, a1, getFReg64(rs1));
+      putFReg64(irsb, rd, binop(Iop_SqrtF64, mkexpr(rm_IR), mkexpr(a1)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(
+                             Ity_I32, 0 /*regparms*/,
+                             "riscv64g_calculate_fflags_fsqrt_d",
+                             riscv64g_calculate_fflags_fsqrt_d,
+                             mkIRExprVec_2(mkexpr(a1), mkexpr(rm_RISCV)))));
+      DIP("fsqrt.d %s, %s%s\n", nameFReg(rd), nameFReg(rs1), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ---------------- fsgnj.d rd, rs1, rs2 ----------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b000 &&
+       INSN(31, 25) == 0b0010001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rs1 == rs2) {
+         putFReg64(irsb, rd, getFReg64(rs1));
+         DIP("fmv.d %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      } else {
+         putFReg64(
+            irsb, rd,
+            unop(Iop_ReinterpI64asF64,
+                 binop(
+                    Iop_Or64,
+                    binop(Iop_And64, unop(Iop_ReinterpF64asI64, getFReg64(rs1)),
+                          mkU64(0x7fffffffffffffff)),
+                    binop(Iop_And64, unop(Iop_ReinterpF64asI64, getFReg64(rs2)),
+                          mkU64(0x8000000000000000)))));
+         DIP("fsgnj.d %s, %s, %s\n", nameFReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+      }
+      return True;
+   }
+
+   /* ---------------- fsgnjn.d rd, rs1, rs2 ---------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b001 &&
+       INSN(31, 25) == 0b0010001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rs1 == rs2) {
+         putFReg64(irsb, rd, unop(Iop_NegF64, getFReg64(rs1)));
+         DIP("fneg.d %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      } else {
+         putFReg64(irsb, rd,
+                   unop(Iop_ReinterpI64asF64,
+                        binop(Iop_Or64,
+                              binop(Iop_And64,
+                                    unop(Iop_ReinterpF64asI64, getFReg64(rs1)),
+                                    mkU64(0x7fffffffffffffff)),
+                              binop(Iop_And64,
+                                    unop(Iop_ReinterpF64asI64,
+                                         unop(Iop_NegF64, getFReg64(rs2))),
+                                    mkU64(0x8000000000000000)))));
+         DIP("fsgnjn.d %s, %s, %s\n", nameFReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+      }
+      return True;
+   }
+
+   /* ---------------- fsgnjx.d rd, rs1, rs2 ---------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b010 &&
+       INSN(31, 25) == 0b0010001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rs1 == rs2) {
+         putFReg64(irsb, rd, unop(Iop_AbsF64, getFReg64(rs1)));
+         DIP("fabs.d %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      } else {
+         putFReg64(
+            irsb, rd,
+            unop(Iop_ReinterpI64asF64,
+                 binop(Iop_Xor64, unop(Iop_ReinterpF64asI64, getFReg64(rs1)),
+                       binop(Iop_And64,
+                             unop(Iop_ReinterpF64asI64, getFReg64(rs2)),
+                             mkU64(0x8000000000000000)))));
+         DIP("fsgnjx.d %s, %s, %s\n", nameFReg(rd), nameIReg(rs1),
+             nameIReg(rs2));
+      }
+      return True;
+   }
+
+   /* -------------- f{min,max}.d rd, rs1, rs2 -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(31, 25) == 0b0010101) {
+      UInt rd  = INSN(11, 7);
+      UInt rm  = INSN(14, 12);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rm != 0b000 && rm != 0b001) {
+         /* Invalid F{MIN,MAX}.D, fall through. */
+      } else {
+         const HChar* name;
+         IROp         op;
+         const HChar* helper_name;
+         void*        helper_addr;
+         switch (rm) {
+         case 0b000:
+            name        = "fmin";
+            op          = Iop_MinNumF64;
+            helper_name = "riscv64g_calculate_fflags_fmin_d";
+            helper_addr = riscv64g_calculate_fflags_fmin_d;
+            break;
+         case 0b001:
+            name        = "fmax";
+            op          = Iop_MaxNumF64;
+            helper_name = "riscv64g_calculate_fflags_fmax_d";
+            helper_addr = riscv64g_calculate_fflags_fmax_d;
+            break;
+         default:
+            vassert(0);
+         }
+         IRTemp a1 = newTemp(irsb, Ity_F64);
+         IRTemp a2 = newTemp(irsb, Ity_F64);
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         putFReg64(irsb, rd, binop(op, mkexpr(a1), mkexpr(a2)));
+         putFCSR(irsb,
+                 binop(Iop_Or32, getFCSR(),
+                       mkIRExprCCall(Ity_I32, 0 /*regparms*/, helper_name,
+                                     helper_addr,
+                                     mkIRExprVec_2(mkexpr(a1), mkexpr(a2)))));
+         DIP("%s.d %s, %s, %s\n", name, nameFReg(rd), nameFReg(rs1),
+             nameFReg(rs2));
+         return True;
+      }
+   }
+
+   /* ---------------- fcvt.s.d rd, rs1, rm ----------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 20) == 0b00001 &&
+       INSN(31, 25) == 0b0100000) {
+      UInt   rd  = INSN(11, 7);
+      UInt   rm  = INSN(14, 12);
+      UInt   rs1 = INSN(19, 15);
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      IRTemp a1 = newTemp(irsb, Ity_F64);
+      assign(irsb, a1, getFReg64(rs1));
+      putFReg32(irsb, rd, binop(Iop_F64toF32, mkexpr(rm_IR), mkexpr(a1)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(
+                             Ity_I32, 0 /*regparms*/,
+                             "riscv64g_calculate_fflags_fcvt_s_d",
+                             riscv64g_calculate_fflags_fcvt_s_d,
+                             mkIRExprVec_2(mkexpr(a1), mkexpr(rm_RISCV)))));
+      DIP("fcvt.s.d %s, %s%s\n", nameFReg(rd), nameFReg(rs1),
+          nameRMOperand(rm));
+      return True;
+   }
+
+   /* ---------------- fcvt.d.s rd, rs1, rm ----------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 20) == 0b00000 &&
+       INSN(31, 25) == 0b0100001) {
+      UInt rd  = INSN(11, 7);
+      UInt rm  = INSN(14, 12); /* Ignored as the result is always exact. */
+      UInt rs1 = INSN(19, 15);
+      putFReg64(irsb, rd, unop(Iop_F32toF64, getFReg32(rs1)));
+      DIP("fcvt.d.s %s, %s%s\n", nameFReg(rd), nameFReg(rs1),
+          nameRMOperand(rm));
+      return True;
+   }
+
+   /* ------------- f{eq,lt,le}.d rd, rs1, rs2 -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(31, 25) == 0b1010001) {
+      UInt rd  = INSN(11, 7);
+      UInt rm  = INSN(14, 12);
+      UInt rs1 = INSN(19, 15);
+      UInt rs2 = INSN(24, 20);
+      if (rm != 0b010 && rm != 0b001 && rm != 0b000) {
+         /* Invalid F{EQ,LT,LE}.D, fall through. */
+      } else {
+         IRTemp a1 = newTemp(irsb, Ity_F64);
+         IRTemp a2 = newTemp(irsb, Ity_F64);
+         assign(irsb, a1, getFReg64(rs1));
+         assign(irsb, a2, getFReg64(rs2));
+         if (rd != 0) {
+            IRTemp cmp = newTemp(irsb, Ity_I32);
+            assign(irsb, cmp, binop(Iop_CmpF64, mkexpr(a1), mkexpr(a2)));
+            IRTemp res = newTemp(irsb, Ity_I1);
+            switch (rm) {
+            case 0b010:
+               assign(irsb, res,
+                      binop(Iop_CmpEQ32, mkexpr(cmp), mkU32(Ircr_EQ)));
+               break;
+            case 0b001:
+               assign(irsb, res,
+                      binop(Iop_CmpEQ32, mkexpr(cmp), mkU32(Ircr_LT)));
+               break;
+            case 0b000:
+               assign(irsb, res,
+                      binop(Iop_Or1,
+                            binop(Iop_CmpEQ32, mkexpr(cmp), mkU32(Ircr_LT)),
+                            binop(Iop_CmpEQ32, mkexpr(cmp), mkU32(Ircr_EQ))));
+               break;
+            default:
+               vassert(0);
+            }
+            putIReg64(irsb, rd, unop(Iop_1Uto64, mkexpr(res)));
+         }
+         const HChar* name;
+         const HChar* helper_name;
+         void*        helper_addr;
+         switch (rm) {
+         case 0b010:
+            name        = "feq";
+            helper_name = "riscv64g_calculate_fflags_feq_d";
+            helper_addr = riscv64g_calculate_fflags_feq_d;
+            break;
+         case 0b001:
+            name        = "flt";
+            helper_name = "riscv64g_calculate_fflags_flt_d";
+            helper_addr = riscv64g_calculate_fflags_flt_d;
+            break;
+         case 0b000:
+            name        = "fle";
+            helper_name = "riscv64g_calculate_fflags_fle_d";
+            helper_addr = riscv64g_calculate_fflags_fle_d;
+            break;
+         default:
+            vassert(0);
+         }
+         putFCSR(irsb,
+                 binop(Iop_Or32, getFCSR(),
+                       mkIRExprCCall(Ity_I32, 0 /*regparms*/, helper_name,
+                                     helper_addr,
+                                     mkIRExprVec_2(mkexpr(a1), mkexpr(a2)))));
+         DIP("%s.d %s, %s, %s\n", name, nameIReg(rd), nameFReg(rs1),
+             nameFReg(rs2));
+         return True;
+      }
+   }
+
+   /* ------------------ fclass.d rd, rs1 ------------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b001 &&
+       INSN(24, 20) == 0b00000 && INSN(31, 25) == 0b1110001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      if (rd != 0)
+         putIReg64(irsb, rd,
+                   mkIRExprCCall(Ity_I64, 0 /*regparms*/,
+                                 "riscv64g_calculate_fclass_d",
+                                 riscv64g_calculate_fclass_d,
+                                 mkIRExprVec_1(getFReg64(rs1))));
+      DIP("fclass.d %s, %s\n", nameIReg(rd), nameFReg(rs1));
+      return True;
+   }
+
+   /* -------------- fcvt.{w,wu}.d rd, rs1, rm -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 21) == 0b0000 &&
+       INSN(31, 25) == 0b1100001) {
+      UInt   rd        = INSN(11, 7);
+      UInt   rm        = INSN(14, 12);
+      UInt   rs1       = INSN(19, 15);
+      Bool   is_signed = INSN(20, 20) == 0b0;
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      IRTemp a1 = newTemp(irsb, Ity_F64);
+      assign(irsb, a1, getFReg64(rs1));
+      if (rd != 0)
+         putIReg32(irsb, rd,
+                   binop(is_signed ? Iop_F64toI32S : Iop_F64toI32U,
+                         mkexpr(rm_IR), mkexpr(a1)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(
+                             Ity_I32, 0 /*regparms*/,
+                             is_signed ? "riscv64g_calculate_fflags_fcvt_w_d"
+                                       : "riscv64g_calculate_fflags_fcvt_wu_d",
+                             is_signed ? riscv64g_calculate_fflags_fcvt_w_d
+                                       : riscv64g_calculate_fflags_fcvt_wu_d,
+                             mkIRExprVec_2(mkexpr(a1), mkexpr(rm_RISCV)))));
+      DIP("fcvt.w%s.d %s, %s%s\n", is_signed ? "" : "u", nameIReg(rd),
+          nameFReg(rs1), nameRMOperand(rm));
+      return True;
+   }
+
+   /* -------------- fcvt.d.{w,wu} rd, rs1, rm -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 21) == 0b0000 &&
+       INSN(31, 25) == 0b1101001) {
+      UInt rd  = INSN(11, 7);
+      UInt rm  = INSN(14, 12); /* Ignored as the result is always exact. */
+      UInt rs1 = INSN(19, 15);
+      Bool is_signed = INSN(20, 20) == 0b0;
+      putFReg64(
+         irsb, rd,
+         unop(is_signed ? Iop_I32StoF64 : Iop_I32UtoF64, getIReg32(rs1)));
+      DIP("fcvt.d.w%s %s, %s%s\n", is_signed ? "" : "u", nameFReg(rd),
+          nameIReg(rs1), nameRMOperand(rm));
+      return True;
+   }
+
+   /* -------------- fcvt.{l,lu}.d rd, rs1, rm -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 21) == 0b0001 &&
+       INSN(31, 25) == 0b1100001) {
+      UInt   rd        = INSN(11, 7);
+      UInt   rm        = INSN(14, 12);
+      UInt   rs1       = INSN(19, 15);
+      Bool   is_signed = INSN(20, 20) == 0b0;
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      IRTemp a1 = newTemp(irsb, Ity_F64);
+      assign(irsb, a1, getFReg64(rs1));
+      if (rd != 0)
+         putIReg64(irsb, rd,
+                   binop(is_signed ? Iop_F64toI64S : Iop_F64toI64U,
+                         mkexpr(rm_IR), mkexpr(a1)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(
+                             Ity_I32, 0 /*regparms*/,
+                             is_signed ? "riscv64g_calculate_fflags_fcvt_l_d"
+                                       : "riscv64g_calculate_fflags_fcvt_lu_d",
+                             is_signed ? riscv64g_calculate_fflags_fcvt_l_d
+                                       : riscv64g_calculate_fflags_fcvt_lu_d,
+                             mkIRExprVec_2(mkexpr(a1), mkexpr(rm_RISCV)))));
+      DIP("fcvt.l%s.d %s, %s%s\n", is_signed ? "" : "u", nameIReg(rd),
+          nameFReg(rs1), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ------------------- fmv.x.d rd, rs1 ------------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b000 &&
+       INSN(24, 20) == 0b00000 && INSN(31, 25) == 0b1110001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      if (rd != 0)
+         putIReg64(irsb, rd, unop(Iop_ReinterpF64asI64, getFReg64(rs1)));
+      DIP("fmv.x.d %s, %s\n", nameIReg(rd), nameFReg(rs1));
+      return True;
+   }
+
+   /* -------------- fcvt.d.{l,lu} rd, rs1, rm -------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(24, 21) == 0b0001 &&
+       INSN(31, 25) == 0b1101001) {
+      UInt   rd        = INSN(11, 7);
+      UInt   rm        = INSN(14, 12);
+      UInt   rs1       = INSN(19, 15);
+      Bool   is_signed = INSN(20, 20) == 0b0;
+      IRTemp rm_RISCV, rm_IR;
+      mk_get_rounding_mode(irsb, &rm_RISCV, &rm_IR, rm);
+      IRTemp a1 = newTemp(irsb, Ity_I64);
+      assign(irsb, a1, getIReg64(rs1));
+      putFReg64(irsb, rd,
+                binop(is_signed ? Iop_I64StoF64 : Iop_I64UtoF64, mkexpr(rm_IR),
+                      mkexpr(a1)));
+      putFCSR(irsb, binop(Iop_Or32, getFCSR(),
+                          mkIRExprCCall(
+                             Ity_I32, 0 /*regparms*/,
+                             is_signed ? "riscv64g_calculate_fflags_fcvt_d_l"
+                                       : "riscv64g_calculate_fflags_fcvt_d_lu",
+                             is_signed ? riscv64g_calculate_fflags_fcvt_d_l
+                                       : riscv64g_calculate_fflags_fcvt_d_lu,
+                             mkIRExprVec_2(mkexpr(a1), mkexpr(rm_RISCV)))));
+      DIP("fcvt.d.l%s %s, %s%s\n", is_signed ? "" : "u", nameFReg(rd),
+          nameIReg(rs1), nameRMOperand(rm));
+      return True;
+   }
+
+   /* ------------------- fmv.d.x rd, rs1 ------------------- */
+   if (INSN(6, 0) == 0b1010011 && INSN(14, 12) == 0b000 &&
+       INSN(24, 20) == 0b00000 && INSN(31, 25) == 0b1111001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      putFReg64(irsb, rd, unop(Iop_ReinterpI64asF64, getIReg64(rs1)));
+      DIP("fmv.d.x %s, %s\n", nameFReg(rd), nameIReg(rs1));
+      return True;
+   }
+
+   return False;
+}
+
+static Bool dis_RV64Zicsr(/*MB_OUT*/ DisResult* dres,
+                          /*OUT*/ IRSB*         irsb,
+                          UInt                  insn)
+{
+   /* ------------ RV64Zicsr standard extension ------------- */
+
+   /* ----------------- csrrw rd, csr, rs1 ------------------ */
+   if (INSN(6, 0) == 0b1110011 && INSN(14, 12) == 0b001) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt csr = INSN(31, 20);
+      if (csr != 0x001 && csr != 0x002 && csr != 0x003) {
+         /* Invalid CSRRW, fall through. */
+      } else {
+         switch (csr) {
+         case 0x001: {
+            /* fflags */
+            IRTemp fcsr = newTemp(irsb, Ity_I32);
+            assign(irsb, fcsr, getFCSR());
+            if (rd != 0)
+               putIReg64(irsb, rd,
+                         unop(Iop_32Uto64,
+                              binop(Iop_And32, mkexpr(fcsr), mkU32(0x1f))));
+            putFCSR(irsb,
+                    binop(Iop_Or32,
+                          binop(Iop_And32, mkexpr(fcsr), mkU32(0xffffffe0)),
+                          binop(Iop_And32, getIReg32(rs1), mkU32(0x1f))));
+            break;
+         }
+         case 0x002: {
+            /* frm */
+            IRTemp fcsr = newTemp(irsb, Ity_I32);
+            assign(irsb, fcsr, getFCSR());
+            if (rd != 0)
+               putIReg64(
+                  irsb, rd,
+                  unop(Iop_32Uto64,
+                       binop(Iop_And32, binop(Iop_Shr32, mkexpr(fcsr), mkU8(5)),
+                             mkU32(0x7))));
+            putFCSR(irsb,
+                    binop(Iop_Or32,
+                          binop(Iop_And32, mkexpr(fcsr), mkU32(0xffffff1f)),
+                          binop(Iop_Shl32,
+                                binop(Iop_And32, getIReg32(rs1), mkU32(0x7)),
+                                mkU8(5))));
+            break;
+         }
+         case 0x003: {
+            /* fcsr */
+            IRTemp fcsr = newTemp(irsb, Ity_I32);
+            assign(irsb, fcsr, getFCSR());
+            if (rd != 0)
+               putIReg64(irsb, rd, unop(Iop_32Uto64, mkexpr(fcsr)));
+            putFCSR(irsb, binop(Iop_And32, getIReg32(rs1), mkU32(0xff)));
+            break;
+         }
+         default:
+            vassert(0);
+         }
+         DIP("csrrs %s, %s, %s\n", nameIReg(rd), nameCSR(csr), nameIReg(rs1));
+         return True;
+      }
+   }
+
+   /* ----------------- csrrs rd, csr, rs1 ------------------ */
+   if (INSN(6, 0) == 0b1110011 && INSN(14, 12) == 0b010) {
+      UInt rd  = INSN(11, 7);
+      UInt rs1 = INSN(19, 15);
+      UInt csr = INSN(31, 20);
+      if (csr != 0x001 && csr != 0x002 && csr != 0x003) {
+         /* Invalid CSRRS, fall through. */
+      } else {
+         switch (csr) {
+         case 0x001: {
+            /* fflags */
+            IRTemp fcsr = newTemp(irsb, Ity_I32);
+            assign(irsb, fcsr, getFCSR());
+            if (rd != 0)
+               putIReg64(irsb, rd,
+                         unop(Iop_32Uto64,
+                              binop(Iop_And32, mkexpr(fcsr), mkU32(0x1f))));
+            putFCSR(irsb, binop(Iop_Or32, mkexpr(fcsr),
+                                binop(Iop_And32, getIReg32(rs1), mkU32(0x1f))));
+            break;
+         }
+         case 0x002: {
+            /* frm */
+            IRTemp fcsr = newTemp(irsb, Ity_I32);
+            assign(irsb, fcsr, getFCSR());
+            if (rd != 0)
+               putIReg64(
+                  irsb, rd,
+                  unop(Iop_32Uto64,
+                       binop(Iop_And32, binop(Iop_Shr32, mkexpr(fcsr), mkU8(5)),
+                             mkU32(0x7))));
+            putFCSR(irsb,
+                    binop(Iop_Or32, mkexpr(fcsr),
+                          binop(Iop_Shl32,
+                                binop(Iop_And32, getIReg32(rs1), mkU32(0x7)),
+                                mkU8(5))));
+            break;
+         }
+         case 0x003: {
+            /* fcsr */
+            IRTemp fcsr = newTemp(irsb, Ity_I32);
+            assign(irsb, fcsr, getFCSR());
+            if (rd != 0)
+               putIReg64(irsb, rd, unop(Iop_32Uto64, mkexpr(fcsr)));
+            putFCSR(irsb, binop(Iop_Or32, mkexpr(fcsr),
+                                binop(Iop_And32, getIReg32(rs1), mkU32(0xff))));
+            break;
+         }
+         default:
+            vassert(0);
+         }
+         DIP("csrrs %s, %s, %s\n", nameIReg(rd), nameCSR(csr), nameIReg(rs1));
+         return True;
+      }
+   }
+
+   return False;
+}
+
+static Bool dis_RISCV64_standard(/*MB_OUT*/ DisResult* dres,
+                                 /*OUT*/ IRSB*         irsb,
+                                 UInt                  insn,
+                                 Addr                  guest_pc_curr_instr,
+                                 const VexAbiInfo*     abiinfo,
+                                 Bool                  sigill_diag)
+{
+   vassert(INSN(1, 0) == 0b11);
+
+   Bool ok = False;
+   if (!ok)
+      ok = dis_RV64I(dres, irsb, insn, guest_pc_curr_instr);
+   if (!ok)
+      ok = dis_RV64M(dres, irsb, insn);
+   if (!ok)
+      ok = dis_RV64A(dres, irsb, insn, guest_pc_curr_instr, abiinfo);
+   if (!ok)
+      ok = dis_RV64F(dres, irsb, insn);
+   if (!ok)
+      ok = dis_RV64D(dres, irsb, insn);
+   if (!ok)
+      ok = dis_RV64Zicsr(dres, irsb, insn);
+   if (ok)
+      return True;
+
+   if (sigill_diag)
+      vex_printf("RISCV64 front end: standard\n");
+   return False;
+}
+
+/* Disassemble a single riscv64 instruction into IR. Returns True iff the
+   instruction was decoded, in which case *dres will be set accordingly, or
+   False, in which case *dres should be ignored by the caller. */
+static Bool disInstr_RISCV64_WRK(/*MB_OUT*/ DisResult* dres,
+                                 /*OUT*/ IRSB*         irsb,
+                                 const UChar*          guest_instr,
+                                 Addr                  guest_pc_curr_instr,
+                                 const VexArchInfo*    archinfo,
+                                 const VexAbiInfo*     abiinfo,
+                                 Bool                  sigill_diag)
+{
+   /* Set result defaults. */
+   dres->whatNext    = Dis_Continue;
+   dres->len         = 0;
+   dres->jk_StopHere = Ijk_INVALID;
+   dres->hint        = Dis_HintNone;
+
+   /* Read the instruction word. */
+   UInt insn = getInsn(guest_instr);
+
+   if (0)
+      vex_printf("insn: 0x%x\n", insn);
+
+   DIP("\t(riscv64) 0x%llx:  ", (ULong)guest_pc_curr_instr);
+
+   vassert((guest_pc_curr_instr & 1) == 0);
+
+   /* Spot "Special" instructions (see comment at top of file). */
+   {
+      const UChar* code = guest_instr;
+      /* Spot the 16-byte preamble:
+            00365613   srli a2, a2, 3
+            00d65613   srli a2, a2, 13
+            03365613   srli a2, a2, 51
+            03d65613   srli a2, a2, 61
+      */
+      UInt word1 = 0x00365613;
+      UInt word2 = 0x00d65613;
+      UInt word3 = 0x03365613;
+      UInt word4 = 0x03d65613;
+      if (getUIntLittleEndianly(code + 0) == word1 &&
+          getUIntLittleEndianly(code + 4) == word2 &&
+          getUIntLittleEndianly(code + 8) == word3 &&
+          getUIntLittleEndianly(code + 12) == word4) {
+         /* Got a "Special" instruction preamble. Which one is it? */
+         dres->len  = 20;
+         UInt which = getUIntLittleEndianly(code + 16);
+         if (which == 0x00a56533 /* or a0, a0, a0 */) {
+            /* a3 = client_request ( a4 ) */
+            DIP("a3 = client_request ( a4 )\n");
+            putPC(irsb, mkU64(guest_pc_curr_instr + 20));
+            dres->jk_StopHere = Ijk_ClientReq;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         } else if (which == 0x00b5e5b3 /* or a1, a1, a1 */) {
+            /* a3 = guest_NRADDR */
+            DIP("a3 = guest_NRADDR\n");
+            putIReg64(irsb, 13 /*x13/a3*/, IRExpr_Get(OFFB_NRADDR, Ity_I64));
+            return True;
+         } else if (which == 0x00c66633 /* or a2, a2, a2 */) {
+            /* branch-and-link-to-noredir t0 */
+            DIP("branch-and-link-to-noredir t0\n");
+            putIReg64(irsb, 1 /*x1/ra*/, mkU64(guest_pc_curr_instr + 20));
+            putPC(irsb, getIReg64(5 /*x5/t0*/));
+            dres->jk_StopHere = Ijk_NoRedir;
+            dres->whatNext    = Dis_StopHere;
+            return True;
+         } else if (which == 0x00d6e6b3 /* or a3, a3, a3 */) {
+            /* IR injection */
+            DIP("IR injection\n");
+            vex_inject_ir(irsb, Iend_LE);
+            /* Invalidate the current insn. The reason is that the IRop we're
+               injecting here can change. In which case the translation has to
+               be redone. For ease of handling, we simply invalidate all the
+               time. */
+            stmt(irsb, IRStmt_Put(OFFB_CMSTART, mkU64(guest_pc_curr_instr)));
+            stmt(irsb, IRStmt_Put(OFFB_CMLEN, mkU64(20)));
+            putPC(irsb, mkU64(guest_pc_curr_instr + 20));
+            dres->whatNext    = Dis_StopHere;
+            dres->jk_StopHere = Ijk_InvalICache;
+            return True;
+         }
+         /* We don't know what it is. */
+         return False;
+      }
+   }
+
+   /* Main riscv64 instruction decoder starts here. */
+   Bool ok = False;
+   UInt inst_size;
+
+   /* Parse insn[1:0] to determine whether the instruction is 16-bit
+      (compressed) or 32-bit. */
+   switch (INSN(1, 0)) {
+   case 0b00:
+   case 0b01:
+   case 0b10:
+      dres->len = inst_size = 2;
+      ok = dis_RV64C(dres, irsb, insn, guest_pc_curr_instr, sigill_diag);
+      break;
+
+   case 0b11:
+      dres->len = inst_size = 4;
+      ok = dis_RISCV64_standard(dres, irsb, insn, guest_pc_curr_instr, abiinfo,
+                                sigill_diag);
+      break;
+
+   default:
+      vassert(0); /* Can't happen. */
+   }
+
+   /* If the next-level down decoders failed, make sure dres didn't get
+      changed. */
+   if (!ok) {
+      vassert(dres->whatNext == Dis_Continue);
+      vassert(dres->len == inst_size);
+      vassert(dres->jk_StopHere == Ijk_INVALID);
+   }
+
+   return ok;
+}
+
+#undef INSN
+
+/*------------------------------------------------------------*/
+/*--- Top-level fn                                         ---*/
+/*------------------------------------------------------------*/
+
+/* Disassemble a single instruction into IR. The instruction is located in host
+   memory at &guest_code[delta]. */
+DisResult disInstr_RISCV64(IRSB*              irsb,
+                           const UChar*       guest_code,
+                           Long               delta,
+                           Addr               guest_IP,
+                           VexArch            guest_arch,
+                           const VexArchInfo* archinfo,
+                           const VexAbiInfo*  abiinfo,
+                           VexEndness         host_endness,
+                           Bool               sigill_diag)
+{
+   DisResult dres;
+   vex_bzero(&dres, sizeof(dres));
+
+   vassert(guest_arch == VexArchRISCV64);
+   /* Check that the host is little-endian as getFReg32() and putFReg32() depend
+      on this fact. */
+   vassert(host_endness == VexEndnessLE);
+
+   /* Try to decode. */
+   Bool ok = disInstr_RISCV64_WRK(&dres, irsb, &guest_code[delta], guest_IP,
+                                  archinfo, abiinfo, sigill_diag);
+   if (ok) {
+      /* All decode successes end up here. */
+      vassert(dres.len == 2 || dres.len == 4 || dres.len == 20);
+      switch (dres.whatNext) {
+      case Dis_Continue:
+         putPC(irsb, mkU64(guest_IP + dres.len));
+         break;
+      case Dis_StopHere:
+         break;
+      default:
+         vassert(0);
+      }
+      DIP("\n");
+   } else {
+      /* All decode failures end up here. */
+      if (sigill_diag) {
+         Int   i, j;
+         UChar buf[64];
+         UInt  insn = getInsn(&guest_code[delta]);
+         vex_bzero(buf, sizeof(buf));
+         for (i = j = 0; i < 32; i++) {
+            if (i > 0) {
+               if ((i & 7) == 0)
+                  buf[j++] = ' ';
+               else if ((i & 3) == 0)
+                  buf[j++] = '\'';
+            }
+            buf[j++] = (insn & (1 << (31 - i))) ? '1' : '0';
+         }
+         vex_printf("disInstr(riscv64): unhandled instruction 0x%08x\n", insn);
+         vex_printf("disInstr(riscv64): %s\n", buf);
+      }
+
+      /* Tell the dispatcher that this insn cannot be decoded, and so has not
+         been executed, and (is currently) the next to be executed. The pc
+         register should be up-to-date since it is made so at the start of each
+         insn, but nevertheless be paranoid and update it again right now. */
+      putPC(irsb, mkU64(guest_IP));
+      dres.len         = 0;
+      dres.whatNext    = Dis_StopHere;
+      dres.jk_StopHere = Ijk_NoDecode;
+   }
+   return dres;
+}
+
+/*--------------------------------------------------------------------*/
+/*--- end                                     guest_riscv64_toIR.c ---*/
+/*--------------------------------------------------------------------*/
--- a/VEX/priv/host_generic_regs.h
+++ b/VEX/priv/host_generic_regs.h
@@ -36,7 +36,7 @@
 
 #include "libvex_basictypes.h"
 
-
+#include "main_util.h"
 /*---------------------------------------------------------*/
 /*--- Representing HOST REGISTERS                       ---*/
 /*---------------------------------------------------------*/
--- /dev/null
+++ b/VEX/priv/host_riscv64_defs.c
@@ -0,0 +1,4643 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                    host_riscv64_defs.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "libvex_trc_values.h"
+
+#include "host_riscv64_defs.h"
+#include "main_util.h"
+
+/*------------------------------------------------------------*/
+/*--- Registers                                            ---*/
+/*------------------------------------------------------------*/
+
+UInt ppHRegRISCV64(HReg reg)
+{
+   static const HChar* inames[32] = {
+      "zero", "ra", "sp", "gp", "tp",  "t0",  "t1", "t2", "s0", "s1", "a0",
+      "a1",   "a2", "a3", "a4", "a5",  "a6",  "a7", "s2", "s3", "s4", "s5",
+      "s6",   "s7", "s8", "s9", "s10", "s11", "t3", "t4", "t5", "t6"};
+
+   static const HChar* fnames[32] = {
+      "ft0", "ft1", "ft2",  "ft3",  "ft4", "ft5", "ft6",  "ft7",
+      "fs0", "fs1", "fa0",  "fa1",  "fa2", "fa3", "fa4",  "fa5",
+      "fa6", "fa7", "fs2",  "fs3",  "fs4", "fs5", "fs6",  "fs7",
+      "fs8", "fs9", "fs10", "fs11", "ft8", "ft9", "ft10", "ft11"};
+
+   /* Be generic for all virtual regs. */
+   if (hregIsVirtual(reg))
+      return ppHReg(reg);
+
+   /* Be specific for real regs. */
+   switch (hregClass(reg)) {
+   case HRcInt64: {
+      UInt r = hregEncoding(reg);
+      vassert(r < 32);
+      return vex_printf("%s", inames[r]);
+   }
+   case HRcFlt64: {
+      UInt r = hregEncoding(reg);
+      vassert(r < 32);
+      return vex_printf("%s", fnames[r]);
+   }
+   default:
+      vpanic("ppHRegRISCV64");
+   }
+}
+
+static inline UInt iregEnc(HReg r)
+{
+   vassert(hregClass(r) == HRcInt64);
+   vassert(!hregIsVirtual(r));
+
+   UInt n = hregEncoding(r);
+   vassert(n < 32);
+   return n;
+}
+
+static inline UInt fregEnc(HReg r)
+{
+   UInt n;
+   vassert(hregClass(r) == HRcFlt64);
+   vassert(!hregIsVirtual(r));
+   n = hregEncoding(r);
+   vassert(n < 32);
+   return n;
+}
+
+/*------------------------------------------------------------*/
+/*--- Instructions                                         ---*/
+/*------------------------------------------------------------*/
+
+static const HChar* showRISCV64CSR(UInt csr)
+{
+   switch (csr) {
+   case 0x001:
+      return "fflags";
+   case 0x002:
+      return "frm";
+   case 0x003:
+      return "fcsr";
+   default:
+      vpanic("showRISCV64CSR");
+   }
+}
+
+RISCV64Instr* RISCV64Instr_LI(HReg dst, ULong imm64)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_LI;
+   i->RISCV64in.LI.dst   = dst;
+   i->RISCV64in.LI.imm64 = imm64;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_MV(HReg dst, HReg src)
+{
+   RISCV64Instr* i     = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag              = RISCV64in_MV;
+   i->RISCV64in.MV.dst = dst;
+   i->RISCV64in.MV.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_ADD(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_ADD;
+   i->RISCV64in.ADD.dst  = dst;
+   i->RISCV64in.ADD.src1 = src1;
+   i->RISCV64in.ADD.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_ADDI(HReg dst, HReg src, Int simm12)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_ADDI;
+   i->RISCV64in.ADDI.dst    = dst;
+   i->RISCV64in.ADDI.src    = src;
+   i->RISCV64in.ADDI.simm12 = simm12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_ADDW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_ADDW;
+   i->RISCV64in.ADDW.dst  = dst;
+   i->RISCV64in.ADDW.src1 = src1;
+   i->RISCV64in.ADDW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_ADDIW(HReg dst, HReg src, Int simm12)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_ADDIW;
+   i->RISCV64in.ADDIW.dst    = dst;
+   i->RISCV64in.ADDIW.src    = src;
+   i->RISCV64in.ADDIW.simm12 = simm12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SUB(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_SUB;
+   i->RISCV64in.SUB.dst  = dst;
+   i->RISCV64in.SUB.src1 = src1;
+   i->RISCV64in.SUB.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SUBW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SUBW;
+   i->RISCV64in.SUBW.dst  = dst;
+   i->RISCV64in.SUBW.src1 = src1;
+   i->RISCV64in.SUBW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_XOR(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_XOR;
+   i->RISCV64in.XOR.dst  = dst;
+   i->RISCV64in.XOR.src1 = src1;
+   i->RISCV64in.XOR.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_XORI(HReg dst, HReg src, Int simm12)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_XORI;
+   i->RISCV64in.XORI.dst    = dst;
+   i->RISCV64in.XORI.src    = src;
+   i->RISCV64in.XORI.simm12 = simm12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_OR(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i      = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag               = RISCV64in_OR;
+   i->RISCV64in.OR.dst  = dst;
+   i->RISCV64in.OR.src1 = src1;
+   i->RISCV64in.OR.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_AND(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_AND;
+   i->RISCV64in.AND.dst  = dst;
+   i->RISCV64in.AND.src1 = src1;
+   i->RISCV64in.AND.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_ANDI(HReg dst, HReg src, Int simm12)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_ANDI;
+   i->RISCV64in.ANDI.dst    = dst;
+   i->RISCV64in.ANDI.src    = src;
+   i->RISCV64in.ANDI.simm12 = simm12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SLL(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_SLL;
+   i->RISCV64in.SLL.dst  = dst;
+   i->RISCV64in.SLL.src1 = src1;
+   i->RISCV64in.SLL.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SRL(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_SRL;
+   i->RISCV64in.SRL.dst  = dst;
+   i->RISCV64in.SRL.src1 = src1;
+   i->RISCV64in.SRL.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SRA(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_SRA;
+   i->RISCV64in.SRA.dst  = dst;
+   i->RISCV64in.SRA.src1 = src1;
+   i->RISCV64in.SRA.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SLLI(HReg dst, HReg src, UInt uimm6)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_SLLI;
+   i->RISCV64in.SLLI.dst   = dst;
+   i->RISCV64in.SLLI.src   = src;
+   i->RISCV64in.SLLI.uimm6 = uimm6;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SRLI(HReg dst, HReg src, UInt uimm6)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_SRLI;
+   i->RISCV64in.SRLI.dst   = dst;
+   i->RISCV64in.SRLI.src   = src;
+   i->RISCV64in.SRLI.uimm6 = uimm6;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SRAI(HReg dst, HReg src, UInt uimm6)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_SRAI;
+   i->RISCV64in.SRAI.dst   = dst;
+   i->RISCV64in.SRAI.src   = src;
+   i->RISCV64in.SRAI.uimm6 = uimm6;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SLLW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SLLW;
+   i->RISCV64in.SLLW.dst  = dst;
+   i->RISCV64in.SLLW.src1 = src1;
+   i->RISCV64in.SLLW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SRLW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SRLW;
+   i->RISCV64in.SRLW.dst  = dst;
+   i->RISCV64in.SRLW.src1 = src1;
+   i->RISCV64in.SRLW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SRAW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SRAW;
+   i->RISCV64in.SRAW.dst  = dst;
+   i->RISCV64in.SRAW.src1 = src1;
+   i->RISCV64in.SRAW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SLT(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_SLT;
+   i->RISCV64in.SLT.dst  = dst;
+   i->RISCV64in.SLT.src1 = src1;
+   i->RISCV64in.SLT.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SLTU(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SLTU;
+   i->RISCV64in.SLTU.dst  = dst;
+   i->RISCV64in.SLTU.src1 = src1;
+   i->RISCV64in.SLTU.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SLTIU(HReg dst, HReg src, Int simm12)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_SLTIU;
+   i->RISCV64in.SLTIU.dst    = dst;
+   i->RISCV64in.SLTIU.src    = src;
+   i->RISCV64in.SLTIU.simm12 = simm12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_CSRRW(HReg dst, HReg src, UInt csr)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_CSRRW;
+   i->RISCV64in.CSRRW.dst = dst;
+   i->RISCV64in.CSRRW.src = src;
+   i->RISCV64in.CSRRW.csr = csr;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_MUL(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_MUL;
+   i->RISCV64in.MUL.dst  = dst;
+   i->RISCV64in.MUL.src1 = src1;
+   i->RISCV64in.MUL.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_MULH(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_MULH;
+   i->RISCV64in.MULH.dst  = dst;
+   i->RISCV64in.MULH.src1 = src1;
+   i->RISCV64in.MULH.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_MULHU(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_MULHU;
+   i->RISCV64in.MULHU.dst  = dst;
+   i->RISCV64in.MULHU.src1 = src1;
+   i->RISCV64in.MULHU.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_DIV(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_DIV;
+   i->RISCV64in.DIV.dst  = dst;
+   i->RISCV64in.DIV.src1 = src1;
+   i->RISCV64in.DIV.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_DIVU(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_DIVU;
+   i->RISCV64in.DIVU.dst  = dst;
+   i->RISCV64in.DIVU.src1 = src1;
+   i->RISCV64in.DIVU.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_REM(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i       = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                = RISCV64in_REM;
+   i->RISCV64in.REM.dst  = dst;
+   i->RISCV64in.REM.src1 = src1;
+   i->RISCV64in.REM.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_REMU(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_REMU;
+   i->RISCV64in.REMU.dst  = dst;
+   i->RISCV64in.REMU.src1 = src1;
+   i->RISCV64in.REMU.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_MULW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_MULW;
+   i->RISCV64in.MULW.dst  = dst;
+   i->RISCV64in.MULW.src1 = src1;
+   i->RISCV64in.MULW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_DIVW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_DIVW;
+   i->RISCV64in.DIVW.dst  = dst;
+   i->RISCV64in.DIVW.src1 = src1;
+   i->RISCV64in.DIVW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_DIVUW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_DIVUW;
+   i->RISCV64in.DIVUW.dst  = dst;
+   i->RISCV64in.DIVUW.src1 = src1;
+   i->RISCV64in.DIVUW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_REMW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_REMW;
+   i->RISCV64in.REMW.dst  = dst;
+   i->RISCV64in.REMW.src1 = src1;
+   i->RISCV64in.REMW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_REMUW(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_REMUW;
+   i->RISCV64in.REMUW.dst  = dst;
+   i->RISCV64in.REMUW.src1 = src1;
+   i->RISCV64in.REMUW.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_LD(HReg dst, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_LD;
+   i->RISCV64in.LD.dst    = dst;
+   i->RISCV64in.LD.base   = base;
+   i->RISCV64in.LD.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_LW(HReg dst, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_LW;
+   i->RISCV64in.LW.dst    = dst;
+   i->RISCV64in.LW.base   = base;
+   i->RISCV64in.LW.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_LH(HReg dst, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_LH;
+   i->RISCV64in.LH.dst    = dst;
+   i->RISCV64in.LH.base   = base;
+   i->RISCV64in.LH.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_LB(HReg dst, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_LB;
+   i->RISCV64in.LB.dst    = dst;
+   i->RISCV64in.LB.base   = base;
+   i->RISCV64in.LB.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SD(HReg src, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SD;
+   i->RISCV64in.SD.src    = src;
+   i->RISCV64in.SD.base   = base;
+   i->RISCV64in.SD.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SW(HReg src, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SW;
+   i->RISCV64in.SW.src    = src;
+   i->RISCV64in.SW.base   = base;
+   i->RISCV64in.SW.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SH(HReg src, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SH;
+   i->RISCV64in.SH.src    = src;
+   i->RISCV64in.SH.base   = base;
+   i->RISCV64in.SH.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SB(HReg src, HReg base, Int soff12)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SB;
+   i->RISCV64in.SB.src    = src;
+   i->RISCV64in.SB.base   = base;
+   i->RISCV64in.SB.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_LR_W(HReg dst, HReg addr)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_LR_W;
+   i->RISCV64in.LR_W.dst  = dst;
+   i->RISCV64in.LR_W.addr = addr;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_SC_W(HReg res, HReg src, HReg addr)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_SC_W;
+   i->RISCV64in.SC_W.res  = res;
+   i->RISCV64in.SC_W.src  = src;
+   i->RISCV64in.SC_W.addr = addr;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMADD_S(HReg dst, HReg src1, HReg src2, HReg src3)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FMADD_S;
+   i->RISCV64in.FMADD_S.dst  = dst;
+   i->RISCV64in.FMADD_S.src1 = src1;
+   i->RISCV64in.FMADD_S.src2 = src2;
+   i->RISCV64in.FMADD_S.src3 = src3;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FADD_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FADD_S;
+   i->RISCV64in.FADD_S.dst  = dst;
+   i->RISCV64in.FADD_S.src1 = src1;
+   i->RISCV64in.FADD_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMUL_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMUL_S;
+   i->RISCV64in.FMUL_S.dst  = dst;
+   i->RISCV64in.FMUL_S.src1 = src1;
+   i->RISCV64in.FMUL_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FDIV_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FDIV_S;
+   i->RISCV64in.FDIV_S.dst  = dst;
+   i->RISCV64in.FDIV_S.src1 = src1;
+   i->RISCV64in.FDIV_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSQRT_S(HReg dst, HReg src1)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FSQRT_S;
+   i->RISCV64in.FSQRT_S.dst  = dst;
+   i->RISCV64in.FSQRT_S.src1 = src1;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSGNJN_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FSGNJN_S;
+   i->RISCV64in.FSGNJN_S.dst  = dst;
+   i->RISCV64in.FSGNJN_S.src1 = src1;
+   i->RISCV64in.FSGNJN_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSGNJX_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FSGNJX_S;
+   i->RISCV64in.FSGNJX_S.dst  = dst;
+   i->RISCV64in.FSGNJX_S.src1 = src1;
+   i->RISCV64in.FSGNJX_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMIN_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMIN_S;
+   i->RISCV64in.FMIN_S.dst  = dst;
+   i->RISCV64in.FMIN_S.src1 = src1;
+   i->RISCV64in.FMIN_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMAX_S(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMAX_S;
+   i->RISCV64in.FMAX_S.dst  = dst;
+   i->RISCV64in.FMAX_S.src1 = src1;
+   i->RISCV64in.FMAX_S.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMV_X_W(HReg dst, HReg src)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMV_X_W;
+   i->RISCV64in.FMV_X_W.dst = dst;
+   i->RISCV64in.FMV_X_W.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMV_W_X(HReg dst, HReg src)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMV_W_X;
+   i->RISCV64in.FMV_W_X.dst = dst;
+   i->RISCV64in.FMV_W_X.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMV_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i        = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                 = RISCV64in_FMV_D;
+   i->RISCV64in.FMV_D.dst = dst;
+   i->RISCV64in.FMV_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMADD_D(HReg dst, HReg src1, HReg src2, HReg src3)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FMADD_D;
+   i->RISCV64in.FMADD_D.dst  = dst;
+   i->RISCV64in.FMADD_D.src1 = src1;
+   i->RISCV64in.FMADD_D.src2 = src2;
+   i->RISCV64in.FMADD_D.src3 = src3;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FADD_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FADD_D;
+   i->RISCV64in.FADD_D.dst  = dst;
+   i->RISCV64in.FADD_D.src1 = src1;
+   i->RISCV64in.FADD_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSUB_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FSUB_D;
+   i->RISCV64in.FSUB_D.dst  = dst;
+   i->RISCV64in.FSUB_D.src1 = src1;
+   i->RISCV64in.FSUB_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMUL_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMUL_D;
+   i->RISCV64in.FMUL_D.dst  = dst;
+   i->RISCV64in.FMUL_D.src1 = src1;
+   i->RISCV64in.FMUL_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FDIV_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FDIV_D;
+   i->RISCV64in.FDIV_D.dst  = dst;
+   i->RISCV64in.FDIV_D.src1 = src1;
+   i->RISCV64in.FDIV_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSQRT_D(HReg dst, HReg src1)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FSQRT_D;
+   i->RISCV64in.FSQRT_D.dst  = dst;
+   i->RISCV64in.FSQRT_D.src1 = src1;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSGNJN_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FSGNJN_D;
+   i->RISCV64in.FSGNJN_D.dst  = dst;
+   i->RISCV64in.FSGNJN_D.src1 = src1;
+   i->RISCV64in.FSGNJN_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSGNJX_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FSGNJX_D;
+   i->RISCV64in.FSGNJX_D.dst  = dst;
+   i->RISCV64in.FSGNJX_D.src1 = src1;
+   i->RISCV64in.FSGNJX_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMIN_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMIN_D;
+   i->RISCV64in.FMIN_D.dst  = dst;
+   i->RISCV64in.FMIN_D.src1 = src1;
+   i->RISCV64in.FMIN_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMAX_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMAX_D;
+   i->RISCV64in.FMAX_D.dst  = dst;
+   i->RISCV64in.FMAX_D.src1 = src1;
+   i->RISCV64in.FMAX_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FEQ_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_FEQ_D;
+   i->RISCV64in.FEQ_D.dst  = dst;
+   i->RISCV64in.FEQ_D.src1 = src1;
+   i->RISCV64in.FEQ_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FLT_D(HReg dst, HReg src1, HReg src2)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_FLT_D;
+   i->RISCV64in.FLT_D.dst  = dst;
+   i->RISCV64in.FLT_D.src1 = src1;
+   i->RISCV64in.FLT_D.src2 = src2;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_S_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FCVT_S_D;
+   i->RISCV64in.FCVT_S_D.dst = dst;
+   i->RISCV64in.FCVT_S_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_D_S(HReg dst, HReg src)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FCVT_D_S;
+   i->RISCV64in.FCVT_D_S.dst = dst;
+   i->RISCV64in.FCVT_D_S.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_W_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FCVT_W_D;
+   i->RISCV64in.FCVT_W_D.dst = dst;
+   i->RISCV64in.FCVT_W_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_WU_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FCVT_WU_D;
+   i->RISCV64in.FCVT_WU_D.dst = dst;
+   i->RISCV64in.FCVT_WU_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_D_W(HReg dst, HReg src)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FCVT_D_W;
+   i->RISCV64in.FCVT_D_W.dst = dst;
+   i->RISCV64in.FCVT_D_W.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_D_WU(HReg dst, HReg src)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FCVT_D_WU;
+   i->RISCV64in.FCVT_D_WU.dst = dst;
+   i->RISCV64in.FCVT_D_WU.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_L_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FCVT_L_D;
+   i->RISCV64in.FCVT_L_D.dst = dst;
+   i->RISCV64in.FCVT_L_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_LU_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FCVT_LU_D;
+   i->RISCV64in.FCVT_LU_D.dst = dst;
+   i->RISCV64in.FCVT_LU_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_D_L(HReg dst, HReg src)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_FCVT_D_L;
+   i->RISCV64in.FCVT_D_L.dst = dst;
+   i->RISCV64in.FCVT_D_L.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FCVT_D_LU(HReg dst, HReg src)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_FCVT_D_LU;
+   i->RISCV64in.FCVT_D_LU.dst = dst;
+   i->RISCV64in.FCVT_D_LU.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMV_X_D(HReg dst, HReg src)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMV_X_D;
+   i->RISCV64in.FMV_X_D.dst = dst;
+   i->RISCV64in.FMV_X_D.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FMV_D_X(HReg dst, HReg src)
+{
+   RISCV64Instr* i          = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                   = RISCV64in_FMV_D_X;
+   i->RISCV64in.FMV_D_X.dst = dst;
+   i->RISCV64in.FMV_D_X.src = src;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FLD(HReg dst, HReg base, Int soff12)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_FLD;
+   i->RISCV64in.FLD.dst    = dst;
+   i->RISCV64in.FLD.base   = base;
+   i->RISCV64in.FLD.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FLW(HReg dst, HReg base, Int soff12)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_FLW;
+   i->RISCV64in.FLW.dst    = dst;
+   i->RISCV64in.FLW.base   = base;
+   i->RISCV64in.FLW.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSD(HReg src, HReg base, Int soff12)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_FSD;
+   i->RISCV64in.FSD.src    = src;
+   i->RISCV64in.FSD.base   = base;
+   i->RISCV64in.FSD.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FSW(HReg src, HReg base, Int soff12)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_FSW;
+   i->RISCV64in.FSW.src    = src;
+   i->RISCV64in.FSW.base   = base;
+   i->RISCV64in.FSW.soff12 = soff12;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_CAS_W(HReg old, HReg addr, HReg expd, HReg data)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_CAS_W;
+   i->RISCV64in.CAS_W.old  = old;
+   i->RISCV64in.CAS_W.addr = addr;
+   i->RISCV64in.CAS_W.expd = expd;
+   i->RISCV64in.CAS_W.data = data;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_CAS_D(HReg old, HReg addr, HReg expd, HReg data)
+{
+   RISCV64Instr* i         = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                  = RISCV64in_CAS_D;
+   i->RISCV64in.CAS_D.old  = old;
+   i->RISCV64in.CAS_D.addr = addr;
+   i->RISCV64in.CAS_D.expd = expd;
+   i->RISCV64in.CAS_D.data = data;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_FENCE(void)
+{
+   RISCV64Instr* i = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag          = RISCV64in_FENCE;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_CSEL(HReg dst, HReg iftrue, HReg iffalse, HReg cond)
+{
+   RISCV64Instr* i           = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                    = RISCV64in_CSEL;
+   i->RISCV64in.CSEL.dst     = dst;
+   i->RISCV64in.CSEL.iftrue  = iftrue;
+   i->RISCV64in.CSEL.iffalse = iffalse;
+   i->RISCV64in.CSEL.cond    = cond;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_Call(
+   RetLoc rloc, Addr64 target, HReg cond, UChar nArgRegs, UChar nFArgRegs)
+{
+   RISCV64Instr* i             = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                      = RISCV64in_Call;
+   i->RISCV64in.Call.rloc      = rloc;
+   i->RISCV64in.Call.target    = target;
+   i->RISCV64in.Call.cond      = cond;
+   i->RISCV64in.Call.nArgRegs  = nArgRegs;
+   i->RISCV64in.Call.nFArgRegs = nFArgRegs;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_XDirect(
+   Addr64 dstGA, HReg base, Int soff12, HReg cond, Bool toFastEP)
+{
+   RISCV64Instr* i               = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                        = RISCV64in_XDirect;
+   i->RISCV64in.XDirect.dstGA    = dstGA;
+   i->RISCV64in.XDirect.base     = base;
+   i->RISCV64in.XDirect.soff12   = soff12;
+   i->RISCV64in.XDirect.cond     = cond;
+   i->RISCV64in.XDirect.toFastEP = toFastEP;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_XIndir(HReg dstGA, HReg base, Int soff12, HReg cond)
+{
+   RISCV64Instr* i            = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                     = RISCV64in_XIndir;
+   i->RISCV64in.XIndir.dstGA  = dstGA;
+   i->RISCV64in.XIndir.base   = base;
+   i->RISCV64in.XIndir.soff12 = soff12;
+   i->RISCV64in.XIndir.cond   = cond;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_XAssisted(
+   HReg dstGA, HReg base, Int soff12, HReg cond, IRJumpKind jk)
+{
+   RISCV64Instr* i               = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag                        = RISCV64in_XAssisted;
+   i->RISCV64in.XAssisted.dstGA  = dstGA;
+   i->RISCV64in.XAssisted.base   = base;
+   i->RISCV64in.XAssisted.soff12 = soff12;
+   i->RISCV64in.XAssisted.cond   = cond;
+   i->RISCV64in.XAssisted.jk     = jk;
+   return i;
+}
+
+RISCV64Instr* RISCV64Instr_EvCheck(HReg base_amCounter,
+                                   Int  soff12_amCounter,
+                                   HReg base_amFailAddr,
+                                   Int  soff12_amFailAddr)
+{
+   RISCV64Instr* i = LibVEX_Alloc_inline(sizeof(RISCV64Instr));
+   i->tag          = RISCV64in_EvCheck;
+   i->RISCV64in.EvCheck.base_amCounter    = base_amCounter;
+   i->RISCV64in.EvCheck.soff12_amCounter  = soff12_amCounter;
+   i->RISCV64in.EvCheck.base_amFailAddr   = base_amFailAddr;
+   i->RISCV64in.EvCheck.soff12_amFailAddr = soff12_amFailAddr;
+   return i;
+}
+
+void ppRISCV64Instr(const RISCV64Instr* i, Bool mode64)
+{
+   vassert(mode64 == True);
+
+   switch (i->tag) {
+   case RISCV64in_LI:
+      vex_printf("li      ");
+      ppHRegRISCV64(i->RISCV64in.LI.dst);
+      vex_printf(", 0x%llx", i->RISCV64in.LI.imm64);
+      return;
+   case RISCV64in_MV:
+      vex_printf("mv      ");
+      ppHRegRISCV64(i->RISCV64in.MV.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MV.src);
+      return;
+   case RISCV64in_ADD:
+      vex_printf("add     ");
+      ppHRegRISCV64(i->RISCV64in.ADD.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ADD.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ADD.src2);
+      return;
+   case RISCV64in_ADDI:
+      vex_printf("addi    ");
+      ppHRegRISCV64(i->RISCV64in.ADDI.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ADDI.src);
+      vex_printf(", %d", i->RISCV64in.ADDI.simm12);
+      return;
+   case RISCV64in_ADDW:
+      vex_printf("addw    ");
+      ppHRegRISCV64(i->RISCV64in.ADDW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ADDW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ADDW.src2);
+      return;
+   case RISCV64in_ADDIW:
+      vex_printf("addiw   ");
+      ppHRegRISCV64(i->RISCV64in.ADDIW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ADDIW.src);
+      vex_printf(", %d", i->RISCV64in.ADDIW.simm12);
+      return;
+   case RISCV64in_SUB:
+      vex_printf("sub     ");
+      ppHRegRISCV64(i->RISCV64in.SUB.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SUB.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SUB.src2);
+      return;
+   case RISCV64in_SUBW:
+      vex_printf("subw    ");
+      ppHRegRISCV64(i->RISCV64in.SUBW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SUBW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SUBW.src2);
+      return;
+   case RISCV64in_XOR:
+      vex_printf("xor     ");
+      ppHRegRISCV64(i->RISCV64in.XOR.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.XOR.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.XOR.src2);
+      return;
+   case RISCV64in_XORI:
+      vex_printf("xori    ");
+      ppHRegRISCV64(i->RISCV64in.XORI.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.XORI.src);
+      vex_printf(", %d", i->RISCV64in.XORI.simm12);
+      return;
+   case RISCV64in_OR:
+      vex_printf("or      ");
+      ppHRegRISCV64(i->RISCV64in.OR.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.OR.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.OR.src2);
+      return;
+   case RISCV64in_AND:
+      vex_printf("and     ");
+      ppHRegRISCV64(i->RISCV64in.AND.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.AND.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.AND.src2);
+      return;
+   case RISCV64in_ANDI:
+      vex_printf("andi    ");
+      ppHRegRISCV64(i->RISCV64in.ANDI.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.ANDI.src);
+      vex_printf(", %d", i->RISCV64in.ANDI.simm12);
+      return;
+   case RISCV64in_SLL:
+      vex_printf("sll     ");
+      ppHRegRISCV64(i->RISCV64in.SLL.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLL.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLL.src2);
+      return;
+   case RISCV64in_SRL:
+      vex_printf("srl     ");
+      ppHRegRISCV64(i->RISCV64in.SRL.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRL.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRL.src2);
+      return;
+   case RISCV64in_SRA:
+      vex_printf("sra     ");
+      ppHRegRISCV64(i->RISCV64in.SRA.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRA.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRA.src2);
+      return;
+   case RISCV64in_SLLI:
+      vex_printf("slli    ");
+      ppHRegRISCV64(i->RISCV64in.SLLI.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLLI.src);
+      vex_printf(", %u", i->RISCV64in.SLLI.uimm6);
+      return;
+   case RISCV64in_SRLI:
+      vex_printf("srli    ");
+      ppHRegRISCV64(i->RISCV64in.SRLI.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRLI.src);
+      vex_printf(", %u", i->RISCV64in.SRLI.uimm6);
+      return;
+   case RISCV64in_SRAI:
+      vex_printf("srai    ");
+      ppHRegRISCV64(i->RISCV64in.SRAI.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRAI.src);
+      vex_printf(", %u", i->RISCV64in.SRAI.uimm6);
+      return;
+   case RISCV64in_SLLW:
+      vex_printf("sllw    ");
+      ppHRegRISCV64(i->RISCV64in.SLLW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLLW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLLW.src2);
+      return;
+   case RISCV64in_SRLW:
+      vex_printf("srlw    ");
+      ppHRegRISCV64(i->RISCV64in.SRLW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRLW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRLW.src2);
+      return;
+   case RISCV64in_SRAW:
+      vex_printf("sraw    ");
+      ppHRegRISCV64(i->RISCV64in.SRAW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRAW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SRAW.src2);
+      return;
+   case RISCV64in_SLT:
+      vex_printf("slt     ");
+      ppHRegRISCV64(i->RISCV64in.SLT.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLT.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLT.src2);
+      return;
+   case RISCV64in_SLTU:
+      vex_printf("sltu    ");
+      ppHRegRISCV64(i->RISCV64in.SLTU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLTU.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLTU.src2);
+      return;
+   case RISCV64in_SLTIU:
+      vex_printf("sltiu   ");
+      ppHRegRISCV64(i->RISCV64in.SLTIU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SLTIU.src);
+      vex_printf(", %d", i->RISCV64in.SLTIU.simm12);
+      return;
+   case RISCV64in_CSRRW:
+      vex_printf("csrrw   ");
+      ppHRegRISCV64(i->RISCV64in.CSRRW.dst);
+      vex_printf(", %s, ", showRISCV64CSR(i->RISCV64in.CSRRW.csr));
+      ppHRegRISCV64(i->RISCV64in.CSRRW.src);
+      return;
+   case RISCV64in_MUL:
+      vex_printf("mul     ");
+      ppHRegRISCV64(i->RISCV64in.MUL.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MUL.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MUL.src2);
+      return;
+   case RISCV64in_MULH:
+      vex_printf("mulh    ");
+      ppHRegRISCV64(i->RISCV64in.MULH.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MULH.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MULH.src2);
+      return;
+   case RISCV64in_MULHU:
+      vex_printf("mulhu   ");
+      ppHRegRISCV64(i->RISCV64in.MULHU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MULHU.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MULHU.src2);
+      return;
+   case RISCV64in_DIV:
+      vex_printf("div     ");
+      ppHRegRISCV64(i->RISCV64in.DIV.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIV.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIV.src2);
+      return;
+   case RISCV64in_DIVU:
+      vex_printf("divu    ");
+      ppHRegRISCV64(i->RISCV64in.DIVU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIVU.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIVU.src2);
+      return;
+   case RISCV64in_REM:
+      vex_printf("rem     ");
+      ppHRegRISCV64(i->RISCV64in.REM.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REM.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REM.src2);
+      return;
+   case RISCV64in_REMU:
+      vex_printf("remu    ");
+      ppHRegRISCV64(i->RISCV64in.REMU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REMU.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REMU.src2);
+      return;
+   case RISCV64in_MULW:
+      vex_printf("mulw    ");
+      ppHRegRISCV64(i->RISCV64in.MULW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MULW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.MULW.src2);
+      return;
+   case RISCV64in_DIVW:
+      vex_printf("divw    ");
+      ppHRegRISCV64(i->RISCV64in.DIVW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIVW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIVW.src2);
+      return;
+   case RISCV64in_DIVUW:
+      vex_printf("divuw   ");
+      ppHRegRISCV64(i->RISCV64in.DIVUW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIVUW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.DIVUW.src2);
+      return;
+   case RISCV64in_REMW:
+      vex_printf("remw    ");
+      ppHRegRISCV64(i->RISCV64in.REMW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REMW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REMW.src2);
+      return;
+   case RISCV64in_REMUW:
+      vex_printf("remuw   ");
+      ppHRegRISCV64(i->RISCV64in.REMUW.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REMUW.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.REMUW.src2);
+      return;
+   case RISCV64in_LD:
+      vex_printf("ld      ");
+      ppHRegRISCV64(i->RISCV64in.LD.dst);
+      vex_printf(", %d(", i->RISCV64in.LD.soff12);
+      ppHRegRISCV64(i->RISCV64in.LD.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_LW:
+      vex_printf("lw      ");
+      ppHRegRISCV64(i->RISCV64in.LW.dst);
+      vex_printf(", %d(", i->RISCV64in.LW.soff12);
+      ppHRegRISCV64(i->RISCV64in.LW.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_LH:
+      vex_printf("lh      ");
+      ppHRegRISCV64(i->RISCV64in.LH.dst);
+      vex_printf(", %d(", i->RISCV64in.LH.soff12);
+      ppHRegRISCV64(i->RISCV64in.LH.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_LB:
+      vex_printf("lb      ");
+      ppHRegRISCV64(i->RISCV64in.LB.dst);
+      vex_printf(", %d(", i->RISCV64in.LB.soff12);
+      ppHRegRISCV64(i->RISCV64in.LB.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_SD:
+      vex_printf("sd      ");
+      ppHRegRISCV64(i->RISCV64in.SD.src);
+      vex_printf(", %d(", i->RISCV64in.SD.soff12);
+      ppHRegRISCV64(i->RISCV64in.SD.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_SW:
+      vex_printf("sw      ");
+      ppHRegRISCV64(i->RISCV64in.SW.src);
+      vex_printf(", %d(", i->RISCV64in.SW.soff12);
+      ppHRegRISCV64(i->RISCV64in.SW.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_SH:
+      vex_printf("sh      ");
+      ppHRegRISCV64(i->RISCV64in.SH.src);
+      vex_printf(", %d(", i->RISCV64in.SH.soff12);
+      ppHRegRISCV64(i->RISCV64in.SH.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_SB:
+      vex_printf("sb      ");
+      ppHRegRISCV64(i->RISCV64in.SB.src);
+      vex_printf(", %d(", i->RISCV64in.SB.soff12);
+      ppHRegRISCV64(i->RISCV64in.SB.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_LR_W:
+      vex_printf("lr.w    ");
+      ppHRegRISCV64(i->RISCV64in.LR_W.dst);
+      vex_printf(", (");
+      ppHRegRISCV64(i->RISCV64in.LR_W.addr);
+      vex_printf(")");
+      return;
+   case RISCV64in_SC_W:
+      vex_printf("sc.w    ");
+      ppHRegRISCV64(i->RISCV64in.SC_W.res);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.SC_W.src);
+      vex_printf(", (");
+      ppHRegRISCV64(i->RISCV64in.SC_W.addr);
+      vex_printf(")");
+      return;
+   case RISCV64in_FMADD_S:
+      vex_printf("fmadd.s ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_S.src2);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_S.src3);
+      return;
+   case RISCV64in_FADD_S:
+      vex_printf("fadd.s  ");
+      ppHRegRISCV64(i->RISCV64in.FADD_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FADD_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FADD_S.src2);
+      return;
+   case RISCV64in_FMUL_S:
+      vex_printf("fmul.s  ");
+      ppHRegRISCV64(i->RISCV64in.FMUL_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMUL_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMUL_S.src2);
+      return;
+   case RISCV64in_FDIV_S:
+      vex_printf("fdiv.s  ");
+      ppHRegRISCV64(i->RISCV64in.FDIV_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FDIV_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FDIV_S.src2);
+      return;
+   case RISCV64in_FSQRT_S:
+      vex_printf("fsqrt.s ");
+      ppHRegRISCV64(i->RISCV64in.FSQRT_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSQRT_S.src1);
+      return;
+   case RISCV64in_FSGNJN_S:
+      vex_printf("fsgnjn.s ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJN_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJN_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJN_S.src2);
+      return;
+   case RISCV64in_FSGNJX_S:
+      vex_printf("fsgnjx.s ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJX_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJX_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJX_S.src2);
+      return;
+   case RISCV64in_FMIN_S:
+      vex_printf("fmin.s  ");
+      ppHRegRISCV64(i->RISCV64in.FMIN_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMIN_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMIN_S.src2);
+      return;
+   case RISCV64in_FMAX_S:
+      vex_printf("fmax.s  ");
+      ppHRegRISCV64(i->RISCV64in.FMAX_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMAX_S.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMAX_S.src2);
+      return;
+   case RISCV64in_FMV_X_W:
+      vex_printf("fmv.x.w  ");
+      ppHRegRISCV64(i->RISCV64in.FMV_X_W.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMV_X_W.src);
+      return;
+   case RISCV64in_FMV_W_X:
+      vex_printf("fmv.w.x  ");
+      ppHRegRISCV64(i->RISCV64in.FMV_W_X.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMV_W_X.src);
+      return;
+   case RISCV64in_FMV_D:
+      vex_printf("fmv.d   ");
+      ppHRegRISCV64(i->RISCV64in.FMV_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMV_D.src);
+      return;
+   case RISCV64in_FMADD_D:
+      vex_printf("fmadd.d ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_D.src2);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMADD_D.src3);
+      return;
+   case RISCV64in_FADD_D:
+      vex_printf("fadd.d  ");
+      ppHRegRISCV64(i->RISCV64in.FADD_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FADD_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FADD_D.src2);
+      return;
+   case RISCV64in_FSUB_D:
+      vex_printf("fsub.d  ");
+      ppHRegRISCV64(i->RISCV64in.FSUB_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSUB_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSUB_D.src2);
+      return;
+   case RISCV64in_FMUL_D:
+      vex_printf("fmul.d  ");
+      ppHRegRISCV64(i->RISCV64in.FMUL_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMUL_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMUL_D.src2);
+      return;
+   case RISCV64in_FDIV_D:
+      vex_printf("fdiv.d  ");
+      ppHRegRISCV64(i->RISCV64in.FDIV_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FDIV_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FDIV_D.src2);
+      return;
+   case RISCV64in_FSQRT_D:
+      vex_printf("fsqrt.d ");
+      ppHRegRISCV64(i->RISCV64in.FSQRT_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSQRT_D.src1);
+      return;
+   case RISCV64in_FSGNJN_D:
+      vex_printf("fsgnjn.d ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJN_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJN_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJN_D.src2);
+      return;
+   case RISCV64in_FSGNJX_D:
+      vex_printf("fsgnjx.d ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJX_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJX_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FSGNJX_D.src2);
+      return;
+   case RISCV64in_FMIN_D:
+      vex_printf("fmin.d  ");
+      ppHRegRISCV64(i->RISCV64in.FMIN_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMIN_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMIN_D.src2);
+      return;
+   case RISCV64in_FMAX_D:
+      vex_printf("fmax.d  ");
+      ppHRegRISCV64(i->RISCV64in.FMAX_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMAX_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMAX_D.src2);
+      return;
+   case RISCV64in_FEQ_D:
+      vex_printf("feq.d   ");
+      ppHRegRISCV64(i->RISCV64in.FEQ_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FEQ_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FEQ_D.src2);
+      return;
+   case RISCV64in_FLT_D:
+      vex_printf("flt.d   ");
+      ppHRegRISCV64(i->RISCV64in.FLT_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FLT_D.src1);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FLT_D.src2);
+      return;
+   case RISCV64in_FCVT_S_D:
+      vex_printf("fcvt.s.d ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_S_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_S_D.src);
+      return;
+   case RISCV64in_FCVT_D_S:
+      vex_printf("fcvt.d.s ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_S.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_S.src);
+      return;
+   case RISCV64in_FCVT_W_D:
+      vex_printf("fcvt.w.d ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_W_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_W_D.src);
+      return;
+   case RISCV64in_FCVT_WU_D:
+      vex_printf("fcvt.wu.d ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_WU_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_WU_D.src);
+      return;
+   case RISCV64in_FCVT_D_W:
+      vex_printf("fcvt.d.w ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_W.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_W.src);
+      return;
+   case RISCV64in_FCVT_D_WU:
+      vex_printf("fcvt.d.wu ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_WU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_WU.src);
+      return;
+   case RISCV64in_FCVT_L_D:
+      vex_printf("fcvt.l.d ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_L_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_L_D.src);
+      return;
+   case RISCV64in_FCVT_LU_D:
+      vex_printf("fcvt.lu.d ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_LU_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_LU_D.src);
+      return;
+   case RISCV64in_FCVT_D_L:
+      vex_printf("fcvt.d.l ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_L.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_L.src);
+      return;
+   case RISCV64in_FCVT_D_LU:
+      vex_printf("fcvt.d.lu ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_LU.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FCVT_D_LU.src);
+      return;
+   case RISCV64in_FMV_X_D:
+      vex_printf("fmv.x.d  ");
+      ppHRegRISCV64(i->RISCV64in.FMV_X_D.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMV_X_D.src);
+      return;
+   case RISCV64in_FMV_D_X:
+      vex_printf("fmv.d.x  ");
+      ppHRegRISCV64(i->RISCV64in.FMV_D_X.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.FMV_D_X.src);
+      return;
+   case RISCV64in_FLD:
+      vex_printf("fld     ");
+      ppHRegRISCV64(i->RISCV64in.FLD.dst);
+      vex_printf(", %d(", i->RISCV64in.FLD.soff12);
+      ppHRegRISCV64(i->RISCV64in.FLD.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_FLW:
+      vex_printf("flw     ");
+      ppHRegRISCV64(i->RISCV64in.FLW.dst);
+      vex_printf(", %d(", i->RISCV64in.FLW.soff12);
+      ppHRegRISCV64(i->RISCV64in.FLW.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_FSD:
+      vex_printf("fsd     ");
+      ppHRegRISCV64(i->RISCV64in.FSD.src);
+      vex_printf(", %d(", i->RISCV64in.FSD.soff12);
+      ppHRegRISCV64(i->RISCV64in.FSD.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_FSW:
+      vex_printf("fsw     ");
+      ppHRegRISCV64(i->RISCV64in.FSW.src);
+      vex_printf(", %d(", i->RISCV64in.FSW.soff12);
+      ppHRegRISCV64(i->RISCV64in.FSW.base);
+      vex_printf(")");
+      return;
+   case RISCV64in_CAS_W:
+      vex_printf("(CAS_W) 1: lr.w ");
+      ppHRegRISCV64(i->RISCV64in.CAS_W.old);
+      vex_printf(", (");
+      ppHRegRISCV64(i->RISCV64in.CAS_W.addr);
+      vex_printf("); bne ");
+      ppHRegRISCV64(i->RISCV64in.CAS_W.old);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.CAS_W.expd);
+      vex_printf(", 2f; sc.w t0, ");
+      ppHRegRISCV64(i->RISCV64in.CAS_W.data);
+      vex_printf(", (");
+      ppHRegRISCV64(i->RISCV64in.CAS_W.addr);
+      vex_printf("); bne t0, zero, 1b; 2:");
+      return;
+   case RISCV64in_CAS_D:
+      vex_printf("(CAS_D) 1: lr.d ");
+      ppHRegRISCV64(i->RISCV64in.CAS_D.old);
+      vex_printf(", (");
+      ppHRegRISCV64(i->RISCV64in.CAS_D.addr);
+      vex_printf("); bne ");
+      ppHRegRISCV64(i->RISCV64in.CAS_D.old);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.CAS_D.expd);
+      vex_printf(", 2f; sc.d t0, ");
+      ppHRegRISCV64(i->RISCV64in.CAS_D.data);
+      vex_printf(", (");
+      ppHRegRISCV64(i->RISCV64in.CAS_D.addr);
+      vex_printf("); bne t0, zero, 1b; 2:");
+      return;
+   case RISCV64in_FENCE:
+      vex_printf("fence");
+      return;
+   case RISCV64in_CSEL:
+      vex_printf("(CSEL) beq ");
+      ppHRegRISCV64(i->RISCV64in.CSEL.cond);
+      vex_printf(", zero, 1f; c.mv ");
+      ppHRegRISCV64(i->RISCV64in.CSEL.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.CSEL.iftrue);
+      vex_printf("; c.j 2f; 1: c.mv ");
+      ppHRegRISCV64(i->RISCV64in.CSEL.dst);
+      vex_printf(", ");
+      ppHRegRISCV64(i->RISCV64in.CSEL.iffalse);
+      vex_printf("; 2:");
+      return;
+   case RISCV64in_Call:
+      vex_printf("(Call) ");
+      if (!hregIsInvalid(i->RISCV64in.Call.cond)) {
+         vex_printf("beq ");
+         ppHRegRISCV64(i->RISCV64in.Call.cond);
+         vex_printf(", zero, 1f; ");
+      }
+      vex_printf("li t0, 0x%llx; c.jalr 0(t0) [nArgRegs=%u, nFArgRegs=%u, ",
+                 i->RISCV64in.Call.target, i->RISCV64in.Call.nArgRegs,
+                 i->RISCV64in.Call.nFArgRegs);
+      ppRetLoc(i->RISCV64in.Call.rloc);
+      vex_printf("]; 1:");
+      return;
+   case RISCV64in_XDirect:
+      vex_printf("(xDirect) ");
+      if (!hregIsInvalid(i->RISCV64in.XDirect.cond)) {
+         vex_printf("beq ");
+         ppHRegRISCV64(i->RISCV64in.XDirect.cond);
+         vex_printf(", zero, 1f; ");
+      }
+      vex_printf("li t0, 0x%llx; sd t0, %d(", i->RISCV64in.XDirect.dstGA,
+                 i->RISCV64in.XDirect.soff12);
+      ppHRegRISCV64(i->RISCV64in.XDirect.base);
+      vex_printf("); li t0, <%s>; c.jalr 0(t0); 1:",
+                 i->RISCV64in.XDirect.toFastEP ? "disp_cp_chain_me_to_fastEP"
+                                               : "disp_cp_chain_me_to_slowEP");
+      return;
+   case RISCV64in_XIndir:
+      vex_printf("(xIndir) ");
+      if (!hregIsInvalid(i->RISCV64in.XIndir.cond)) {
+         vex_printf("beq ");
+         ppHRegRISCV64(i->RISCV64in.XIndir.cond);
+         vex_printf(", zero, 1f; ");
+      }
+      vex_printf("sd ");
+      ppHRegRISCV64(i->RISCV64in.XIndir.dstGA);
+      vex_printf(", %d(", i->RISCV64in.XIndir.soff12);
+      ppHRegRISCV64(i->RISCV64in.XIndir.base);
+      vex_printf("); li t0, <disp_cp_xindir>; c.jr 0(t0); 1:");
+      return;
+   case RISCV64in_XAssisted:
+      vex_printf("(xAssisted) ");
+      if (!hregIsInvalid(i->RISCV64in.XAssisted.cond)) {
+         vex_printf("beq ");
+         ppHRegRISCV64(i->RISCV64in.XAssisted.cond);
+         vex_printf(", zero, 1f; ");
+      }
+      vex_printf("sd ");
+      ppHRegRISCV64(i->RISCV64in.XAssisted.dstGA);
+      vex_printf(", %d(", i->RISCV64in.XAssisted.soff12);
+      ppHRegRISCV64(i->RISCV64in.XAssisted.base);
+      vex_printf("); mv s0, $IRJumpKind_to_TRCVAL(%d)",
+                 (Int)i->RISCV64in.XAssisted.jk);
+      vex_printf("; li t0, <disp_cp_xassisted>; c.jr 0(t0); 1:");
+      return;
+   case RISCV64in_EvCheck:
+      vex_printf("(evCheck) lw t0, %d(", i->RISCV64in.EvCheck.soff12_amCounter);
+      ppHRegRISCV64(i->RISCV64in.EvCheck.base_amCounter);
+      vex_printf("); c.addiw t0, -1; sw t0, %d(",
+                 i->RISCV64in.EvCheck.soff12_amCounter);
+      ppHRegRISCV64(i->RISCV64in.EvCheck.base_amCounter);
+      vex_printf("); bge t0, zero, 1f; ld t0, %d(",
+                 i->RISCV64in.EvCheck.soff12_amFailAddr);
+      ppHRegRISCV64(i->RISCV64in.EvCheck.base_amFailAddr);
+      vex_printf("); c.jr 0(t0); 1:");
+      return;
+   default:
+      vpanic("ppRISCV64Instr");
+   }
+}
+
+/*------------------------------------------------------------*/
+/*--- Helpers for register allocation                      ---*/
+/*------------------------------------------------------------*/
+
+/* Initialise and return the "register universe", i.e. a list of all hardware
+   registers. Called once. */
+const RRegUniverse* getRRegUniverse_RISCV64(void)
+{
+   static RRegUniverse all_regs;
+   static Bool         initialised = False;
+   RRegUniverse*       ru          = &all_regs;
+
+   if (LIKELY(initialised))
+      return ru;
+
+   RRegUniverse__init(ru);
+
+   /* Add the registers that are available to the register allocator. */
+   /* TODO */
+   ru->allocable_start[HRcInt64] = ru->size;
+   ru->regs[ru->size++]          = hregRISCV64_x18(); /* s2 */
+   ru->regs[ru->size++]          = hregRISCV64_x19(); /* s3 */
+   ru->regs[ru->size++]          = hregRISCV64_x20(); /* s4 */
+   ru->regs[ru->size++]          = hregRISCV64_x21(); /* s5 */
+   ru->regs[ru->size++]          = hregRISCV64_x22(); /* s6 */
+   ru->regs[ru->size++]          = hregRISCV64_x23(); /* s7 */
+   ru->regs[ru->size++]          = hregRISCV64_x24(); /* s8 */
+   ru->regs[ru->size++]          = hregRISCV64_x25(); /* s9 */
+   ru->regs[ru->size++]          = hregRISCV64_x26(); /* s10 */
+   ru->regs[ru->size++]          = hregRISCV64_x27(); /* s11 */
+   ru->regs[ru->size++]          = hregRISCV64_x10(); /* a0 */
+   ru->regs[ru->size++]          = hregRISCV64_x11(); /* a1 */
+   ru->regs[ru->size++]          = hregRISCV64_x12(); /* a2 */
+   ru->regs[ru->size++]          = hregRISCV64_x13(); /* a3 */
+   ru->regs[ru->size++]          = hregRISCV64_x14(); /* a4 */
+   ru->regs[ru->size++]          = hregRISCV64_x15(); /* a5 */
+   ru->regs[ru->size++]          = hregRISCV64_x16(); /* a6 */
+   ru->regs[ru->size++]          = hregRISCV64_x17(); /* a7 */
+   ru->allocable_end[HRcInt64]   = ru->size - 1;
+
+   /* Floating-point registers, all of which are caller-saved. */
+   ru->allocable_start[HRcFlt64] = ru->size;
+   ru->regs[ru->size++]          = hregRISCV64_f0();  /* ft0 */
+   ru->regs[ru->size++]          = hregRISCV64_f1();  /* ft1 */
+   ru->regs[ru->size++]          = hregRISCV64_f2();  /* ft2 */
+   ru->regs[ru->size++]          = hregRISCV64_f3();  /* ft3 */
+   ru->regs[ru->size++]          = hregRISCV64_f4();  /* ft4 */
+   ru->regs[ru->size++]          = hregRISCV64_f5();  /* ft5 */
+   ru->regs[ru->size++]          = hregRISCV64_f6();  /* ft6 */
+   ru->regs[ru->size++]          = hregRISCV64_f7();  /* ft7 */
+   ru->regs[ru->size++]          = hregRISCV64_f10(); /* fa0 */
+   ru->regs[ru->size++]          = hregRISCV64_f11(); /* fa1 */
+   ru->regs[ru->size++]          = hregRISCV64_f12(); /* fa2 */
+   ru->regs[ru->size++]          = hregRISCV64_f13(); /* fa3 */
+   ru->regs[ru->size++]          = hregRISCV64_f14(); /* fa4 */
+   ru->regs[ru->size++]          = hregRISCV64_f15(); /* fa5 */
+   ru->regs[ru->size++]          = hregRISCV64_f16(); /* fa6 */
+   ru->regs[ru->size++]          = hregRISCV64_f17(); /* fa7 */
+   ru->regs[ru->size++]          = hregRISCV64_f28(); /* ft8 */
+   ru->regs[ru->size++]          = hregRISCV64_f29(); /* ft9 */
+   ru->regs[ru->size++]          = hregRISCV64_f30(); /* ft10 */
+   ru->regs[ru->size++]          = hregRISCV64_f31(); /* ft11 */
+   ru->allocable_end[HRcFlt64]   = ru->size - 1;
+   ru->allocable                 = ru->size;
+
+   /* Add the registers that are not available for allocation. */
+   /* TODO */
+   ru->regs[ru->size++] = hregRISCV64_x0(); /* zero */
+   ru->regs[ru->size++] = hregRISCV64_x2(); /* sp */
+   ru->regs[ru->size++] = hregRISCV64_x8(); /* s0 */
+
+   initialised = True;
+
+   RRegUniverse__check_is_sane(ru);
+   return ru;
+}
+
+/* Tell the register allocator how the given instruction uses the registers it
+   refers to. */
+void getRegUsage_RISCV64Instr(HRegUsage* u, const RISCV64Instr* i, Bool mode64)
+{
+   vassert(mode64 == True);
+
+   initHRegUsage(u);
+   switch (i->tag) {
+   case RISCV64in_LI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.LI.dst);
+      return;
+   case RISCV64in_MV:
+      addHRegUse(u, HRmWrite, i->RISCV64in.MV.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.MV.src);
+      return;
+   case RISCV64in_ADD:
+      addHRegUse(u, HRmWrite, i->RISCV64in.ADD.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.ADD.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.ADD.src2);
+      return;
+   case RISCV64in_ADDI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.ADDI.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.ADDI.src);
+      return;
+   case RISCV64in_ADDW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.ADDW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.ADDW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.ADDW.src2);
+      return;
+   case RISCV64in_ADDIW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.ADDIW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.ADDIW.src);
+      return;
+   case RISCV64in_SUB:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SUB.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SUB.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SUB.src2);
+      return;
+   case RISCV64in_SUBW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SUBW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SUBW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SUBW.src2);
+      return;
+   case RISCV64in_XOR:
+      addHRegUse(u, HRmWrite, i->RISCV64in.XOR.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.XOR.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.XOR.src2);
+      return;
+   case RISCV64in_XORI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.XORI.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.XORI.src);
+      return;
+   case RISCV64in_OR:
+      addHRegUse(u, HRmWrite, i->RISCV64in.OR.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.OR.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.OR.src2);
+      return;
+   case RISCV64in_AND:
+      addHRegUse(u, HRmWrite, i->RISCV64in.AND.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.AND.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.AND.src2);
+      return;
+   case RISCV64in_ANDI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.ANDI.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.ANDI.src);
+      return;
+   case RISCV64in_SLL:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SLL.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLL.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLL.src2);
+      return;
+   case RISCV64in_SRL:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SRL.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRL.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRL.src2);
+      return;
+   case RISCV64in_SRA:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SRA.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRA.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRA.src2);
+      return;
+   case RISCV64in_SLLI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SLLI.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLLI.src);
+      return;
+   case RISCV64in_SRLI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SRLI.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRLI.src);
+      return;
+   case RISCV64in_SRAI:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SRAI.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRAI.src);
+      return;
+   case RISCV64in_SLLW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SLLW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLLW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLLW.src2);
+      return;
+   case RISCV64in_SRLW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SRLW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRLW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRLW.src2);
+      return;
+   case RISCV64in_SRAW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SRAW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRAW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SRAW.src2);
+      return;
+   case RISCV64in_SLT:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SLT.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLT.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLT.src2);
+      return;
+   case RISCV64in_SLTU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SLTU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLTU.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLTU.src2);
+      return;
+   case RISCV64in_SLTIU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SLTIU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.SLTIU.src);
+      return;
+   case RISCV64in_CSRRW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.CSRRW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.CSRRW.src);
+      return;
+   case RISCV64in_MUL:
+      addHRegUse(u, HRmWrite, i->RISCV64in.MUL.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.MUL.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.MUL.src2);
+      return;
+   case RISCV64in_MULH:
+      addHRegUse(u, HRmWrite, i->RISCV64in.MULH.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.MULH.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.MULH.src2);
+      return;
+   case RISCV64in_MULHU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.MULHU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.MULHU.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.MULHU.src2);
+      return;
+   case RISCV64in_DIV:
+      addHRegUse(u, HRmWrite, i->RISCV64in.DIV.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIV.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIV.src2);
+      return;
+   case RISCV64in_DIVU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.DIVU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIVU.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIVU.src2);
+      return;
+   case RISCV64in_REM:
+      addHRegUse(u, HRmWrite, i->RISCV64in.REM.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.REM.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.REM.src2);
+      return;
+   case RISCV64in_REMU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.REMU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.REMU.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.REMU.src2);
+      return;
+   case RISCV64in_MULW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.MULW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.MULW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.MULW.src2);
+      return;
+   case RISCV64in_DIVW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.DIVW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIVW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIVW.src2);
+      return;
+   case RISCV64in_DIVUW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.DIVUW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIVUW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.DIVUW.src2);
+      return;
+   case RISCV64in_REMW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.REMW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.REMW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.REMW.src2);
+      return;
+   case RISCV64in_REMUW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.REMUW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.REMUW.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.REMUW.src2);
+      return;
+   case RISCV64in_LD:
+      addHRegUse(u, HRmWrite, i->RISCV64in.LD.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.LD.base);
+      return;
+   case RISCV64in_LW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.LW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.LW.base);
+      return;
+   case RISCV64in_LH:
+      addHRegUse(u, HRmWrite, i->RISCV64in.LH.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.LH.base);
+      return;
+   case RISCV64in_LB:
+      addHRegUse(u, HRmWrite, i->RISCV64in.LB.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.LB.base);
+      return;
+   case RISCV64in_SD:
+      addHRegUse(u, HRmRead, i->RISCV64in.SD.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.SD.base);
+      return;
+   case RISCV64in_SW:
+      addHRegUse(u, HRmRead, i->RISCV64in.SW.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.SW.base);
+      return;
+   case RISCV64in_SH:
+      addHRegUse(u, HRmRead, i->RISCV64in.SH.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.SH.base);
+      return;
+   case RISCV64in_SB:
+      addHRegUse(u, HRmRead, i->RISCV64in.SB.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.SB.base);
+      return;
+   case RISCV64in_LR_W:
+      addHRegUse(u, HRmWrite, i->RISCV64in.LR_W.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.LR_W.addr);
+      return;
+   case RISCV64in_SC_W:
+      addHRegUse(u, HRmWrite, i->RISCV64in.SC_W.res);
+      addHRegUse(u, HRmRead, i->RISCV64in.SC_W.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.SC_W.addr);
+      return;
+   case RISCV64in_FMADD_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMADD_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMADD_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMADD_S.src2);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMADD_S.src3);
+      return;
+   case RISCV64in_FADD_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FADD_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FADD_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FADD_S.src2);
+      return;
+   case RISCV64in_FMUL_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMUL_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMUL_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMUL_S.src2);
+      return;
+   case RISCV64in_FDIV_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FDIV_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FDIV_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FDIV_S.src2);
+      return;
+   case RISCV64in_FSQRT_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSQRT_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSQRT_S.src1);
+      return;
+   case RISCV64in_FSGNJN_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSGNJN_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJN_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJN_S.src2);
+      return;
+   case RISCV64in_FSGNJX_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSGNJX_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJX_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJX_S.src2);
+      return;
+   case RISCV64in_FMIN_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMIN_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMIN_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMIN_S.src2);
+      return;
+   case RISCV64in_FMAX_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMAX_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMAX_S.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMAX_S.src2);
+      return;
+   case RISCV64in_FMV_X_W:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMV_X_W.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMV_X_W.src);
+      return;
+   case RISCV64in_FMV_W_X:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMV_W_X.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMV_W_X.src);
+      return;
+   case RISCV64in_FMV_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMV_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMV_D.src);
+      return;
+   case RISCV64in_FMADD_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMADD_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMADD_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMADD_D.src2);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMADD_D.src3);
+      return;
+   case RISCV64in_FADD_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FADD_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FADD_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FADD_D.src2);
+      return;
+   case RISCV64in_FSUB_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSUB_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSUB_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSUB_D.src2);
+      return;
+   case RISCV64in_FMUL_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMUL_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMUL_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMUL_D.src2);
+      return;
+   case RISCV64in_FDIV_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FDIV_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FDIV_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FDIV_D.src2);
+      return;
+   case RISCV64in_FSQRT_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSQRT_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSQRT_D.src1);
+      return;
+   case RISCV64in_FSGNJN_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSGNJN_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJN_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJN_D.src2);
+      return;
+   case RISCV64in_FSGNJX_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FSGNJX_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJX_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSGNJX_D.src2);
+      return;
+   case RISCV64in_FMIN_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMIN_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMIN_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMIN_D.src2);
+      return;
+   case RISCV64in_FMAX_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMAX_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMAX_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMAX_D.src2);
+      return;
+   case RISCV64in_FEQ_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FEQ_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FEQ_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FEQ_D.src2);
+      return;
+   case RISCV64in_FLT_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FLT_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FLT_D.src1);
+      addHRegUse(u, HRmRead, i->RISCV64in.FLT_D.src2);
+      return;
+   case RISCV64in_FCVT_S_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_S_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_S_D.src);
+      return;
+   case RISCV64in_FCVT_D_S:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_D_S.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_D_S.src);
+      return;
+   case RISCV64in_FCVT_W_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_W_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_W_D.src);
+      return;
+   case RISCV64in_FCVT_WU_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_WU_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_WU_D.src);
+      return;
+   case RISCV64in_FCVT_D_W:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_D_W.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_D_W.src);
+      return;
+   case RISCV64in_FCVT_D_WU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_D_WU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_D_WU.src);
+      return;
+   case RISCV64in_FCVT_L_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_L_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_L_D.src);
+      return;
+   case RISCV64in_FCVT_LU_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_LU_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_LU_D.src);
+      return;
+   case RISCV64in_FCVT_D_L:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_D_L.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_D_L.src);
+      return;
+   case RISCV64in_FCVT_D_LU:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FCVT_D_LU.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FCVT_D_LU.src);
+      return;
+   case RISCV64in_FMV_X_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMV_X_D.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMV_X_D.src);
+      return;
+   case RISCV64in_FMV_D_X:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FMV_D_X.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FMV_D_X.src);
+      return;
+   case RISCV64in_FLD:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FLD.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FLD.base);
+      return;
+   case RISCV64in_FLW:
+      addHRegUse(u, HRmWrite, i->RISCV64in.FLW.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.FLW.base);
+      return;
+   case RISCV64in_FSD:
+      addHRegUse(u, HRmRead, i->RISCV64in.FSD.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSD.base);
+      return;
+   case RISCV64in_FSW:
+      addHRegUse(u, HRmRead, i->RISCV64in.FSW.src);
+      addHRegUse(u, HRmRead, i->RISCV64in.FSW.base);
+      return;
+   case RISCV64in_CAS_W:
+      addHRegUse(u, HRmWrite, i->RISCV64in.CAS_W.old);
+      addHRegUse(u, HRmRead, i->RISCV64in.CAS_W.addr);
+      addHRegUse(u, HRmRead, i->RISCV64in.CAS_W.expd);
+      addHRegUse(u, HRmRead, i->RISCV64in.CAS_W.data);
+      return;
+   case RISCV64in_CAS_D:
+      addHRegUse(u, HRmWrite, i->RISCV64in.CAS_D.old);
+      addHRegUse(u, HRmRead, i->RISCV64in.CAS_D.addr);
+      addHRegUse(u, HRmRead, i->RISCV64in.CAS_D.expd);
+      addHRegUse(u, HRmRead, i->RISCV64in.CAS_D.data);
+      return;
+   case RISCV64in_FENCE:
+      return;
+   case RISCV64in_CSEL:
+      addHRegUse(u, HRmWrite, i->RISCV64in.CSEL.dst);
+      addHRegUse(u, HRmRead, i->RISCV64in.CSEL.iftrue);
+      addHRegUse(u, HRmRead, i->RISCV64in.CSEL.iffalse);
+      addHRegUse(u, HRmRead, i->RISCV64in.CSEL.cond);
+      return;
+   case RISCV64in_Call:
+      /* Logic and comments copied/modified from the arm64 backend. */
+      /* First off, claim it trashes all the caller-saved registers which fall
+         within the register allocator's jurisdiction. */
+      addHRegUse(u, HRmWrite, hregRISCV64_x10());
+      addHRegUse(u, HRmWrite, hregRISCV64_x11());
+      addHRegUse(u, HRmWrite, hregRISCV64_x12());
+      addHRegUse(u, HRmWrite, hregRISCV64_x13());
+      addHRegUse(u, HRmWrite, hregRISCV64_x14());
+      addHRegUse(u, HRmWrite, hregRISCV64_x15());
+      addHRegUse(u, HRmWrite, hregRISCV64_x16());
+      addHRegUse(u, HRmWrite, hregRISCV64_x17());
+      addHRegUse(u, HRmWrite, hregRISCV64_f0());
+      addHRegUse(u, HRmWrite, hregRISCV64_f1());
+      addHRegUse(u, HRmWrite, hregRISCV64_f2());
+      addHRegUse(u, HRmWrite, hregRISCV64_f3());
+      addHRegUse(u, HRmWrite, hregRISCV64_f4());
+      addHRegUse(u, HRmWrite, hregRISCV64_f5());
+      addHRegUse(u, HRmWrite, hregRISCV64_f6());
+      addHRegUse(u, HRmWrite, hregRISCV64_f7());
+      addHRegUse(u, HRmWrite, hregRISCV64_f10());
+      addHRegUse(u, HRmWrite, hregRISCV64_f11());
+      addHRegUse(u, HRmWrite, hregRISCV64_f12());
+      addHRegUse(u, HRmWrite, hregRISCV64_f13());
+      addHRegUse(u, HRmWrite, hregRISCV64_f14());
+      addHRegUse(u, HRmWrite, hregRISCV64_f15());
+      addHRegUse(u, HRmWrite, hregRISCV64_f16());
+      addHRegUse(u, HRmWrite, hregRISCV64_f17());
+      addHRegUse(u, HRmWrite, hregRISCV64_f28());
+      addHRegUse(u, HRmWrite, hregRISCV64_f29());
+      addHRegUse(u, HRmWrite, hregRISCV64_f30());
+      addHRegUse(u, HRmWrite, hregRISCV64_f31());
+      /* Now we have to state any parameter-carrying registers which might be
+         read. This depends on nArgRegs and nFArgRegs. */
+      switch (i->RISCV64in.Call.nArgRegs) {
+      case 8:
+         addHRegUse(u, HRmRead, hregRISCV64_x17()); /*fallthru*/
+      case 7:
+         addHRegUse(u, HRmRead, hregRISCV64_x16()); /*fallthru*/
+      case 6:
+         addHRegUse(u, HRmRead, hregRISCV64_x15()); /*fallthru*/
+      case 5:
+         addHRegUse(u, HRmRead, hregRISCV64_x14()); /*fallthru*/
+      case 4:
+         addHRegUse(u, HRmRead, hregRISCV64_x13()); /*fallthru*/
+      case 3:
+         addHRegUse(u, HRmRead, hregRISCV64_x12()); /*fallthru*/
+      case 2:
+         addHRegUse(u, HRmRead, hregRISCV64_x11()); /*fallthru*/
+      case 1:
+         addHRegUse(u, HRmRead, hregRISCV64_x10());
+         break;
+      case 0:
+         break;
+      default:
+         vpanic("getRegUsage_RISCV64Instr:Call:regparms");
+      }
+      switch (i->RISCV64in.Call.nFArgRegs) {
+      case 8:
+         addHRegUse(u, HRmRead, hregRISCV64_f17()); /*fallthru*/
+      case 7:
+         addHRegUse(u, HRmRead, hregRISCV64_f16()); /*fallthru*/
+      case 6:
+         addHRegUse(u, HRmRead, hregRISCV64_f15()); /*fallthru*/
+      case 5:
+         addHRegUse(u, HRmRead, hregRISCV64_f14()); /*fallthru*/
+      case 4:
+         addHRegUse(u, HRmRead, hregRISCV64_f13()); /*fallthru*/
+      case 3:
+         addHRegUse(u, HRmRead, hregRISCV64_f12()); /*fallthru*/
+      case 2:
+         addHRegUse(u, HRmRead, hregRISCV64_f11()); /*fallthru*/
+      case 1:
+         addHRegUse(u, HRmRead, hregRISCV64_f10());
+         break;
+      case 0:
+         break;
+      default:
+         vpanic("getRegUsage_RISCV64Instr:Call:fregparms");
+      }
+      /* Finally, add the condition register. */
+      if (!hregIsInvalid(i->RISCV64in.Call.cond))
+         addHRegUse(u, HRmRead, i->RISCV64in.Call.cond);
+      return;
+   /* XDirect/XIndir/XAssisted are also a bit subtle. They conditionally exit
+      the block. Hence we only need to list (1) the registers that they read,
+      and (2) the registers that they write in the case where the block is not
+      exited. (2) is empty, hence only (1) is relevant here. */
+   case RISCV64in_XDirect:
+      addHRegUse(u, HRmRead, i->RISCV64in.XDirect.base);
+      if (!hregIsInvalid(i->RISCV64in.XDirect.cond))
+         addHRegUse(u, HRmRead, i->RISCV64in.XDirect.cond);
+      return;
+   case RISCV64in_XIndir:
+      addHRegUse(u, HRmRead, i->RISCV64in.XIndir.dstGA);
+      addHRegUse(u, HRmRead, i->RISCV64in.XIndir.base);
+      if (!hregIsInvalid(i->RISCV64in.XIndir.cond))
+         addHRegUse(u, HRmRead, i->RISCV64in.XIndir.cond);
+      return;
+   case RISCV64in_XAssisted:
+      addHRegUse(u, HRmRead, i->RISCV64in.XAssisted.dstGA);
+      addHRegUse(u, HRmRead, i->RISCV64in.XAssisted.base);
+      if (!hregIsInvalid(i->RISCV64in.XAssisted.cond))
+         addHRegUse(u, HRmRead, i->RISCV64in.XAssisted.cond);
+      return;
+   case RISCV64in_EvCheck:
+      addHRegUse(u, HRmRead, i->RISCV64in.EvCheck.base_amCounter);
+      addHRegUse(u, HRmRead, i->RISCV64in.EvCheck.base_amFailAddr);
+      return;
+   default:
+      ppRISCV64Instr(i, mode64);
+      vpanic("getRegUsage_RISCV64Instr");
+   }
+}
+
+/* Local helper. */
+static void mapReg(HRegRemap* m, HReg* r) { *r = lookupHRegRemap(m, *r); }
+
+/* Map the registers of the given instruction. */
+void mapRegs_RISCV64Instr(HRegRemap* m, RISCV64Instr* i, Bool mode64)
+{
+   vassert(mode64 == True);
+
+   switch (i->tag) {
+   case RISCV64in_LI:
+      mapReg(m, &i->RISCV64in.LI.dst);
+      return;
+   case RISCV64in_MV:
+      mapReg(m, &i->RISCV64in.MV.dst);
+      mapReg(m, &i->RISCV64in.MV.src);
+      return;
+   case RISCV64in_ADD:
+      mapReg(m, &i->RISCV64in.ADD.dst);
+      mapReg(m, &i->RISCV64in.ADD.src1);
+      mapReg(m, &i->RISCV64in.ADD.src2);
+      return;
+   case RISCV64in_ADDI:
+      mapReg(m, &i->RISCV64in.ADDI.dst);
+      mapReg(m, &i->RISCV64in.ADDI.src);
+      return;
+   case RISCV64in_ADDW:
+      mapReg(m, &i->RISCV64in.ADDW.dst);
+      mapReg(m, &i->RISCV64in.ADDW.src1);
+      mapReg(m, &i->RISCV64in.ADDW.src2);
+      return;
+   case RISCV64in_ADDIW:
+      mapReg(m, &i->RISCV64in.ADDIW.dst);
+      mapReg(m, &i->RISCV64in.ADDIW.src);
+      return;
+   case RISCV64in_SUB:
+      mapReg(m, &i->RISCV64in.SUB.dst);
+      mapReg(m, &i->RISCV64in.SUB.src1);
+      mapReg(m, &i->RISCV64in.SUB.src2);
+      return;
+   case RISCV64in_SUBW:
+      mapReg(m, &i->RISCV64in.SUBW.dst);
+      mapReg(m, &i->RISCV64in.SUBW.src1);
+      mapReg(m, &i->RISCV64in.SUBW.src2);
+      return;
+   case RISCV64in_XOR:
+      mapReg(m, &i->RISCV64in.XOR.dst);
+      mapReg(m, &i->RISCV64in.XOR.src1);
+      mapReg(m, &i->RISCV64in.XOR.src2);
+      return;
+   case RISCV64in_XORI:
+      mapReg(m, &i->RISCV64in.XORI.dst);
+      mapReg(m, &i->RISCV64in.XORI.src);
+      return;
+   case RISCV64in_OR:
+      mapReg(m, &i->RISCV64in.OR.dst);
+      mapReg(m, &i->RISCV64in.OR.src1);
+      mapReg(m, &i->RISCV64in.OR.src2);
+      return;
+   case RISCV64in_AND:
+      mapReg(m, &i->RISCV64in.AND.dst);
+      mapReg(m, &i->RISCV64in.AND.src1);
+      mapReg(m, &i->RISCV64in.AND.src2);
+      return;
+   case RISCV64in_ANDI:
+      mapReg(m, &i->RISCV64in.ANDI.dst);
+      mapReg(m, &i->RISCV64in.ANDI.src);
+      return;
+   case RISCV64in_SLL:
+      mapReg(m, &i->RISCV64in.SLL.dst);
+      mapReg(m, &i->RISCV64in.SLL.src1);
+      mapReg(m, &i->RISCV64in.SLL.src2);
+      return;
+   case RISCV64in_SRL:
+      mapReg(m, &i->RISCV64in.SRL.dst);
+      mapReg(m, &i->RISCV64in.SRL.src1);
+      mapReg(m, &i->RISCV64in.SRL.src2);
+      return;
+   case RISCV64in_SRA:
+      mapReg(m, &i->RISCV64in.SRA.dst);
+      mapReg(m, &i->RISCV64in.SRA.src1);
+      mapReg(m, &i->RISCV64in.SRA.src2);
+      return;
+   case RISCV64in_SLLI:
+      mapReg(m, &i->RISCV64in.SLLI.dst);
+      mapReg(m, &i->RISCV64in.SLLI.src);
+      return;
+   case RISCV64in_SRLI:
+      mapReg(m, &i->RISCV64in.SRLI.dst);
+      mapReg(m, &i->RISCV64in.SRLI.src);
+      return;
+   case RISCV64in_SRAI:
+      mapReg(m, &i->RISCV64in.SRAI.dst);
+      mapReg(m, &i->RISCV64in.SRAI.src);
+      return;
+   case RISCV64in_SLLW:
+      mapReg(m, &i->RISCV64in.SLLW.dst);
+      mapReg(m, &i->RISCV64in.SLLW.src1);
+      mapReg(m, &i->RISCV64in.SLLW.src2);
+      return;
+   case RISCV64in_SRLW:
+      mapReg(m, &i->RISCV64in.SRLW.dst);
+      mapReg(m, &i->RISCV64in.SRLW.src1);
+      mapReg(m, &i->RISCV64in.SRLW.src2);
+      return;
+   case RISCV64in_SRAW:
+      mapReg(m, &i->RISCV64in.SRAW.dst);
+      mapReg(m, &i->RISCV64in.SRAW.src1);
+      mapReg(m, &i->RISCV64in.SRAW.src2);
+      return;
+   case RISCV64in_SLT:
+      mapReg(m, &i->RISCV64in.SLT.dst);
+      mapReg(m, &i->RISCV64in.SLT.src1);
+      mapReg(m, &i->RISCV64in.SLT.src2);
+      return;
+   case RISCV64in_SLTU:
+      mapReg(m, &i->RISCV64in.SLTU.dst);
+      mapReg(m, &i->RISCV64in.SLTU.src1);
+      mapReg(m, &i->RISCV64in.SLTU.src2);
+      return;
+   case RISCV64in_SLTIU:
+      mapReg(m, &i->RISCV64in.SLTIU.dst);
+      mapReg(m, &i->RISCV64in.SLTIU.src);
+      return;
+   case RISCV64in_CSRRW:
+      mapReg(m, &i->RISCV64in.CSRRW.dst);
+      mapReg(m, &i->RISCV64in.CSRRW.src);
+      return;
+   case RISCV64in_MUL:
+      mapReg(m, &i->RISCV64in.MUL.dst);
+      mapReg(m, &i->RISCV64in.MUL.src1);
+      mapReg(m, &i->RISCV64in.MUL.src2);
+      return;
+   case RISCV64in_MULH:
+      mapReg(m, &i->RISCV64in.MULH.dst);
+      mapReg(m, &i->RISCV64in.MULH.src1);
+      mapReg(m, &i->RISCV64in.MULH.src2);
+      return;
+   case RISCV64in_MULHU:
+      mapReg(m, &i->RISCV64in.MULHU.dst);
+      mapReg(m, &i->RISCV64in.MULHU.src1);
+      mapReg(m, &i->RISCV64in.MULHU.src2);
+      return;
+   case RISCV64in_DIV:
+      mapReg(m, &i->RISCV64in.DIV.dst);
+      mapReg(m, &i->RISCV64in.DIV.src1);
+      mapReg(m, &i->RISCV64in.DIV.src2);
+      return;
+   case RISCV64in_DIVU:
+      mapReg(m, &i->RISCV64in.DIVU.dst);
+      mapReg(m, &i->RISCV64in.DIVU.src1);
+      mapReg(m, &i->RISCV64in.DIVU.src2);
+      return;
+   case RISCV64in_REM:
+      mapReg(m, &i->RISCV64in.REM.dst);
+      mapReg(m, &i->RISCV64in.REM.src1);
+      mapReg(m, &i->RISCV64in.REM.src2);
+      return;
+   case RISCV64in_REMU:
+      mapReg(m, &i->RISCV64in.REMU.dst);
+      mapReg(m, &i->RISCV64in.REMU.src1);
+      mapReg(m, &i->RISCV64in.REMU.src2);
+      return;
+   case RISCV64in_MULW:
+      mapReg(m, &i->RISCV64in.MULW.dst);
+      mapReg(m, &i->RISCV64in.MULW.src1);
+      mapReg(m, &i->RISCV64in.MULW.src2);
+      return;
+   case RISCV64in_DIVW:
+      mapReg(m, &i->RISCV64in.DIVW.dst);
+      mapReg(m, &i->RISCV64in.DIVW.src1);
+      mapReg(m, &i->RISCV64in.DIVW.src2);
+      return;
+   case RISCV64in_DIVUW:
+      mapReg(m, &i->RISCV64in.DIVUW.dst);
+      mapReg(m, &i->RISCV64in.DIVUW.src1);
+      mapReg(m, &i->RISCV64in.DIVUW.src2);
+      return;
+   case RISCV64in_REMW:
+      mapReg(m, &i->RISCV64in.REMW.dst);
+      mapReg(m, &i->RISCV64in.REMW.src1);
+      mapReg(m, &i->RISCV64in.REMW.src2);
+      return;
+   case RISCV64in_REMUW:
+      mapReg(m, &i->RISCV64in.REMUW.dst);
+      mapReg(m, &i->RISCV64in.REMUW.src1);
+      mapReg(m, &i->RISCV64in.REMUW.src2);
+      return;
+   case RISCV64in_LD:
+      mapReg(m, &i->RISCV64in.LD.dst);
+      mapReg(m, &i->RISCV64in.LD.base);
+      return;
+   case RISCV64in_LW:
+      mapReg(m, &i->RISCV64in.LW.dst);
+      mapReg(m, &i->RISCV64in.LW.base);
+      return;
+   case RISCV64in_LH:
+      mapReg(m, &i->RISCV64in.LH.dst);
+      mapReg(m, &i->RISCV64in.LH.base);
+      return;
+   case RISCV64in_LB:
+      mapReg(m, &i->RISCV64in.LB.dst);
+      mapReg(m, &i->RISCV64in.LB.base);
+      return;
+   case RISCV64in_SD:
+      mapReg(m, &i->RISCV64in.SD.src);
+      mapReg(m, &i->RISCV64in.SD.base);
+      return;
+   case RISCV64in_SW:
+      mapReg(m, &i->RISCV64in.SW.src);
+      mapReg(m, &i->RISCV64in.SW.base);
+      return;
+   case RISCV64in_SH:
+      mapReg(m, &i->RISCV64in.SH.src);
+      mapReg(m, &i->RISCV64in.SH.base);
+      return;
+   case RISCV64in_SB:
+      mapReg(m, &i->RISCV64in.SB.src);
+      mapReg(m, &i->RISCV64in.SB.base);
+      return;
+   case RISCV64in_LR_W:
+      mapReg(m, &i->RISCV64in.LR_W.dst);
+      mapReg(m, &i->RISCV64in.LR_W.addr);
+      return;
+   case RISCV64in_SC_W:
+      mapReg(m, &i->RISCV64in.SC_W.res);
+      mapReg(m, &i->RISCV64in.SC_W.src);
+      mapReg(m, &i->RISCV64in.SC_W.addr);
+      return;
+   case RISCV64in_FMADD_S:
+      mapReg(m, &i->RISCV64in.FMADD_S.dst);
+      mapReg(m, &i->RISCV64in.FMADD_S.src1);
+      mapReg(m, &i->RISCV64in.FMADD_S.src2);
+      mapReg(m, &i->RISCV64in.FMADD_S.src3);
+      return;
+   case RISCV64in_FADD_S:
+      mapReg(m, &i->RISCV64in.FADD_S.dst);
+      mapReg(m, &i->RISCV64in.FADD_S.src1);
+      mapReg(m, &i->RISCV64in.FADD_S.src2);
+      return;
+   case RISCV64in_FMUL_S:
+      mapReg(m, &i->RISCV64in.FMUL_S.dst);
+      mapReg(m, &i->RISCV64in.FMUL_S.src1);
+      mapReg(m, &i->RISCV64in.FMUL_S.src2);
+      return;
+   case RISCV64in_FDIV_S:
+      mapReg(m, &i->RISCV64in.FDIV_S.dst);
+      mapReg(m, &i->RISCV64in.FDIV_S.src1);
+      mapReg(m, &i->RISCV64in.FDIV_S.src2);
+      return;
+   case RISCV64in_FSQRT_S:
+      mapReg(m, &i->RISCV64in.FSQRT_S.dst);
+      mapReg(m, &i->RISCV64in.FSQRT_S.src1);
+      return;
+   case RISCV64in_FSGNJN_S:
+      mapReg(m, &i->RISCV64in.FSGNJN_S.dst);
+      mapReg(m, &i->RISCV64in.FSGNJN_S.src1);
+      mapReg(m, &i->RISCV64in.FSGNJN_S.src2);
+      return;
+   case RISCV64in_FSGNJX_S:
+      mapReg(m, &i->RISCV64in.FSGNJX_S.dst);
+      mapReg(m, &i->RISCV64in.FSGNJX_S.src1);
+      mapReg(m, &i->RISCV64in.FSGNJX_S.src2);
+      return;
+   case RISCV64in_FMIN_S:
+      mapReg(m, &i->RISCV64in.FMIN_S.dst);
+      mapReg(m, &i->RISCV64in.FMIN_S.src1);
+      mapReg(m, &i->RISCV64in.FMIN_S.src2);
+      return;
+   case RISCV64in_FMAX_S:
+      mapReg(m, &i->RISCV64in.FMAX_S.dst);
+      mapReg(m, &i->RISCV64in.FMAX_S.src1);
+      mapReg(m, &i->RISCV64in.FMAX_S.src2);
+      return;
+   case RISCV64in_FMV_X_W:
+      mapReg(m, &i->RISCV64in.FMV_X_W.dst);
+      mapReg(m, &i->RISCV64in.FMV_X_W.src);
+      return;
+   case RISCV64in_FMV_W_X:
+      mapReg(m, &i->RISCV64in.FMV_W_X.dst);
+      mapReg(m, &i->RISCV64in.FMV_W_X.src);
+      return;
+   case RISCV64in_FMV_D:
+      mapReg(m, &i->RISCV64in.FMV_D.dst);
+      mapReg(m, &i->RISCV64in.FMV_D.src);
+      return;
+   case RISCV64in_FMADD_D:
+      mapReg(m, &i->RISCV64in.FMADD_D.dst);
+      mapReg(m, &i->RISCV64in.FMADD_D.src1);
+      mapReg(m, &i->RISCV64in.FMADD_D.src2);
+      mapReg(m, &i->RISCV64in.FMADD_D.src3);
+      return;
+   case RISCV64in_FADD_D:
+      mapReg(m, &i->RISCV64in.FADD_D.dst);
+      mapReg(m, &i->RISCV64in.FADD_D.src1);
+      mapReg(m, &i->RISCV64in.FADD_D.src2);
+      return;
+   case RISCV64in_FSUB_D:
+      mapReg(m, &i->RISCV64in.FSUB_D.dst);
+      mapReg(m, &i->RISCV64in.FSUB_D.src1);
+      mapReg(m, &i->RISCV64in.FSUB_D.src2);
+      return;
+   case RISCV64in_FMUL_D:
+      mapReg(m, &i->RISCV64in.FMUL_D.dst);
+      mapReg(m, &i->RISCV64in.FMUL_D.src1);
+      mapReg(m, &i->RISCV64in.FMUL_D.src2);
+      return;
+   case RISCV64in_FDIV_D:
+      mapReg(m, &i->RISCV64in.FDIV_D.dst);
+      mapReg(m, &i->RISCV64in.FDIV_D.src1);
+      mapReg(m, &i->RISCV64in.FDIV_D.src2);
+      return;
+   case RISCV64in_FSQRT_D:
+      mapReg(m, &i->RISCV64in.FSQRT_D.dst);
+      mapReg(m, &i->RISCV64in.FSQRT_D.src1);
+      return;
+   case RISCV64in_FSGNJN_D:
+      mapReg(m, &i->RISCV64in.FSGNJN_D.dst);
+      mapReg(m, &i->RISCV64in.FSGNJN_D.src1);
+      mapReg(m, &i->RISCV64in.FSGNJN_D.src2);
+      return;
+   case RISCV64in_FSGNJX_D:
+      mapReg(m, &i->RISCV64in.FSGNJX_D.dst);
+      mapReg(m, &i->RISCV64in.FSGNJX_D.src1);
+      mapReg(m, &i->RISCV64in.FSGNJX_D.src2);
+      return;
+   case RISCV64in_FMIN_D:
+      mapReg(m, &i->RISCV64in.FMIN_D.dst);
+      mapReg(m, &i->RISCV64in.FMIN_D.src1);
+      mapReg(m, &i->RISCV64in.FMIN_D.src2);
+      return;
+   case RISCV64in_FMAX_D:
+      mapReg(m, &i->RISCV64in.FMAX_D.dst);
+      mapReg(m, &i->RISCV64in.FMAX_D.src1);
+      mapReg(m, &i->RISCV64in.FMAX_D.src2);
+      return;
+   case RISCV64in_FEQ_D:
+      mapReg(m, &i->RISCV64in.FEQ_D.dst);
+      mapReg(m, &i->RISCV64in.FEQ_D.src1);
+      mapReg(m, &i->RISCV64in.FEQ_D.src2);
+      return;
+   case RISCV64in_FLT_D:
+      mapReg(m, &i->RISCV64in.FLT_D.dst);
+      mapReg(m, &i->RISCV64in.FLT_D.src1);
+      mapReg(m, &i->RISCV64in.FLT_D.src2);
+      return;
+   case RISCV64in_FCVT_S_D:
+      mapReg(m, &i->RISCV64in.FCVT_S_D.dst);
+      mapReg(m, &i->RISCV64in.FCVT_S_D.src);
+      return;
+   case RISCV64in_FCVT_D_S:
+      mapReg(m, &i->RISCV64in.FCVT_D_S.dst);
+      mapReg(m, &i->RISCV64in.FCVT_D_S.src);
+      return;
+   case RISCV64in_FCVT_W_D:
+      mapReg(m, &i->RISCV64in.FCVT_W_D.dst);
+      mapReg(m, &i->RISCV64in.FCVT_W_D.src);
+      return;
+   case RISCV64in_FCVT_WU_D:
+      mapReg(m, &i->RISCV64in.FCVT_WU_D.dst);
+      mapReg(m, &i->RISCV64in.FCVT_WU_D.src);
+      return;
+   case RISCV64in_FCVT_D_W:
+      mapReg(m, &i->RISCV64in.FCVT_D_W.dst);
+      mapReg(m, &i->RISCV64in.FCVT_D_W.src);
+      return;
+   case RISCV64in_FCVT_D_WU:
+      mapReg(m, &i->RISCV64in.FCVT_D_WU.dst);
+      mapReg(m, &i->RISCV64in.FCVT_D_WU.src);
+      return;
+   case RISCV64in_FCVT_L_D:
+      mapReg(m, &i->RISCV64in.FCVT_L_D.dst);
+      mapReg(m, &i->RISCV64in.FCVT_L_D.src);
+      return;
+   case RISCV64in_FCVT_LU_D:
+      mapReg(m, &i->RISCV64in.FCVT_LU_D.dst);
+      mapReg(m, &i->RISCV64in.FCVT_LU_D.src);
+      return;
+   case RISCV64in_FCVT_D_L:
+      mapReg(m, &i->RISCV64in.FCVT_D_L.dst);
+      mapReg(m, &i->RISCV64in.FCVT_D_L.src);
+      return;
+   case RISCV64in_FCVT_D_LU:
+      mapReg(m, &i->RISCV64in.FCVT_D_LU.dst);
+      mapReg(m, &i->RISCV64in.FCVT_D_LU.src);
+      return;
+   case RISCV64in_FMV_X_D:
+      mapReg(m, &i->RISCV64in.FMV_X_D.dst);
+      mapReg(m, &i->RISCV64in.FMV_X_D.src);
+      return;
+   case RISCV64in_FMV_D_X:
+      mapReg(m, &i->RISCV64in.FMV_D_X.dst);
+      mapReg(m, &i->RISCV64in.FMV_D_X.src);
+      return;
+   case RISCV64in_FLD:
+      mapReg(m, &i->RISCV64in.FLD.dst);
+      mapReg(m, &i->RISCV64in.FLD.base);
+      return;
+   case RISCV64in_FLW:
+      mapReg(m, &i->RISCV64in.FLW.dst);
+      mapReg(m, &i->RISCV64in.FLW.base);
+      return;
+   case RISCV64in_FSD:
+      mapReg(m, &i->RISCV64in.FSD.src);
+      mapReg(m, &i->RISCV64in.FSD.base);
+      return;
+   case RISCV64in_FSW:
+      mapReg(m, &i->RISCV64in.FSW.src);
+      mapReg(m, &i->RISCV64in.FSW.base);
+      return;
+   case RISCV64in_CAS_W:
+      mapReg(m, &i->RISCV64in.CAS_W.old);
+      mapReg(m, &i->RISCV64in.CAS_W.addr);
+      mapReg(m, &i->RISCV64in.CAS_W.expd);
+      mapReg(m, &i->RISCV64in.CAS_W.data);
+      return;
+   case RISCV64in_CAS_D:
+      mapReg(m, &i->RISCV64in.CAS_D.old);
+      mapReg(m, &i->RISCV64in.CAS_D.addr);
+      mapReg(m, &i->RISCV64in.CAS_D.expd);
+      mapReg(m, &i->RISCV64in.CAS_D.data);
+      return;
+   case RISCV64in_FENCE:
+      return;
+   case RISCV64in_CSEL:
+      mapReg(m, &i->RISCV64in.CSEL.dst);
+      mapReg(m, &i->RISCV64in.CSEL.iftrue);
+      mapReg(m, &i->RISCV64in.CSEL.iffalse);
+      mapReg(m, &i->RISCV64in.CSEL.cond);
+      return;
+   case RISCV64in_Call:
+      if (!hregIsInvalid(i->RISCV64in.Call.cond))
+         mapReg(m, &i->RISCV64in.Call.cond);
+      return;
+   case RISCV64in_XDirect:
+      mapReg(m, &i->RISCV64in.XDirect.base);
+      if (!hregIsInvalid(i->RISCV64in.XDirect.cond))
+         mapReg(m, &i->RISCV64in.XDirect.cond);
+      return;
+   case RISCV64in_XIndir:
+      mapReg(m, &i->RISCV64in.XIndir.dstGA);
+      mapReg(m, &i->RISCV64in.XIndir.base);
+      if (!hregIsInvalid(i->RISCV64in.XIndir.cond))
+         mapReg(m, &i->RISCV64in.XIndir.cond);
+      return;
+   case RISCV64in_XAssisted:
+      mapReg(m, &i->RISCV64in.XAssisted.dstGA);
+      mapReg(m, &i->RISCV64in.XAssisted.base);
+      if (!hregIsInvalid(i->RISCV64in.XAssisted.cond))
+         mapReg(m, &i->RISCV64in.XAssisted.cond);
+      return;
+   case RISCV64in_EvCheck:
+      mapReg(m, &i->RISCV64in.EvCheck.base_amCounter);
+      mapReg(m, &i->RISCV64in.EvCheck.base_amFailAddr);
+      return;
+   default:
+      ppRISCV64Instr(i, mode64);
+      vpanic("mapRegs_RISCV64Instr");
+   }
+}
+
+/* Generate riscv64 spill/reload instructions under the direction of the
+   register allocator. Note it's critical these don't write the condition
+   codes. */
+void genSpill_RISCV64(/*OUT*/ HInstr** i1,
+                      /*OUT*/ HInstr** i2,
+                      HReg             rreg,
+                      Int              offsetB,
+                      Bool             mode64)
+{
+   vassert(offsetB >= 0);
+   vassert(!hregIsVirtual(rreg));
+   vassert(mode64 == True);
+
+   HReg base   = get_baseblock_register();
+   Int  soff12 = offsetB - BASEBLOCK_OFFSET_ADJUSTMENT;
+   vassert(soff12 >= -2048 && soff12 < 2048);
+
+   HRegClass rclass = hregClass(rreg);
+   switch (rclass) {
+   case HRcInt64:
+      *i1 = RISCV64Instr_SD(rreg, base, soff12);
+      return;
+   case HRcFlt64:
+      *i1 = RISCV64Instr_FSD(rreg, base, soff12);
+      return;
+   default:
+      ppHRegClass(rclass);
+      vpanic("genSpill_RISCV64: unimplemented regclass");
+   }
+}
+
+void genReload_RISCV64(/*OUT*/ HInstr** i1,
+                       /*OUT*/ HInstr** i2,
+                       HReg             rreg,
+                       Int              offsetB,
+                       Bool             mode64)
+{
+   vassert(offsetB >= 0);
+   vassert(!hregIsVirtual(rreg));
+   vassert(mode64 == True);
+
+   HReg base   = get_baseblock_register();
+   Int  soff12 = offsetB - BASEBLOCK_OFFSET_ADJUSTMENT;
+   vassert(soff12 >= -2048 && soff12 < 2048);
+
+   HRegClass rclass = hregClass(rreg);
+   switch (rclass) {
+   case HRcInt64:
+      *i1 = RISCV64Instr_LD(rreg, base, soff12);
+      return;
+   case HRcFlt64:
+      *i1 = RISCV64Instr_FLD(rreg, base, soff12);
+      return;
+   default:
+      ppHRegClass(rclass);
+      vpanic("genReload_RISCV64: unimplemented regclass");
+   }
+}
+
+RISCV64Instr* genMove_RISCV64(HReg from, HReg to, Bool mode64)
+{
+   vassert(mode64 == True);
+
+   HRegClass rclass = hregClass(from);
+   switch (rclass) {
+   case HRcInt64:
+      return RISCV64Instr_MV(to, from);
+   case HRcFlt64:
+      return RISCV64Instr_FMV_D(to, from);
+   default:
+      ppHRegClass(rclass);
+      vpanic("genMove_RISCV64: unimplemented regclass");
+   }
+}
+
+/*------------------------------------------------------------*/
+/*--- Functions to emit a sequence of bytes                ---*/
+/*------------------------------------------------------------*/
+
+static inline UChar* emit16(UChar* p, UShort val)
+{
+   *p++ = (val >> 0) & 0xff;
+   *p++ = (val >> 8) & 0xff;
+   return p;
+}
+
+static inline UChar* emit32(UChar* p, UInt val)
+{
+   *p++ = (val >> 0) & 0xff;
+   *p++ = (val >> 8) & 0xff;
+   *p++ = (val >> 16) & 0xff;
+   *p++ = (val >> 24) & 0xff;
+   return p;
+}
+
+/*------------------------------------------------------------*/
+/*--- Functions to emit various instruction formats        ---*/
+/*------------------------------------------------------------*/
+
+/* Emit an R-type instruction. */
+static UChar* emit_R(
+   UChar* p, UInt opcode, UInt rd, UInt funct3, UInt rs1, UInt rs2, UInt funct7)
+{
+   vassert(opcode >> 7 == 0);
+   vassert(rd >> 5 == 0);
+   vassert(funct3 >> 3 == 0);
+   vassert(rs1 >> 5 == 0);
+   vassert(rs2 >> 5 == 0);
+   vassert(funct7 >> 7 == 0);
+
+   UInt the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= rd << 7;
+   the_insn |= funct3 << 12;
+   the_insn |= rs1 << 15;
+   the_insn |= rs2 << 20;
+   the_insn |= funct7 << 25;
+
+   return emit32(p, the_insn);
+}
+
+/* Emit an I-type instruction. */
+static UChar*
+emit_I(UChar* p, UInt opcode, UInt rd, UInt funct3, UInt rs1, UInt imm11_0)
+{
+   vassert(opcode >> 7 == 0);
+   vassert(rd >> 5 == 0);
+   vassert(funct3 >> 3 == 0);
+   vassert(rs1 >> 5 == 0);
+   vassert(imm11_0 >> 12 == 0);
+
+   UInt the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= rd << 7;
+   the_insn |= funct3 << 12;
+   the_insn |= rs1 << 15;
+   the_insn |= imm11_0 << 20;
+
+   return emit32(p, the_insn);
+}
+
+/* Emit an S-type instruction. */
+static UChar*
+emit_S(UChar* p, UInt opcode, UInt imm11_0, UInt funct3, UInt rs1, UInt rs2)
+{
+   vassert(opcode >> 7 == 0);
+   vassert(imm11_0 >> 12 == 0);
+   vassert(funct3 >> 3 == 0);
+   vassert(rs1 >> 5 == 0);
+   vassert(rs2 >> 5 == 0);
+
+   UInt imm4_0  = (imm11_0 >> 0) & 0x1f;
+   UInt imm11_5 = (imm11_0 >> 5) & 0x7f;
+
+   UInt the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= imm4_0 << 7;
+   the_insn |= funct3 << 12;
+   the_insn |= rs1 << 15;
+   the_insn |= rs2 << 20;
+   the_insn |= imm11_5 << 25;
+
+   return emit32(p, the_insn);
+}
+
+/* Emit a B-type instruction. */
+static UChar*
+emit_B(UChar* p, UInt opcode, UInt imm12_1, UInt funct3, UInt rs1, UInt rs2)
+{
+   vassert(opcode >> 7 == 0);
+   vassert(imm12_1 >> 12 == 0);
+   vassert(funct3 >> 3 == 0);
+   vassert(rs1 >> 5 == 0);
+   vassert(rs2 >> 5 == 0);
+
+   UInt imm11_11 = (imm12_1 >> 10) & 0x1;
+   UInt imm4_1   = (imm12_1 >> 0) & 0xf;
+   UInt imm10_5  = (imm12_1 >> 4) & 0x3f;
+   UInt imm12_12 = (imm12_1 >> 11) & 0x1;
+
+   UInt the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= imm11_11 << 7;
+   the_insn |= imm4_1 << 8;
+   the_insn |= funct3 << 12;
+   the_insn |= rs1 << 15;
+   the_insn |= rs2 << 20;
+   the_insn |= imm10_5 << 25;
+   the_insn |= imm12_12 << 31;
+
+   return emit32(p, the_insn);
+}
+
+/* Emit a U-type instruction. */
+static UChar* emit_U(UChar* p, UInt opcode, UInt rd, UInt imm31_12)
+{
+   vassert(opcode >> 7 == 0);
+   vassert(rd >> 5 == 0);
+   vassert(imm31_12 >> 20 == 0);
+
+   UInt the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= rd << 7;
+   the_insn |= imm31_12 << 12;
+
+   return emit32(p, the_insn);
+}
+
+/* Emit a CR-type instruction. */
+static UChar* emit_CR(UChar* p, UInt opcode, UInt rs2, UInt rd, UInt funct4)
+{
+   vassert(opcode >> 2 == 0);
+   vassert(rs2 >> 5 == 0);
+   vassert(rd >> 5 == 0);
+   vassert(funct4 >> 4 == 0);
+
+   UShort the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= rs2 << 2;
+   the_insn |= rd << 7;
+   the_insn |= funct4 << 12;
+
+   return emit16(p, the_insn);
+}
+
+/* Emit a CI-type instruction. */
+static UChar* emit_CI(UChar* p, UInt opcode, UInt imm5_0, UInt rd, UInt funct3)
+{
+   vassert(opcode >> 2 == 0);
+   vassert(imm5_0 >> 6 == 0);
+   vassert(rd >> 5 == 0);
+   vassert(funct3 >> 3 == 0);
+
+   UInt imm4_0 = (imm5_0 >> 0) & 0x1f;
+   UInt imm5_5 = (imm5_0 >> 5) & 0x1;
+
+   UShort the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= imm4_0 << 2;
+   the_insn |= rd << 7;
+   the_insn |= imm5_5 << 12;
+   the_insn |= funct3 << 13;
+
+   return emit16(p, the_insn);
+}
+
+/* Emit a CJ-type instruction. */
+static UChar* emit_CJ(UChar* p, UInt opcode, UInt imm11_1, UInt funct3)
+{
+   vassert(opcode >> 2 == 0);
+   vassert(imm11_1 >> 11 == 0);
+   vassert(funct3 >> 3 == 0);
+
+   UInt imm5_5   = (imm11_1 >> 4) & 0x1;
+   UInt imm3_1   = (imm11_1 >> 0) & 0x7;
+   UInt imm7_7   = (imm11_1 >> 6) & 0x1;
+   UInt imm6_6   = (imm11_1 >> 5) & 0x1;
+   UInt imm10_10 = (imm11_1 >> 9) & 0x1;
+   UInt imm9_8   = (imm11_1 >> 7) & 0x3;
+   UInt imm4_4   = (imm11_1 >> 3) & 0x1;
+   UInt imm11_11 = (imm11_1 >> 10) & 0x1;
+
+   UShort the_insn = 0;
+
+   the_insn |= opcode << 0;
+   the_insn |= imm5_5 << 2;
+   the_insn |= imm3_1 << 3;
+   the_insn |= imm7_7 << 6;
+   the_insn |= imm6_6 << 7;
+   the_insn |= imm10_10 << 8;
+   the_insn |= imm9_8 << 9;
+   the_insn |= imm4_4 << 11;
+   the_insn |= imm11_11 << 12;
+   the_insn |= funct3 << 13;
+
+   return emit16(p, the_insn);
+}
+
+/*------------------------------------------------------------*/
+/*--- Code generation                                      ---*/
+/*------------------------------------------------------------*/
+
+/* Get an immediate into a register, using only that register. */
+static UChar* imm64_to_ireg(UChar* p, UInt dst, ULong imm64)
+{
+   vassert(dst > 0 && dst <= 31);
+
+   Long simm64 = imm64;
+
+   if (simm64 >= -32 && simm64 <= 31) {
+      /* c.li dst, simm64[5:0] */
+      return emit_CI(p, 0b01, imm64 & 0x3f, dst, 0b010);
+   }
+
+   /* TODO Add implementation with addi only and c.lui+addi. */
+
+   if (simm64 >= -2147483648 && simm64 <= 2147483647) {
+      /* lui dst, simm64[31:12]+simm64[11] */
+      p = emit_U(p, 0b0110111, dst, ((imm64 + 0x800) >> 12) & 0xfffff);
+      if ((imm64 & 0xfff) == 0)
+         return p;
+      /* addiw dst, dst, simm64[11:0] */
+      return emit_I(p, 0b0011011, dst, 0b000, dst, imm64 & 0xfff);
+   }
+
+   /* Handle a constant that is out of the 32-bit signed integer range. */
+   /* Strip the low 12 bits. */
+   ULong imm11_0 = imm64 & 0xfff;
+
+   /* Get the remaining adjusted upper bits. */
+   ULong rem   = (simm64 + 0x800) >> 12;
+   UInt  sham6 = 12 + __builtin_ctzll(rem);
+   vassert(sham6 < 64);
+   rem = vex_sx_to_64(rem >> (sham6 - 12), 64 - sham6);
+
+   /* Generate instructions to load the upper bits. */
+   p = imm64_to_ireg(p, dst, rem);
+   /* c.slli dst, sham6 */
+   p = emit_CI(p, 0b10, sham6, dst, 0b000);
+
+   /* Add the low bits in. */
+   if (imm11_0 == 0)
+      return p;
+   UInt imm5_0 = imm11_0 & 0x3f;
+   if (vex_sx_to_64(imm5_0, 6) == vex_sx_to_64(imm11_0, 12)) {
+      /* c.addi dst, imm5_0 */
+      p = emit_CI(p, 0b01, imm5_0, dst, 0b000);
+   } else {
+      /* addi dst, dst, imm11_0 */
+      p = emit_I(p, 0b0010011, dst, 0b000, dst, imm11_0);
+   }
+
+   return p;
+}
+
+/* Get a 48-bit address into a register, using only that register, and
+   generating a constant number of instructions with 18 bytes in size,
+   regardless of the value of the address. This is used when generating
+   sections of code that need to be patched later, so as to guarantee a
+   specific size.
+
+   Notice that this function is designed to support target systems that use the
+   Sv39 or Sv48 virtual-memory system. The input address is checked to be in
+   the Sv48 format, that is bits [63:48] must be all equal to bit 47.
+   Utilizing the fact that the address is only 48-bits in size allows to save 2
+   instructions compared to materializing a full 64-bit address.
+
+   TODO Review if generating instead 'c.ld dst, 1f; c.j 2f; .align 3;
+   1: .quad imm; 2:' is possible and would be better.
+   */
+static UChar* addr48_to_ireg_EXACTLY_18B(UChar* p, UInt dst, ULong imm48)
+{
+   vassert(imm48 >> 47 == 0 || imm48 >> 47 == 0x1ffff);
+
+   ULong rem = imm48;
+   ULong imm47_28, imm27_16, imm15_4, imm3_0;
+   imm3_0   = rem & 0xf;
+   rem      = (rem + 0x8) >> 4;
+   imm15_4  = rem & 0xfff;
+   rem      = (rem + 0x800) >> 12;
+   imm27_16 = rem & 0xfff;
+   rem      = (rem + 0x800) >> 12;
+   imm47_28 = rem & 0xfffff;
+
+   /* lui dst, imm47_28 */
+   p = emit_U(p, 0b0110111, dst, imm47_28);
+   /* addiw dst, dst, imm27_16 */
+   p = emit_I(p, 0b0011011, dst, 0b000, dst, imm27_16);
+   /* c.slli dst, 12 */
+   p = emit_CI(p, 0b10, 12, dst, 0b000);
+   /* addi dst, dst, imm15_4 */
+   p = emit_I(p, 0b0010011, dst, 0b000, dst, imm15_4);
+   /* c.slli dst, 4 */
+   p = emit_CI(p, 0b10, 4, dst, 0b000);
+   if (imm3_0 != 0) {
+      /* c.addi dst, imm3_0 */
+      p = emit_CI(p, 0b01, vex_sx_to_64(imm3_0, 4) & 0x3f, dst, 0b000);
+   } else {
+      /* c.nop */
+      p = emit_CI(p, 0b01, 0, 0, 0b000);
+   }
+
+   return p;
+}
+
+/* Check whether p points at an instruction sequence cooked up by
+   addr48_to_ireg_EXACTLY_18B(). */
+static Bool is_addr48_to_ireg_EXACTLY_18B(UChar* p, UInt dst, ULong imm48)
+{
+   UChar  tmp[18];
+   UChar* q;
+
+   q = addr48_to_ireg_EXACTLY_18B(&tmp[0], dst, imm48);
+   if (q - &tmp[0] != 18)
+      return False;
+
+   q = &tmp[0];
+   for (UInt i = 0; i < 18; i++) {
+      if (*p != *q)
+         return False;
+      p++;
+      q++;
+   }
+   return True;
+}
+
+/* Emit an instruction into buf and return the number of bytes used. Note that
+   buf is not the insn's final place, and therefore it is imperative to emit
+   position-independent code. If the emitted instruction was a profiler inc, set
+   *is_profInc to True, else leave it unchanged. */
+Int emit_RISCV64Instr(/*MB_MOD*/ Bool*    is_profInc,
+                      UChar*              buf,
+                      Int                 nbuf,
+                      const RISCV64Instr* i,
+                      Bool                mode64,
+                      VexEndness          endness_host,
+                      const void*         disp_cp_chain_me_to_slowEP,
+                      const void*         disp_cp_chain_me_to_fastEP,
+                      const void*         disp_cp_xindir,
+                      const void*         disp_cp_xassisted)
+{
+   vassert(nbuf >= 32);
+   vassert(mode64 == True);
+   vassert(((HWord)buf & 1) == 0);
+
+   UChar* p = &buf[0];
+
+   switch (i->tag) {
+   case RISCV64in_LI:
+      p = imm64_to_ireg(p, iregEnc(i->RISCV64in.LI.dst), i->RISCV64in.LI.imm64);
+      goto done;
+   case RISCV64in_MV: {
+      /* c.mv dst, src */
+      UInt dst = iregEnc(i->RISCV64in.MV.dst);
+      UInt src = iregEnc(i->RISCV64in.MV.src);
+
+      p = emit_CR(p, 0b10, src, dst, 0b1000);
+      goto done;
+   }
+   case RISCV64in_ADD: {
+      /* add dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.ADD.dst);
+      UInt src1 = iregEnc(i->RISCV64in.ADD.src1);
+      UInt src2 = iregEnc(i->RISCV64in.ADD.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b000, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_ADDI: {
+      /* addi dst, src, simm12 */
+      UInt dst    = iregEnc(i->RISCV64in.ADDI.dst);
+      UInt src    = iregEnc(i->RISCV64in.ADDI.src);
+      Int  simm12 = i->RISCV64in.ADDI.simm12;
+      vassert(simm12 >= -2048 && simm12 < 2048);
+      UInt imm11_0 = simm12 & 0xfff;
+
+      p = emit_I(p, 0b0010011, dst, 0b000, src, imm11_0);
+      goto done;
+   }
+   case RISCV64in_ADDW: {
+      /* addw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.ADDW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.ADDW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.ADDW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b000, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_ADDIW: {
+      /* addiw dst, src, simm12 */
+      UInt dst    = iregEnc(i->RISCV64in.ADDIW.dst);
+      UInt src    = iregEnc(i->RISCV64in.ADDIW.src);
+      Int  simm12 = i->RISCV64in.ADDIW.simm12;
+      vassert(simm12 >= -2048 && simm12 < 2048);
+      UInt imm11_0 = simm12 & 0xfff;
+
+      p = emit_I(p, 0b0011011, dst, 0b000, src, imm11_0);
+      goto done;
+   }
+   case RISCV64in_SUB: {
+      /* sub dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SUB.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SUB.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SUB.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b000, src1, src2, 0b0100000);
+      goto done;
+   }
+   case RISCV64in_SUBW: {
+      /* subw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SUBW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SUBW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SUBW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b000, src1, src2, 0b0100000);
+      goto done;
+   }
+   case RISCV64in_XOR: {
+      /* xor dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.XOR.dst);
+      UInt src1 = iregEnc(i->RISCV64in.XOR.src1);
+      UInt src2 = iregEnc(i->RISCV64in.XOR.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b100, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_XORI: {
+      /* xori dst, src, simm12 */
+      UInt dst    = iregEnc(i->RISCV64in.XORI.dst);
+      UInt src    = iregEnc(i->RISCV64in.XORI.src);
+      Int  simm12 = i->RISCV64in.XORI.simm12;
+      vassert(simm12 >= -2048 && simm12 < 2048);
+      UInt imm11_0 = simm12 & 0xfff;
+
+      p = emit_I(p, 0b0010011, dst, 0b100, src, imm11_0);
+      goto done;
+   }
+   case RISCV64in_OR: {
+      /* or dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.OR.dst);
+      UInt src1 = iregEnc(i->RISCV64in.OR.src1);
+      UInt src2 = iregEnc(i->RISCV64in.OR.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b110, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_AND: {
+      /* and dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.AND.dst);
+      UInt src1 = iregEnc(i->RISCV64in.AND.src1);
+      UInt src2 = iregEnc(i->RISCV64in.AND.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b111, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_ANDI: {
+      /* andi dst, src, simm12 */
+      UInt dst    = iregEnc(i->RISCV64in.ANDI.dst);
+      UInt src    = iregEnc(i->RISCV64in.ANDI.src);
+      Int  simm12 = i->RISCV64in.ANDI.simm12;
+      vassert(simm12 >= -2048 && simm12 < 2048);
+      UInt imm11_0 = simm12 & 0xfff;
+
+      p = emit_I(p, 0b0010011, dst, 0b111, src, imm11_0);
+      goto done;
+   }
+   case RISCV64in_SLL: {
+      /* sll dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SLL.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SLL.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SLL.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b001, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_SRL: {
+      /* srl dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SRL.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SRL.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SRL.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b101, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_SRA: {
+      /* sra dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SRA.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SRA.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SRA.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b101, src1, src2, 0b0100000);
+      goto done;
+   }
+   case RISCV64in_SLLI: {
+      /* slli dst, src, uimm6 */
+      UInt dst   = iregEnc(i->RISCV64in.SLLI.dst);
+      UInt src   = iregEnc(i->RISCV64in.SLLI.src);
+      UInt uimm6 = i->RISCV64in.SLLI.uimm6;
+      vassert(uimm6 < 64);
+      UInt uimm4_0 = uimm6 & 0x1f;
+      UInt uimm5_5 = (uimm6 >> 5) & 0x1;
+
+      p = emit_R(p, 0b0010011, dst, 0b001, src, uimm4_0,
+                 0b000000 << 1 | uimm5_5);
+      goto done;
+   }
+   case RISCV64in_SRLI: {
+      /* srli dst, src, uimm6 */
+      UInt dst   = iregEnc(i->RISCV64in.SRLI.dst);
+      UInt src   = iregEnc(i->RISCV64in.SRLI.src);
+      UInt uimm6 = i->RISCV64in.SRLI.uimm6;
+      vassert(uimm6 < 64);
+      UInt uimm4_0 = uimm6 & 0x1f;
+      UInt uimm5_5 = (uimm6 >> 5) & 0x1;
+
+      p = emit_R(p, 0b0010011, dst, 0b101, src, uimm4_0,
+                 0b000000 << 1 | uimm5_5);
+      goto done;
+   }
+   case RISCV64in_SRAI: {
+      /* srai dst, src, uimm6 */
+      UInt dst   = iregEnc(i->RISCV64in.SRAI.dst);
+      UInt src   = iregEnc(i->RISCV64in.SRAI.src);
+      UInt uimm6 = i->RISCV64in.SRAI.uimm6;
+      vassert(uimm6 < 64);
+      UInt uimm4_0 = uimm6 & 0x1f;
+      UInt uimm5_5 = (uimm6 >> 5) & 0x1;
+
+      p = emit_R(p, 0b0010011, dst, 0b101, src, uimm4_0,
+                 0b010000 << 1 | uimm5_5);
+      goto done;
+   }
+   case RISCV64in_SLLW: {
+      /* sllw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SLLW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SLLW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SLLW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b001, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_SRLW: {
+      /* srlw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SRLW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SRLW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SRLW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b101, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_SRAW: {
+      /* sraw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SRAW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SRAW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SRAW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b101, src1, src2, 0b0100000);
+      goto done;
+   }
+   case RISCV64in_SLT: {
+      /* slt dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SLT.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SLT.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SLT.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b010, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_SLTU: {
+      /* sltu dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.SLTU.dst);
+      UInt src1 = iregEnc(i->RISCV64in.SLTU.src1);
+      UInt src2 = iregEnc(i->RISCV64in.SLTU.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b011, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_SLTIU: {
+      /* sltiu dst, src, simm12 */
+      UInt dst    = iregEnc(i->RISCV64in.SLTIU.dst);
+      UInt src    = iregEnc(i->RISCV64in.SLTIU.src);
+      Int  simm12 = i->RISCV64in.SLTIU.simm12;
+      vassert(simm12 >= -2048 && simm12 < 2048);
+      UInt imm11_0 = simm12 & 0xfff;
+
+      p = emit_I(p, 0b0010011, dst, 0b011, src, imm11_0);
+      goto done;
+   }
+   case RISCV64in_CSRRW: {
+      /* csrrw dst, csr, src */
+      UInt dst = iregEnc(i->RISCV64in.CSRRW.dst);
+      UInt src = iregEnc(i->RISCV64in.CSRRW.src);
+      UInt csr = i->RISCV64in.CSRRW.csr;
+      vassert(csr < 4096);
+
+      p = emit_I(p, 0b1110011, dst, 0b001, src, csr);
+      goto done;
+   }
+   case RISCV64in_MUL: {
+      /* mul dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.MUL.dst);
+      UInt src1 = iregEnc(i->RISCV64in.MUL.src1);
+      UInt src2 = iregEnc(i->RISCV64in.MUL.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b000, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_MULH: {
+      /* mulh dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.MULH.dst);
+      UInt src1 = iregEnc(i->RISCV64in.MULH.src1);
+      UInt src2 = iregEnc(i->RISCV64in.MULH.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b001, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_MULHU: {
+      /* mulhu dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.MULHU.dst);
+      UInt src1 = iregEnc(i->RISCV64in.MULHU.src1);
+      UInt src2 = iregEnc(i->RISCV64in.MULHU.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b011, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_DIV: {
+      /* div dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.DIV.dst);
+      UInt src1 = iregEnc(i->RISCV64in.DIV.src1);
+      UInt src2 = iregEnc(i->RISCV64in.DIV.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b100, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_DIVU: {
+      /* divu dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.DIVU.dst);
+      UInt src1 = iregEnc(i->RISCV64in.DIVU.src1);
+      UInt src2 = iregEnc(i->RISCV64in.DIVU.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b101, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_REM: {
+      /* rem dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.REM.dst);
+      UInt src1 = iregEnc(i->RISCV64in.REM.src1);
+      UInt src2 = iregEnc(i->RISCV64in.REM.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b110, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_REMU: {
+      /* remu dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.REMU.dst);
+      UInt src1 = iregEnc(i->RISCV64in.REMU.src1);
+      UInt src2 = iregEnc(i->RISCV64in.REMU.src2);
+
+      p = emit_R(p, 0b0110011, dst, 0b111, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_MULW: {
+      /* mulw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.MULW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.MULW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.MULW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b000, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_DIVW: {
+      /* divw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.DIVW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.DIVW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.DIVW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b100, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_DIVUW: {
+      /* divuw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.DIVUW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.DIVUW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.DIVUW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b101, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_REMW: {
+      /* remw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.REMW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.REMW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.REMW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b110, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_REMUW: {
+      /* remuw dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.REMUW.dst);
+      UInt src1 = iregEnc(i->RISCV64in.REMUW.src1);
+      UInt src2 = iregEnc(i->RISCV64in.REMUW.src2);
+
+      p = emit_R(p, 0b0111011, dst, 0b111, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_LD: {
+      /* ld dst, soff12(base) */
+      UInt dst    = iregEnc(i->RISCV64in.LD.dst);
+      UInt base   = iregEnc(i->RISCV64in.LD.base);
+      Int  soff12 = i->RISCV64in.LD.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_I(p, 0b0000011, dst, 0b011, base, imm11_0);
+      goto done;
+   }
+   case RISCV64in_LW: {
+      /* lw dst, soff12(base) */
+      UInt dst    = iregEnc(i->RISCV64in.LW.dst);
+      UInt base   = iregEnc(i->RISCV64in.LW.base);
+      Int  soff12 = i->RISCV64in.LW.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_I(p, 0b0000011, dst, 0b010, base, imm11_0);
+      goto done;
+   }
+   case RISCV64in_LH: {
+      /* lh dst, soff12(base) */
+      UInt dst    = iregEnc(i->RISCV64in.LH.dst);
+      UInt base   = iregEnc(i->RISCV64in.LH.base);
+      Int  soff12 = i->RISCV64in.LH.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_I(p, 0b0000011, dst, 0b001, base, imm11_0);
+      goto done;
+   }
+   case RISCV64in_LB: {
+      /* lb dst, soff12(base) */
+      UInt dst    = iregEnc(i->RISCV64in.LB.dst);
+      UInt base   = iregEnc(i->RISCV64in.LB.base);
+      Int  soff12 = i->RISCV64in.LB.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_I(p, 0b0000011, dst, 0b000, base, imm11_0);
+      goto done;
+   }
+   case RISCV64in_SD: {
+      /* sd src, soff12(base) */
+      UInt src    = iregEnc(i->RISCV64in.SD.src);
+      UInt base   = iregEnc(i->RISCV64in.SD.base);
+      Int  soff12 = i->RISCV64in.SD.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_S(p, 0b0100011, imm11_0, 0b011, base, src);
+      goto done;
+   }
+   case RISCV64in_SW: {
+      /* sw src, soff12(base) */
+      UInt src    = iregEnc(i->RISCV64in.SW.src);
+      UInt base   = iregEnc(i->RISCV64in.SW.base);
+      Int  soff12 = i->RISCV64in.SW.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_S(p, 0b0100011, imm11_0, 0b010, base, src);
+      goto done;
+   }
+   case RISCV64in_SH: {
+      /* sh src, soff12(base) */
+      UInt src    = iregEnc(i->RISCV64in.SH.src);
+      UInt base   = iregEnc(i->RISCV64in.SH.base);
+      Int  soff12 = i->RISCV64in.SH.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_S(p, 0b0100011, imm11_0, 0b001, base, src);
+      goto done;
+   }
+   case RISCV64in_SB: {
+      /* sb src, soff12(base) */
+      UInt src    = iregEnc(i->RISCV64in.SB.src);
+      UInt base   = iregEnc(i->RISCV64in.SB.base);
+      Int  soff12 = i->RISCV64in.SB.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_S(p, 0b0100011, imm11_0, 0b000, base, src);
+      goto done;
+   }
+   case RISCV64in_LR_W: {
+      /* lr.w dst, (addr) */
+      UInt dst  = iregEnc(i->RISCV64in.LR_W.dst);
+      UInt addr = iregEnc(i->RISCV64in.LR_W.addr);
+
+      p = emit_R(p, 0b0101111, dst, 0b010, addr, 0b00000, 0b0001000);
+      goto done;
+   }
+   case RISCV64in_SC_W: {
+      /* sc.w res, dst, (addr) */
+      UInt res  = iregEnc(i->RISCV64in.SC_W.res);
+      UInt src  = iregEnc(i->RISCV64in.SC_W.src);
+      UInt addr = iregEnc(i->RISCV64in.SC_W.addr);
+
+      p = emit_R(p, 0b0101111, res, 0b010, addr, src, 0b0001100);
+      goto done;
+   }
+   case RISCV64in_FMADD_S: {
+      /* fmadd.s dst, src1, src2, src3 */
+      UInt dst  = fregEnc(i->RISCV64in.FMADD_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMADD_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMADD_S.src2);
+      UInt src3 = fregEnc(i->RISCV64in.FMADD_S.src3);
+
+      p = emit_R(p, 0b1000011, dst, 0b111, src1, src2, src3 << 2 | 0b00);
+      goto done;
+   }
+   case RISCV64in_FADD_S: {
+      /* fadd.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FADD_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FADD_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FADD_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0000000);
+      goto done;
+   }
+   case RISCV64in_FMUL_S: {
+      /* fmul.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FMUL_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMUL_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMUL_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0001000);
+      goto done;
+   }
+   case RISCV64in_FDIV_S: {
+      /* fdiv.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FDIV_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FDIV_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FDIV_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0001100);
+      goto done;
+   }
+   case RISCV64in_FSQRT_S: {
+      /* fsqrt.s dst, src1 */
+      UInt dst  = fregEnc(i->RISCV64in.FSQRT_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSQRT_S.src1);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, 0b00000, 0b0101100);
+      goto done;
+   }
+   case RISCV64in_FSGNJN_S: {
+      /* fsgnjn.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FSGNJN_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSGNJN_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FSGNJN_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b001, src1, src2, 0b0010000);
+      goto done;
+   }
+   case RISCV64in_FSGNJX_S: {
+      /* fsgnjx.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FSGNJX_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSGNJX_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FSGNJX_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b010, src1, src2, 0b0010000);
+      goto done;
+   }
+   case RISCV64in_FMIN_S: {
+      /* fmin.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FMIN_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMIN_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMIN_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src1, src2, 0b0010100);
+      goto done;
+   }
+   case RISCV64in_FMAX_S: {
+      /* fmax.s dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FMAX_S.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMAX_S.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMAX_S.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b001, src1, src2, 0b0010100);
+      goto done;
+   }
+   case RISCV64in_FMV_X_W: {
+      /* fmv.x.w dst, src */
+      UInt dst = iregEnc(i->RISCV64in.FMV_X_W.dst);
+      UInt src = fregEnc(i->RISCV64in.FMV_X_W.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, 0b00000, 0b1110000);
+      goto done;
+   }
+   case RISCV64in_FMV_W_X: {
+      /* fmv.w.x dst, src */
+      UInt dst = fregEnc(i->RISCV64in.FMV_W_X.dst);
+      UInt src = iregEnc(i->RISCV64in.FMV_W_X.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, 0b00000, 0b1111000);
+      goto done;
+   }
+   case RISCV64in_FMV_D: {
+      /* fsgnj.d dst, src, src */
+      UInt dst = fregEnc(i->RISCV64in.FMV_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FMV_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, src, 0b0010001);
+      goto done;
+   }
+   case RISCV64in_FMADD_D: {
+      /* fmadd.d dst, src1, src2, src3 */
+      UInt dst  = fregEnc(i->RISCV64in.FMADD_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMADD_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMADD_D.src2);
+      UInt src3 = fregEnc(i->RISCV64in.FMADD_D.src3);
+
+      p = emit_R(p, 0b1000011, dst, 0b111, src1, src2, src3 << 2 | 0b01);
+      goto done;
+   }
+   case RISCV64in_FADD_D: {
+      /* fadd.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FADD_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FADD_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FADD_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0000001);
+      goto done;
+   }
+   case RISCV64in_FSUB_D: {
+      /* fsub.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FSUB_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSUB_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FSUB_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0000101);
+      goto done;
+   }
+   case RISCV64in_FMUL_D: {
+      /* fmul.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FMUL_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMUL_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMUL_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0001001);
+      goto done;
+   }
+   case RISCV64in_FDIV_D: {
+      /* fdiv.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FDIV_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FDIV_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FDIV_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, src2, 0b0001101);
+      goto done;
+   }
+   case RISCV64in_FSQRT_D: {
+      /* fsqrt.d dst, src1 */
+      UInt dst  = fregEnc(i->RISCV64in.FSQRT_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSQRT_D.src1);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src1, 0b00000, 0b0101101);
+      goto done;
+   }
+   case RISCV64in_FSGNJN_D: {
+      /* fsgnjn.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FSGNJN_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSGNJN_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FSGNJN_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b001, src1, src2, 0b0010001);
+      goto done;
+   }
+   case RISCV64in_FSGNJX_D: {
+      /* fsgnjx.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FSGNJX_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FSGNJX_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FSGNJX_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b010, src1, src2, 0b0010001);
+      goto done;
+   }
+   case RISCV64in_FMIN_D: {
+      /* fmin.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FMIN_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMIN_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMIN_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src1, src2, 0b0010101);
+      goto done;
+   }
+   case RISCV64in_FMAX_D: {
+      /* fmax.d dst, src1, src2 */
+      UInt dst  = fregEnc(i->RISCV64in.FMAX_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FMAX_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FMAX_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b001, src1, src2, 0b0010101);
+      goto done;
+   }
+   case RISCV64in_FEQ_D: {
+      /* feq.d dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.FEQ_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FEQ_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FEQ_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b010, src1, src2, 0b1010001);
+      goto done;
+   }
+   case RISCV64in_FLT_D: {
+      /* flt.d dst, src1, src2 */
+      UInt dst  = iregEnc(i->RISCV64in.FLT_D.dst);
+      UInt src1 = fregEnc(i->RISCV64in.FLT_D.src1);
+      UInt src2 = fregEnc(i->RISCV64in.FLT_D.src2);
+
+      p = emit_R(p, 0b1010011, dst, 0b001, src1, src2, 0b1010001);
+      goto done;
+   }
+   case RISCV64in_FCVT_S_D: {
+      /* fcvt.s.d dst, src1 */
+      UInt dst = fregEnc(i->RISCV64in.FCVT_S_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FCVT_S_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00001, 0b0100000);
+      goto done;
+   }
+   case RISCV64in_FCVT_D_S: {
+      /* fcvt.d.s dst, src1 */
+      UInt dst = fregEnc(i->RISCV64in.FCVT_D_S.dst);
+      UInt src = fregEnc(i->RISCV64in.FCVT_D_S.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00000, 0b0100001);
+      goto done;
+   }
+   case RISCV64in_FCVT_W_D: {
+      /* fcvt.w.d dst, src */
+      UInt dst = iregEnc(i->RISCV64in.FCVT_W_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FCVT_W_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00000, 0b1100001);
+      goto done;
+   }
+   case RISCV64in_FCVT_WU_D: {
+      /* fcvt.wu.d dst, src */
+      UInt dst = iregEnc(i->RISCV64in.FCVT_WU_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FCVT_WU_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00001, 0b1100001);
+      goto done;
+   }
+   case RISCV64in_FCVT_D_W: {
+      /* fcvt.d.w dst, src */
+      UInt dst = fregEnc(i->RISCV64in.FCVT_D_W.dst);
+      UInt src = iregEnc(i->RISCV64in.FCVT_D_W.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, 0b00000, 0b1101001);
+      goto done;
+   }
+   case RISCV64in_FCVT_D_WU: {
+      /* fcvt.d.wu dst, src */
+      UInt dst = fregEnc(i->RISCV64in.FCVT_D_WU.dst);
+      UInt src = iregEnc(i->RISCV64in.FCVT_D_WU.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, 0b00001, 0b1101001);
+      goto done;
+   }
+   case RISCV64in_FCVT_L_D: {
+      /* fcvt.l.d dst, src */
+      UInt dst = iregEnc(i->RISCV64in.FCVT_L_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FCVT_L_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00010, 0b1100001);
+      goto done;
+   }
+   case RISCV64in_FCVT_LU_D: {
+      /* fcvt.lu.d dst, src */
+      UInt dst = iregEnc(i->RISCV64in.FCVT_LU_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FCVT_LU_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00011, 0b1100001);
+      goto done;
+   }
+   case RISCV64in_FCVT_D_L: {
+      /* fcvt.d.l dst, src */
+      UInt dst = fregEnc(i->RISCV64in.FCVT_D_L.dst);
+      UInt src = iregEnc(i->RISCV64in.FCVT_D_L.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00010, 0b1101001);
+      goto done;
+   }
+   case RISCV64in_FCVT_D_LU: {
+      /* fcvt.d.lu dst, src */
+      UInt dst = fregEnc(i->RISCV64in.FCVT_D_LU.dst);
+      UInt src = iregEnc(i->RISCV64in.FCVT_D_LU.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b111, src, 0b00011, 0b1101001);
+      goto done;
+   }
+   case RISCV64in_FMV_X_D: {
+      /* fmv.x.d dst, src */
+      UInt dst = iregEnc(i->RISCV64in.FMV_X_D.dst);
+      UInt src = fregEnc(i->RISCV64in.FMV_X_D.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, 0b00000, 0b1110001);
+      goto done;
+   }
+   case RISCV64in_FMV_D_X: {
+      /* fmv.d.x dst, src */
+      UInt dst = fregEnc(i->RISCV64in.FMV_D_X.dst);
+      UInt src = iregEnc(i->RISCV64in.FMV_D_X.src);
+
+      p = emit_R(p, 0b1010011, dst, 0b000, src, 0b00000, 0b1111001);
+      goto done;
+   }
+   case RISCV64in_FLD: {
+      /* fld dst, soff12(base) */
+      UInt dst    = fregEnc(i->RISCV64in.FLD.dst);
+      UInt base   = iregEnc(i->RISCV64in.FLD.base);
+      Int  soff12 = i->RISCV64in.FLD.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_I(p, 0b0000111, dst, 0b011, base, imm11_0);
+      goto done;
+   }
+   case RISCV64in_FLW: {
+      /* flw dst, soff12(base) */
+      UInt dst    = fregEnc(i->RISCV64in.FLW.dst);
+      UInt base   = iregEnc(i->RISCV64in.FLW.base);
+      Int  soff12 = i->RISCV64in.FLW.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_I(p, 0b0000111, dst, 0b010, base, imm11_0);
+      goto done;
+   }
+   case RISCV64in_FSD: {
+      /* fsd src, soff12(base) */
+      UInt src    = fregEnc(i->RISCV64in.FSD.src);
+      UInt base   = iregEnc(i->RISCV64in.FSD.base);
+      Int  soff12 = i->RISCV64in.FSD.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_S(p, 0b0100111, imm11_0, 0b011, base, src);
+      goto done;
+   }
+   case RISCV64in_FSW: {
+      /* fsw src, soff12(base) */
+      UInt src    = fregEnc(i->RISCV64in.FSW.src);
+      UInt base   = iregEnc(i->RISCV64in.FSW.base);
+      Int  soff12 = i->RISCV64in.FSW.soff12;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+      UInt imm11_0 = soff12 & 0xfff;
+
+      p = emit_S(p, 0b0100111, imm11_0, 0b010, base, src);
+      goto done;
+   }
+   case RISCV64in_CAS_W: {
+      /* 1: lr.w old, (addr)
+            bne old, expd, 2f
+            sc.w t0, data, (addr)
+            bne t0, zero, 1b
+         2:
+       */
+      UInt old  = iregEnc(i->RISCV64in.CAS_W.old);
+      UInt addr = iregEnc(i->RISCV64in.CAS_W.addr);
+      UInt expd = iregEnc(i->RISCV64in.CAS_W.expd);
+      UInt data = iregEnc(i->RISCV64in.CAS_W.data);
+
+      p = emit_R(p, 0b0101111, old, 0b010, addr, 0b00000, 0b0001000);
+      p = emit_B(p, 0b1100011, (12 >> 1) & 0xfff, 0b001, old, expd);
+      p = emit_R(p, 0b0101111, 5 /*x5/t0*/, 0b010, addr, data, 0b0001100);
+      p = emit_B(p, 0b1100011, (-12 >> 1) & 0xfff, 0b001, 5 /*x5/t0*/,
+                 0 /*x0/zero*/);
+      goto done;
+   }
+   case RISCV64in_CAS_D: {
+      /* 1: lr.d old, (addr)
+            bne old, expd, 2f
+            sc.d t0, data, (addr)
+            bne t0, zero, 1b
+         2:
+       */
+      UInt old  = iregEnc(i->RISCV64in.CAS_D.old);
+      UInt addr = iregEnc(i->RISCV64in.CAS_D.addr);
+      UInt expd = iregEnc(i->RISCV64in.CAS_D.expd);
+      UInt data = iregEnc(i->RISCV64in.CAS_D.data);
+
+      p = emit_R(p, 0b0101111, old, 0b011, addr, 0b00000, 0b0001000);
+      p = emit_B(p, 0b1100011, (12 >> 1) & 0xfff, 0b001, old, expd);
+      p = emit_R(p, 0b0101111, 5 /*x5/t0*/, 0b011, addr, data, 0b0001100);
+      p = emit_B(p, 0b1100011, (-12 >> 1) & 0xfff, 0b001, 5 /*x5/t0*/,
+                 0 /*x0/zero*/);
+      goto done;
+   }
+   case RISCV64in_FENCE: {
+      /* fence */
+      p = emit_I(p, 0b0001111, 0b00000, 0b000, 0b00000, 0b000011111111);
+      goto done;
+   }
+   case RISCV64in_CSEL: {
+      /*    beq cond, zero, 1f
+            c.mv dst, iftrue
+            c.j 2f
+         1: c.mv dst, iffalse
+         2:
+       */
+      UInt dst     = iregEnc(i->RISCV64in.CSEL.dst);
+      UInt iftrue  = iregEnc(i->RISCV64in.CSEL.iftrue);
+      UInt iffalse = iregEnc(i->RISCV64in.CSEL.iffalse);
+      UInt cond    = iregEnc(i->RISCV64in.CSEL.cond);
+
+      p = emit_B(p, 0b1100011, (8 >> 1) & 0xfff, 0b000, cond, 0 /*x0/zero*/);
+      p = emit_CR(p, 0b10, iftrue, dst, 0b1000);
+      p = emit_CJ(p, 0b01, (4 >> 1) & 0x7ff, 0b101);
+      p = emit_CR(p, 0b10, iffalse, dst, 0b1000);
+      goto done;
+   }
+   case RISCV64in_Call: {
+      /*    beq cond, zero, 1f
+            li t0, target
+            c.jalr 0(t0)
+         1:
+       */
+      UChar* ptmp = NULL;
+      if (!hregIsInvalid(i->RISCV64in.Call.cond)) {
+         ptmp = p;
+         p += 4;
+      }
+
+      /* li t0, target */
+      p = imm64_to_ireg(p, 5 /*x5/t0*/, i->RISCV64in.Call.target);
+
+      /* c.jalr 0(t0) */
+      p = emit_CR(p, 0b10, 0 /*x0/zero*/, 5 /*x5/t0*/, 0b1001);
+
+      /* Fix up the conditional jump, if there was one. */
+      if (!hregIsInvalid(i->RISCV64in.Call.cond)) {
+         /* beq cond, zero, delta */
+         UInt cond  = iregEnc(i->RISCV64in.Call.cond);
+         UInt delta = p - ptmp;
+         /* delta_min = 4 (beq) + 2 (c.li) + 2 (c.jalr) = 8 */
+         vassert(delta >= 8 && delta < 4096 && (delta & 1) == 0);
+         UInt imm12_1 = (delta >> 1) & 0xfff;
+
+         emit_B(ptmp, 0b1100011, imm12_1, 0b000, cond, 0 /*x0/zero*/);
+      }
+
+      goto done;
+   }
+
+   case RISCV64in_XDirect: {
+      /* NB: what goes on here has to be very closely coordinated with the
+         chainXDirect_RISCV64() and unchainXDirect_RISCV64() below. */
+      /* We're generating chain-me requests here, so we need to be sure this is
+         actually allowed -- no-redir translations can't use chain-me's.
+         Hence: */
+      vassert(disp_cp_chain_me_to_slowEP != NULL);
+      vassert(disp_cp_chain_me_to_fastEP != NULL);
+
+      /* First off, if this is conditional, create a conditional jump over the
+         rest of it. Or at least, leave a space for it that we will shortly fill
+         in. */
+      UChar* ptmp = NULL;
+      if (!hregIsInvalid(i->RISCV64in.XDirect.cond)) {
+         ptmp = p;
+         p += 4;
+      }
+
+      /* Update the guest pc. */
+      {
+         /* li t0, dstGA */
+         p = imm64_to_ireg(p, 5 /*x5/t0*/, i->RISCV64in.XDirect.dstGA);
+
+         /* sd t0, soff12(base) */
+         UInt base   = iregEnc(i->RISCV64in.XDirect.base);
+         Int  soff12 = i->RISCV64in.XDirect.soff12;
+         vassert(soff12 >= -2048 && soff12 < 2048);
+         UInt imm11_0 = soff12 & 0xfff;
+
+         p = emit_S(p, 0b0100011, imm11_0, 0b011, base, 5 /*x5/t0*/);
+      }
+
+      /* --- FIRST PATCHABLE BYTE follows --- */
+      /* VG_(disp_cp_chain_me_to_{slowEP,fastEP}) (where we're calling to) backs
+         up the return address, so as to find the address of the first patchable
+         byte. So: don't change the number of instructions (3) below. */
+      /* li t0, VG_(disp_cp_chain_me_to_{slowEP,fastEP}) */
+      const void* disp_cp_chain_me = i->RISCV64in.XDirect.toFastEP
+                                        ? disp_cp_chain_me_to_fastEP
+                                        : disp_cp_chain_me_to_slowEP;
+
+      p = addr48_to_ireg_EXACTLY_18B(p, 5 /*x5/t0*/, (ULong)disp_cp_chain_me);
+
+      /* c.jalr 0(t0) */
+      p = emit_CR(p, 0b10, 0 /*x0/zero*/, 5 /*x5/t0*/, 0b1001);
+      /* --- END of PATCHABLE BYTES --- */
+
+      /* Fix up the conditional jump, if there was one. */
+      if (!hregIsInvalid(i->RISCV64in.XDirect.cond)) {
+         /* beq cond, zero, delta */
+         UInt cond  = iregEnc(i->RISCV64in.XDirect.cond);
+         UInt delta = p - ptmp;
+         /* delta_min = 4 (beq) + 2 (c.li) + 4 (sd) + 18 (addr48) + 2 (c.jalr)
+                      = 30 */
+         vassert(delta >= 30 && delta < 4096 && (delta & 1) == 0);
+         UInt imm12_1 = (delta >> 1) & 0xfff;
+
+         emit_B(ptmp, 0b1100011, imm12_1, 0b000, cond, 0 /*x0/zero*/);
+      }
+
+      goto done;
+   }
+
+   case RISCV64in_XIndir: {
+      /* We're generating transfers that could lead indirectly to a chain-me, so
+         we need to be sure this is actually allowed -- no-redir translations
+         are not allowed to reach normal translations without going through the
+         scheduler. That means no XDirects or XIndirs out from no-redir
+         translations. Hence: */
+      vassert(disp_cp_xindir != NULL);
+
+      /* First off, if this is conditional, create a conditional jump over the
+         rest of it. Or at least, leave a space for it that we will shortly fill
+         in. */
+      UChar* ptmp = NULL;
+      if (!hregIsInvalid(i->RISCV64in.XIndir.cond)) {
+         ptmp = p;
+         p += 4;
+      }
+
+      /* Update the guest pc. */
+      {
+         /* sd r-dstGA, soff12(base) */
+         UInt src    = iregEnc(i->RISCV64in.XIndir.dstGA);
+         UInt base   = iregEnc(i->RISCV64in.XIndir.base);
+         Int  soff12 = i->RISCV64in.XIndir.soff12;
+         vassert(soff12 >= -2048 && soff12 < 2048);
+         UInt imm11_0 = soff12 & 0xfff;
+
+         p = emit_S(p, 0b0100011, imm11_0, 0b011, base, src);
+      }
+
+      /* li t0, VG_(disp_cp_xindir) */
+      p = imm64_to_ireg(p, 5 /*x5/t0*/, (ULong)disp_cp_xindir);
+
+      /* c.jr 0(t0) */
+      p = emit_CR(p, 0b10, 0 /*x0/zero*/, 5 /*x5/t0*/, 0b1000);
+
+      /* Fix up the conditional jump, if there was one. */
+      if (!hregIsInvalid(i->RISCV64in.XIndir.cond)) {
+         /* beq cond, zero, delta */
+         UInt cond  = iregEnc(i->RISCV64in.XIndir.cond);
+         UInt delta = p - ptmp;
+         /* delta_min = 4 (beq) + 4 (sd) + 2 (c.li) + 2 (c.jr) = 12 */
+         vassert(delta >= 12 && delta < 4096 && (delta & 1) == 0);
+         UInt imm12_1 = (delta >> 1) & 0xfff;
+
+         emit_B(ptmp, 0b1100011, imm12_1, 0b000, cond, 0 /*x0/zero*/);
+      }
+
+      goto done;
+   }
+
+   case RISCV64in_XAssisted: {
+      /* First off, if this is conditional, create a conditional jump over the
+         rest of it. Or at least, leave a space for it that we will shortly fill
+         in. */
+      UChar* ptmp = NULL;
+      if (!hregIsInvalid(i->RISCV64in.XAssisted.cond)) {
+         ptmp = p;
+         p += 4;
+      }
+
+      /* Update the guest pc. */
+      {
+         /* sd r-dstGA, soff12(base) */
+         UInt src    = iregEnc(i->RISCV64in.XAssisted.dstGA);
+         UInt base   = iregEnc(i->RISCV64in.XAssisted.base);
+         Int  soff12 = i->RISCV64in.XAssisted.soff12;
+         vassert(soff12 >= -2048 && soff12 < 2048);
+         UInt imm11_0 = soff12 & 0xfff;
+
+         p = emit_S(p, 0b0100011, imm11_0, 0b011, base, src);
+      }
+
+      /* li s0, $magic_number */
+      UInt trcval = 0;
+      switch (i->RISCV64in.XAssisted.jk) {
+      case Ijk_ClientReq:
+         trcval = VEX_TRC_JMP_CLIENTREQ;
+         break;
+      case Ijk_Sys_syscall:
+         trcval = VEX_TRC_JMP_SYS_SYSCALL;
+         break;
+      case Ijk_EmWarn:
+         trcval = VEX_TRC_JMP_EMWARN;
+         break;
+      case Ijk_EmFail:
+         trcval = VEX_TRC_JMP_EMFAIL;
+         break;
+      case Ijk_NoDecode:
+         trcval = VEX_TRC_JMP_NODECODE;
+         break;
+      case Ijk_InvalICache:
+         trcval = VEX_TRC_JMP_INVALICACHE;
+         break;
+      case Ijk_NoRedir:
+         trcval = VEX_TRC_JMP_NOREDIR;
+         break;
+      case Ijk_SigILL:
+         trcval = VEX_TRC_JMP_SIGILL;
+         break;
+      case Ijk_SigTRAP:
+         trcval = VEX_TRC_JMP_SIGTRAP;
+         break;
+      case Ijk_SigBUS:
+         trcval = VEX_TRC_JMP_SIGBUS;
+         break;
+      case Ijk_SigFPE_IntDiv:
+         trcval = VEX_TRC_JMP_SIGFPE_INTDIV;
+         break;
+      case Ijk_SigFPE_IntOvf:
+         trcval = VEX_TRC_JMP_SIGFPE_INTOVF;
+         break;
+      case Ijk_Boring:
+         trcval = VEX_TRC_JMP_BORING;
+         break;
+      default:
+         ppIRJumpKind(i->RISCV64in.XAssisted.jk);
+         vpanic("emit_RISCV64Instr.RISCV64in_XAssisted: unexpected jump kind");
+      }
+      vassert(trcval != 0);
+      p = imm64_to_ireg(p, 8 /*x8/s0*/, trcval);
+
+      /* li t0, VG_(disp_cp_xassisted) */
+      p = imm64_to_ireg(p, 5 /*x5/t0*/, (ULong)disp_cp_xassisted);
+
+      /* c.jr 0(t0) */
+      p = emit_CR(p, 0b10, 0 /*x0/zero*/, 5 /*x5/t0*/, 0b1000);
+
+      /* Fix up the conditional jump, if there was one. */
+      if (!hregIsInvalid(i->RISCV64in.XAssisted.cond)) {
+         /* beq cond, zero, delta */
+         UInt cond  = iregEnc(i->RISCV64in.XAssisted.cond);
+         UInt delta = p - ptmp;
+         /* delta_min = 4 (beq) + 4 (sd) + 2 (c.li) + 2 (c.li) + 2 (c.jr)
+                      = 14 */
+         vassert(delta >= 14 && delta < 4096 && (delta & 1) == 0);
+         UInt imm12_1 = (delta >> 1) & 0xfff;
+
+         emit_B(ptmp, 0b1100011, imm12_1, 0b000, cond, 0 /*x0/zero*/);
+      }
+
+      goto done;
+   }
+
+   case RISCV64in_EvCheck: {
+      /*    lw t0, soff12_amCounter(base_amCounter)
+            c.addiw t0, -1
+            sw t0, soff12_amCounter(base_amCounter)
+            bge t0, zero, 1f
+            ld t0, soff12_amFailAddr(base_amFailAddr)
+            c.jr 0(t0)
+         1:
+      */
+      UInt base_amCounter   = iregEnc(i->RISCV64in.EvCheck.base_amCounter);
+      Int  soff12_amCounter = i->RISCV64in.EvCheck.soff12_amCounter;
+      vassert(soff12_amCounter >= -2048 && soff12_amCounter < 2048);
+      UInt imm11_0_amCounter = soff12_amCounter & 0xfff;
+
+      UInt base_amFailAddr   = iregEnc(i->RISCV64in.EvCheck.base_amFailAddr);
+      Int  soff12_amFailAddr = i->RISCV64in.EvCheck.soff12_amFailAddr;
+      vassert(soff12_amFailAddr >= -2048 && soff12_amFailAddr < 2048);
+      UInt imm11_0_amFailAddr = soff12_amFailAddr & 0xfff;
+
+      p = emit_I(p, 0b0000011, 5 /*x5/t0*/, 0b010, base_amCounter,
+                 imm11_0_amCounter);
+      p = emit_CI(p, 0b01, -1 & 0x3f, 5 /*x5/t0*/, 0b001);
+      p = emit_S(p, 0b0100011, imm11_0_amCounter, 0b010, base_amCounter,
+                 5 /*x5/t0*/);
+      p = emit_B(p, 0b1100011, (10 >> 1) & 0xfff, 0b101, 5 /*x5/t0*/,
+                 0 /*x0/zero*/);
+      p = emit_I(p, 0b0000011, 5 /*x5/t0*/, 0b011, base_amFailAddr,
+                 imm11_0_amFailAddr);
+      p = emit_CR(p, 0b10, 0 /*x0/zero*/, 5 /*x5/t0*/, 0b1000);
+
+      /* Crosscheck. */
+      vassert(evCheckSzB_RISCV64() == p - buf);
+      goto done;
+   }
+
+   default:
+      goto bad;
+   }
+
+bad:
+   ppRISCV64Instr(i, mode64);
+   vpanic("emit_RISCV64Instr");
+   /*NOTREACHED*/
+
+done:
+   vassert(p - &buf[0] <= 44);
+   return p - &buf[0];
+}
+
+/* Return the number of bytes emitted for an RISCV64in_EvCheck, as produced by
+   emit_RISCV64Instr(). */
+Int evCheckSzB_RISCV64(void) { return 20; }
+
+/* NB: what goes on here has to be very closely coordinated with the emitInstr
+   case for XDirect, above. */
+VexInvalRange chainXDirect_RISCV64(VexEndness  endness_host,
+                                   void*       place_to_chain,
+                                   const void* disp_cp_chain_me_EXPECTED,
+                                   const void* place_to_jump_to)
+{
+   vassert(endness_host == VexEndnessLE);
+
+   /* What we're expecting to see is:
+        lui t0, disp_cp_chain_me_to_EXPECTED[47:28]'
+        addiw t0, t0, disp_cp_chain_me_to_EXPECTED[27:16]'
+        c.slli t0, 12
+        addi t0, t0, disp_cp_chain_me_to_EXPECTED[15:4]'
+        c.slli t0, 4
+        c.addi t0, disp_cp_chain_me_to_EXPECTED[3:0]'
+        c.jalr 0(t0)
+      viz
+        <18 bytes generated by addr48_to_ireg_EXACTLY_18B>
+        82 92
+   */
+   UChar* p = place_to_chain;
+   vassert(((HWord)p & 1) == 0);
+   vassert(is_addr48_to_ireg_EXACTLY_18B(p, 5 /*x5/t0*/,
+                                         (ULong)disp_cp_chain_me_EXPECTED));
+   vassert(p[18] == 0x82 && p[19] == 0x92);
+
+   /* And what we want to change it to is:
+        lui t0, place_to_jump[47:28]'
+        addiw t0, t0, place_to_jump[27:16]'
+        c.slli t0, 12
+        addi t0, t0, place_to_jump[15:4]'
+        c.slli t0, 4
+        c.addi t0, place_to_jump[3:0]'
+        c.jr 0(t0)
+      viz
+        <18 bytes generated by addr48_to_ireg_EXACTLY_18B>
+        82 82
+
+      The replacement has the same length as the original.
+   */
+   (void)addr48_to_ireg_EXACTLY_18B(p, 5 /*x5/t0*/, (ULong)place_to_jump_to);
+   p[18] = 0x82;
+   p[19] = 0x82;
+
+   VexInvalRange vir = {(HWord)p, 20};
+   return vir;
+}
+
+/* NB: what goes on here has to be very closely coordinated with the emitInstr
+   case for XDirect, above. */
+VexInvalRange unchainXDirect_RISCV64(VexEndness  endness_host,
+                                     void*       place_to_unchain,
+                                     const void* place_to_jump_to_EXPECTED,
+                                     const void* disp_cp_chain_me)
+{
+   vassert(endness_host == VexEndnessLE);
+
+   /* What we're expecting to see is:
+        lui t0, place_to_jump_to_EXPECTED[47:28]'
+        addiw t0, t0, place_to_jump_to_EXPECTED[27:16]'
+        c.slli t0, 12
+        addi t0, t0, place_to_jump_to_EXPECTED[15:4]'
+        c.slli t0, 4
+        c.addi t0, place_to_jump_to_EXPECTED[3:0]'
+        c.jr 0(t0)
+      viz
+        <18 bytes generated by addr48_to_ireg_EXACTLY_18B>
+        82 82
+   */
+   UChar* p = place_to_unchain;
+   vassert(((HWord)p & 1) == 0);
+   vassert(is_addr48_to_ireg_EXACTLY_18B(p, 5 /*x5/t0*/,
+                                         (ULong)place_to_jump_to_EXPECTED));
+   vassert(p[18] == 0x82 && p[19] == 0x82);
+
+   /* And what we want to change it to is:
+        lui t0, disp_cp_chain_me[47:28]'
+        addiw t0, t0, disp_cp_chain_me[27:16]'
+        c.slli t0, 12
+        addi t0, t0, disp_cp_chain_me[15:4]'
+        c.slli t0, 4
+        c.addi t0, disp_cp_chain_me[3:0]'
+        c.jalr 0(t0)
+      viz
+        <18 bytes generated by addr48_to_ireg_EXACTLY_18B>
+        82 92
+
+      The replacement has the same length as the original.
+   */
+   (void)addr48_to_ireg_EXACTLY_18B(p, 5 /*x5/t0*/, (ULong)disp_cp_chain_me);
+   p[18] = 0x82;
+   p[19] = 0x89;
+
+   VexInvalRange vir = {(HWord)p, 20};
+   return vir;
+}
+
+/* Patch the counter address into a profile inc point, as previously created by
+   the Ain_ProfInc case for emit_RISCV64Instr(). */
+VexInvalRange patchProfInc_RISCV64(VexEndness   endness_host,
+                                   void*        place_to_patch,
+                                   const ULong* location_of_counter)
+{
+   vassert(endness_host == VexEndnessLE);
+
+   vpanic("patchProfInc_RISCV64");
+
+   VexInvalRange vir = {(HWord)place_to_patch, 0};
+   return vir;
+}
+
+/*--------------------------------------------------------------------*/
+/*--- end                                      host_riscv64_defs.c ---*/
+/*--------------------------------------------------------------------*/
--- /dev/null
+++ b/VEX/priv/host_riscv64_defs.h
@@ -0,0 +1,1033 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                    host_riscv64_defs.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef __VEX_HOST_RISCV64_DEFS_H
+#define __VEX_HOST_RISCV64_DEFS_H
+
+#include "libvex.h"
+#include "libvex_basictypes.h"
+
+#include "host_generic_regs.h"
+
+/*------------------------------------------------------------*/
+/*--- Registers                                            ---*/
+/*------------------------------------------------------------*/
+
+#define ST_IN static inline
+ST_IN HReg hregRISCV64_x18(void) { return mkHReg(False, HRcInt64, 18, 0); }
+ST_IN HReg hregRISCV64_x19(void) { return mkHReg(False, HRcInt64, 19, 1); }
+ST_IN HReg hregRISCV64_x20(void) { return mkHReg(False, HRcInt64, 20, 2); }
+ST_IN HReg hregRISCV64_x21(void) { return mkHReg(False, HRcInt64, 21, 3); }
+ST_IN HReg hregRISCV64_x22(void) { return mkHReg(False, HRcInt64, 22, 4); }
+ST_IN HReg hregRISCV64_x23(void) { return mkHReg(False, HRcInt64, 23, 5); }
+ST_IN HReg hregRISCV64_x24(void) { return mkHReg(False, HRcInt64, 24, 6); }
+ST_IN HReg hregRISCV64_x25(void) { return mkHReg(False, HRcInt64, 25, 7); }
+ST_IN HReg hregRISCV64_x26(void) { return mkHReg(False, HRcInt64, 26, 8); }
+ST_IN HReg hregRISCV64_x27(void) { return mkHReg(False, HRcInt64, 27, 9); }
+
+ST_IN HReg hregRISCV64_x10(void) { return mkHReg(False, HRcInt64, 10, 10); }
+ST_IN HReg hregRISCV64_x11(void) { return mkHReg(False, HRcInt64, 11, 11); }
+ST_IN HReg hregRISCV64_x12(void) { return mkHReg(False, HRcInt64, 12, 12); }
+ST_IN HReg hregRISCV64_x13(void) { return mkHReg(False, HRcInt64, 13, 13); }
+ST_IN HReg hregRISCV64_x14(void) { return mkHReg(False, HRcInt64, 14, 14); }
+ST_IN HReg hregRISCV64_x15(void) { return mkHReg(False, HRcInt64, 15, 15); }
+ST_IN HReg hregRISCV64_x16(void) { return mkHReg(False, HRcInt64, 16, 16); }
+ST_IN HReg hregRISCV64_x17(void) { return mkHReg(False, HRcInt64, 17, 17); }
+
+ST_IN HReg hregRISCV64_f0(void) { return mkHReg(False, HRcFlt64, 0, 18); }
+ST_IN HReg hregRISCV64_f1(void) { return mkHReg(False, HRcFlt64, 1, 19); }
+ST_IN HReg hregRISCV64_f2(void) { return mkHReg(False, HRcFlt64, 2, 20); }
+ST_IN HReg hregRISCV64_f3(void) { return mkHReg(False, HRcFlt64, 3, 21); }
+ST_IN HReg hregRISCV64_f4(void) { return mkHReg(False, HRcFlt64, 4, 22); }
+ST_IN HReg hregRISCV64_f5(void) { return mkHReg(False, HRcFlt64, 5, 23); }
+ST_IN HReg hregRISCV64_f6(void) { return mkHReg(False, HRcFlt64, 6, 24); }
+ST_IN HReg hregRISCV64_f7(void) { return mkHReg(False, HRcFlt64, 7, 25); }
+
+ST_IN HReg hregRISCV64_f10(void) { return mkHReg(False, HRcFlt64, 10, 26); }
+ST_IN HReg hregRISCV64_f11(void) { return mkHReg(False, HRcFlt64, 11, 27); }
+ST_IN HReg hregRISCV64_f12(void) { return mkHReg(False, HRcFlt64, 12, 28); }
+ST_IN HReg hregRISCV64_f13(void) { return mkHReg(False, HRcFlt64, 13, 29); }
+ST_IN HReg hregRISCV64_f14(void) { return mkHReg(False, HRcFlt64, 14, 30); }
+ST_IN HReg hregRISCV64_f15(void) { return mkHReg(False, HRcFlt64, 15, 31); }
+ST_IN HReg hregRISCV64_f16(void) { return mkHReg(False, HRcFlt64, 16, 32); }
+ST_IN HReg hregRISCV64_f17(void) { return mkHReg(False, HRcFlt64, 17, 33); }
+
+ST_IN HReg hregRISCV64_f28(void) { return mkHReg(False, HRcFlt64, 28, 34); }
+ST_IN HReg hregRISCV64_f29(void) { return mkHReg(False, HRcFlt64, 29, 35); }
+ST_IN HReg hregRISCV64_f30(void) { return mkHReg(False, HRcFlt64, 30, 36); }
+ST_IN HReg hregRISCV64_f31(void) { return mkHReg(False, HRcFlt64, 31, 37); }
+
+ST_IN HReg hregRISCV64_x0(void) { return mkHReg(False, HRcInt64, 0, 38); }
+ST_IN HReg hregRISCV64_x2(void) { return mkHReg(False, HRcInt64, 2, 39); }
+ST_IN HReg hregRISCV64_x8(void) { return mkHReg(False, HRcInt64, 8, 40); }
+#undef ST_IN
+
+/* Number of registers used for argument passing in function calls. */
+#define RISCV64_N_ARGREGS  8 /* x10/a0 .. x17/a7 */
+#define RISCV64_N_FARGREGS 8 /* f10/fa0 .. f17/fa7 */
+
+/*------------------------------------------------------------*/
+/*--- Instructions                                         ---*/
+/*------------------------------------------------------------*/
+
+/* The kind of instructions. */
+typedef enum {
+   RISCV64in_LI = 0x52640000, /* Load immediate pseudoinstruction. */
+   RISCV64in_MV,              /* Copy one register to another. */
+   RISCV64in_ADD,             /* Addition of two registers. */
+   RISCV64in_ADDI,            /* Addition of a register and a sx-12-bit
+                                 immediate. */
+   RISCV64in_ADDW,            /* 32-bit addition of two registers. */
+   RISCV64in_ADDIW,           /* 32-bit addition of a register and a sx-12-bit
+                                 immediate. */
+   RISCV64in_SUB,             /* Subtraction of one register from another. */
+   RISCV64in_SUBW,            /* 32-bit subtraction of one register from
+                                 another. */
+   RISCV64in_XOR,             /* Bitwise XOR of two registers. */
+   RISCV64in_XORI,            /* Bitwise XOR of a register and a sx-12-bit
+                                 immediate. */
+   RISCV64in_OR,              /* Bitwise OR of two registers. */
+   RISCV64in_AND,             /* Bitwise AND of two registers. */
+   RISCV64in_ANDI,            /* Bitwise AND of a register and a sx-12-bit
+                                 immediate. */
+   RISCV64in_SLL,             /* Logical left shift on a register. */
+   RISCV64in_SRL,             /* Logical right shift on a register. */
+   RISCV64in_SRA,             /* Arithmetic right shift on a register. */
+   RISCV64in_SLLI,            /* Logical left shift on a register by a 6-bit
+                                 immediate. */
+   RISCV64in_SRLI,            /* Logical right shift on a register by a 6-bit
+                                 immediate. */
+   RISCV64in_SRAI,            /* Arithmetic right shift on a register by a 6-bit
+                                 immediate. */
+   RISCV64in_SLLW,            /* 32-bit logical left shift on a register. */
+   RISCV64in_SRLW,            /* 32-bit logical right shift on a register. */
+   RISCV64in_SRAW,            /* 32-bit arithmetic right shift on a register. */
+   RISCV64in_SLT,             /* Signed comparison of two registers. */
+   RISCV64in_SLTU,            /* Unsigned comparison of two registers. */
+   RISCV64in_SLTIU,           /* Unsigned comparison of a register and
+                                 a sx-12-bit immediate. */
+   RISCV64in_CSRRW,           /* Atomic swap of values in a CSR an integer
+                                 register. */
+   RISCV64in_MUL,             /* Multiplication of two registers, producing the
+                                 lower 64 bits. */
+   RISCV64in_MULH,            /* Signed multiplication of two registers,
+                                 producing the upper 64 bits. */
+   RISCV64in_MULHU,           /* Unsigned multiplication of two registers,
+                                 producing the upper 64 bits. */
+   RISCV64in_DIV,             /* Signed division of one register by another. */
+   RISCV64in_DIVU,            /* Unsigned division of one register by
+                                 another. */
+   RISCV64in_REM,             /* Remainder from signed division of one register
+                                 by another. */
+   RISCV64in_REMU,            /* Remainder from unsigned division of one
+                                 register by another. */
+   RISCV64in_MULW,            /* 32-bit multiplication of two registers,
+                                 producing the lower 32 bits. */
+   RISCV64in_DIVW,            /* 32-bit signed division of one register by
+                                 another. */
+   RISCV64in_DIVUW,           /* 32-bit unsigned division of one register by
+                                 another. */
+   RISCV64in_REMW,            /* Remainder from 32-bit signed division of one
+                                 register by another. */
+   RISCV64in_REMUW,           /* Remainder from 32-bit unsigned division of one
+                                 register by another. */
+   RISCV64in_LD,              /* 64-bit load. */
+   RISCV64in_LW,              /* sx-32-to-64-bit load. */
+   RISCV64in_LH,              /* sx-16-to-64-bit load. */
+   RISCV64in_LB,              /* sx-8-to-64-bit load. */
+   RISCV64in_SD,              /* 64-bit store. */
+   RISCV64in_SW,              /* 32-bit store. */
+   RISCV64in_SH,              /* 16-bit store. */
+   RISCV64in_SB,              /* 8-bit store. */
+   RISCV64in_LR_W,            /* sx-32-to-64-bit load-reserved. */
+   RISCV64in_SC_W,            /* 32-bit store-conditional. */
+   RISCV64in_FMADD_S,         /* Fused multiply-add of 32-bit floating-point
+                                 registers. */
+   RISCV64in_FADD_S,          /* Addition of two 32-bit floating-point
+                                 registers. */
+   RISCV64in_FMUL_S,          /* Multiplication of two 32-bit floating-point
+                                 registers. */
+   RISCV64in_FDIV_S,          /* Division of a 32-bit floating-point register by
+                                 another. */
+   RISCV64in_FSQRT_S,         /* Square root of a 32-bit floating-point
+                                 register. */
+   RISCV64in_FSGNJN_S,        /* Copy of a 32-bit floating-point register to
+                                 another with the sign bit taken from the second
+                                 input and negated. */
+   RISCV64in_FSGNJX_S,        /* Copy of a 32-bit floating-point register to
+                                 another with the sign bit XOR'ed from the
+                                 second input. */
+   RISCV64in_FMIN_S,          /* Select minimum-number of two 32-bit
+                                 floating-point registers. */
+   RISCV64in_FMAX_S,          /* Select maximum-number of two 32-bit
+                                 floating-point registers. */
+   RISCV64in_FMV_X_W,         /* Move as-is a 32-bit value from a floating-point
+                                 register to an integer register. */
+   RISCV64in_FMV_W_X,         /* Move as-is a 32-bit value from an integer
+                                 register to a floating-point register. */
+   RISCV64in_FMV_D,           /* Copy one 64-bit floating-point register to
+                                 another. */
+   RISCV64in_FMADD_D,         /* Fused multiply-add of 64-bit floating-point
+                                 registers. */
+   RISCV64in_FADD_D,          /* Addition of two 64-bit floating-point
+                                 registers. */
+   RISCV64in_FSUB_D,          /* Subtraction of one 64-bit floating-point
+                                 register from another. */
+   RISCV64in_FMUL_D,          /* Multiplication of two 64-bit floating-point
+                                 registers. */
+   RISCV64in_FDIV_D,          /* Division of a 64-bit floating-point register by
+                                 another. */
+   RISCV64in_FSQRT_D,         /* Square root of a 64-bit floating-point
+                                 register. */
+   RISCV64in_FSGNJN_D,        /* Copy of a 64-bit floating-point register to
+                                 another with the sign bit taken from the second
+                                 input and negated. */
+   RISCV64in_FSGNJX_D,        /* Copy of a 64-bit floating-point register to
+                                 another with the sign bit XOR'ed from the
+                                 second input. */
+   RISCV64in_FMIN_D,          /* Select minimum-number of two 64-bit
+                                 floating-point registers. */
+   RISCV64in_FMAX_D,          /* Select maximum-number of two 64-bit
+                                 floating-point registers. */
+   RISCV64in_FEQ_D,           /* Equality comparison of two 64-bit
+                                 floating-point registers. */
+   RISCV64in_FLT_D,           /* Less-than comparison of two 64-bit
+                                 floating-point registers. */
+   RISCV64in_FCVT_S_D,        /* Convert a 64-bit floating-point number to
+                                 a 32-bit floating-point number. */
+   RISCV64in_FCVT_D_S,        /* Convert a 32-bit floating-point number to
+                                 a 64-bit floating-point number. */
+   RISCV64in_FCVT_W_D,        /* Convert a 64-bit floating-point number to
+                                 a 32-bit signed integer. */
+   RISCV64in_FCVT_WU_D,       /* Convert a 64-bit floating-point number to
+                                 a 32-bit unsigned integer. */
+   RISCV64in_FCVT_D_W,        /* Convert a 32-bit signed integer to a 64-bit
+                                 floating-point number. */
+   RISCV64in_FCVT_D_WU,       /* Convert a 32-bit unsigned integer to a 64-bit
+                                 floating-point number. */
+   RISCV64in_FCVT_L_D,        /* Convert a 64-bit floating-point number to
+                                 a 64-bit signed integer. */
+   RISCV64in_FCVT_LU_D,       /* Convert a 64-bit floating-point number to
+                                 a 64-bit unsigned integer. */
+   RISCV64in_FCVT_D_L,        /* Convert a 64-bit signed integer to a 64-bit
+                                 floating-point number. */
+   RISCV64in_FCVT_D_LU,       /* Convert a 64-bit unsigned integer to a 64-bit
+                                 floating-point number. */
+   RISCV64in_FMV_X_D,         /* Move as-is a 64-bit value from a floating-point
+                                 register to an integer register. */
+   RISCV64in_FMV_D_X,         /* Move as-is a 64-bit value from an integer
+                                 register to a floating-point register. */
+   RISCV64in_FLW,             /* 32-bit floating-point load. */
+   RISCV64in_FLD,             /* 64-bit floating-point load. */
+   RISCV64in_FSW,             /* 32-bit floating-point store. */
+   RISCV64in_FSD,             /* 64-bit floating-point store. */
+   RISCV64in_CAS_W,           /* 32-bit compare-and-swap pseudoinstruction. */
+   RISCV64in_CAS_D,           /* 64-bit compare-and-swap pseudoinstruction. */
+   RISCV64in_FENCE,           /* Device I/O and memory fence. */
+   RISCV64in_CSEL,            /* Conditional-select pseudoinstruction. */
+   RISCV64in_Call,            /* Call pseudoinstruction. */
+   RISCV64in_XDirect,         /* Direct transfer to guest address. */
+   RISCV64in_XIndir,          /* Indirect transfer to guest address. */
+   RISCV64in_XAssisted,       /* Assisted transfer to guest address. */
+   RISCV64in_EvCheck,         /* Event check. */
+} RISCV64InstrTag;
+
+typedef struct {
+   RISCV64InstrTag tag;
+   union {
+      /* Load immediate pseudoinstruction. */
+      struct {
+         HReg  dst;
+         ULong imm64;
+      } LI;
+      /* Copy one register to another. */
+      struct {
+         HReg dst;
+         HReg src;
+      } MV;
+      /* Addition of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } ADD;
+      /* Addition of a register and a sx-12-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         Int  simm12;
+      } ADDI;
+      /* 32-bit addition of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } ADDW;
+      /* 32-bit addition of a register and a sx-12-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         Int  simm12;
+      } ADDIW;
+      /* Subtraction of one register from another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SUB;
+      /* 32-bit subtraction of one register from another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SUBW;
+      /* Bitwise XOR of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } XOR;
+      /* Bitwise XOR of a register and a sx-12-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         Int  simm12;
+      } XORI;
+      /* Bitwise OR of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } OR;
+      /* Bitwise AND of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } AND;
+      /* Bitwise AND of a register and a sx-12-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         Int  simm12;
+      } ANDI;
+      /* Logical left shift on a register. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SLL;
+      /* Logical right shift on a register. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SRL;
+      /* Arithmetic right shift on a register. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SRA;
+      /* Logical left shift on a register by a 6-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         UInt uimm6;
+      } SLLI;
+      /* Logical right shift on a register by a 6-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         UInt uimm6;
+      } SRLI;
+      /* Arithmetic right shift on a register by a 6-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         UInt uimm6;
+      } SRAI;
+      /* 32-bit logical left shift on a register. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SLLW;
+      /* 32-bit logical right shift on a register. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SRLW;
+      /* 32-bit arithmetic right shift on a register. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SRAW;
+      /* Signed comparison of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SLT;
+      /* Unsigned comparison of two registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } SLTU;
+      /* Unsigned comparison of a register and a sx-12-bit immediate. */
+      struct {
+         HReg dst;
+         HReg src;
+         Int  simm12;
+      } SLTIU;
+      /* Atomic swap of values in a CSR an integer register. */
+      struct {
+         HReg dst;
+         HReg src;
+         UInt csr;
+      } CSRRW;
+      /* Multiplication of two registers, producing the lower 64 bits. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } MUL;
+      /* Signed multiplication of two registers, producing the upper 64 bits. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } MULH;
+      /* Unsigned multiplication of two registers, producing the upper 64 bits.
+       */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } MULHU;
+      /* Signed division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } DIV;
+      /* Unsigned division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } DIVU;
+      /* Remainder from signed division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } REM;
+      /* Remainder from unsigned division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } REMU;
+      /* 32-bit multiplication of two registers, producing the lower 32 bits. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } MULW;
+      /* 32-bit signed division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } DIVW;
+      /* 32-bit unsigned division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } DIVUW;
+      /* Remainder from 32-bit signed division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } REMW;
+      /* Remainder from 32-bit unsigned division of one register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } REMUW;
+      /* 64-bit load. */
+      struct {
+         HReg dst;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } LD;
+      /* sx-32-to-64-bit load. */
+      struct {
+         HReg dst;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } LW;
+      /* sx-16-to-64-bit load. */
+      struct {
+         HReg dst;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } LH;
+      /* sx-8-to-64-bit load. */
+      struct {
+         HReg dst;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } LB;
+      /* 64-bit store. */
+      struct {
+         HReg src;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } SD;
+      /* 32-bit store. */
+      struct {
+         HReg src;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } SW;
+      /* 16-bit store. */
+      struct {
+         HReg src;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } SH;
+      /* 8-bit store. */
+      struct {
+         HReg src;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } SB;
+      /* sx-32-to-64-bit load-reserved. */
+      struct {
+         HReg dst;
+         HReg addr;
+      } LR_W;
+      /* 32-bit store-conditional. */
+      struct {
+         HReg res;
+         HReg src;
+         HReg addr;
+      } SC_W;
+      /* Fused multiply-add of 32-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+         HReg src3;
+      } FMADD_S;
+      /* Addition of two 32-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FADD_S;
+      /* Multiplication of two 32-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FMUL_S;
+      /* Division of a 32-bit floating-point register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FDIV_S;
+      /* Square root of a 32-bit floating-point register. */
+      struct {
+         HReg dst;
+         HReg src1;
+      } FSQRT_S;
+      /* Copy of a 32-bit floating-point register to another with the sign bit
+         taken from the second input and negated. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FSGNJN_S;
+      /* Copy of a 32-bit floating-point register to another with the sign bit
+         XOR'ed from the second input. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FSGNJX_S;
+      /* Select minimum-number of two 32-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FMIN_S;
+      /* Select maximum-number of two 32-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FMAX_S;
+      /* Move as-is a 32-bit value from a floating-point register to an integer
+         register. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FMV_X_W;
+      /* Move as-is a 32-bit value from an integer register to a floating-point
+         register. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FMV_W_X;
+      /* Copy one 64-bit floating-point register to another. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FMV_D;
+      /* Fused multiply-add of 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+         HReg src3;
+      } FMADD_D;
+      /* Addition of two 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FADD_D;
+      /* Subtraction of one 64-bit floating-point register from another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FSUB_D;
+      /* Multiplication of two 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FMUL_D;
+      /* Division of a 64-bit floating-point register by another. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FDIV_D;
+      /* Square root of a 64-bit floating-point register. */
+      struct {
+         HReg dst;
+         HReg src1;
+      } FSQRT_D;
+      /* Copy of a 64-bit floating-point register to another with the sign bit
+         taken from the second input and negated. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FSGNJN_D;
+      /* Copy of a 64-bit floating-point register to another with the sign bit
+         XOR'ed from the second input. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FSGNJX_D;
+      /* Select minimum-number of two 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FMIN_D;
+      /* Select maximum-number of two 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FMAX_D;
+      /* Equality comparison of two 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FEQ_D;
+      /* Less-than comparison of two 64-bit floating-point registers. */
+      struct {
+         HReg dst;
+         HReg src1;
+         HReg src2;
+      } FLT_D;
+      /* Convert a 64-bit floating-point number to a 32-bit floating-point
+         number. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_S_D;
+      /* Convert a 32-bit floating-point number to a 64-bit floating-point
+         number. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_D_S;
+      /* Convert a 64-bit floating-point number to a 32-bit signed integer. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_W_D;
+      /* Convert a 64-bit floating-point number to a 32-bit unsigned integer. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_WU_D;
+      /* Convert a 32-bit signed integer to a 64-bit floating-point number. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_D_W;
+      /* Convert a 32-bit unsigned integer to a 64-bit floating-point number. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_D_WU;
+      /* Convert a 64-bit floating-point value to a 64-bit signed integer. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_L_D;
+      /* Convert a 64-bit floating-point value to a 64-bit unsigned integer. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_LU_D;
+      /* Convert a 64-bit signed integer to a 64-bit floating-point number. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_D_L;
+      /* Convert a 64-bit unsigned integer to a 64-bit floating-point number. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FCVT_D_LU;
+      /* Move as-is a 64-bit value from a floating-point register to an integer
+         register. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FMV_X_D;
+      /* Move as-is a 64-bit value from an integer register to a floating-point
+         register. */
+      struct {
+         HReg dst;
+         HReg src;
+      } FMV_D_X;
+      /* 64-bit floating-point load. */
+      struct {
+         HReg dst;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } FLD;
+      /* 32-bit floating-point load. */
+      struct {
+         HReg dst;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } FLW;
+      /* 64-bit floating-point store. */
+      struct {
+         HReg src;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } FSD;
+      /* 32-bit floating-point store. */
+      struct {
+         HReg src;
+         HReg base;
+         Int  soff12; /* -2048 .. +2047 */
+      } FSW;
+      /* 32-bit compare-and-swap pseudoinstruction. */
+      struct {
+         HReg old;
+         HReg addr;
+         HReg expd;
+         HReg data;
+      } CAS_W;
+      /* 64-bit compare-and-swap pseudoinstruction. */
+      struct {
+         HReg old;
+         HReg addr;
+         HReg expd;
+         HReg data;
+      } CAS_D;
+      /* Device I/O and memory fence. */
+      struct {
+      } FENCE;
+      /* Conditional-select pseudoinstruction. */
+      struct {
+         HReg dst;
+         HReg iftrue;
+         HReg iffalse;
+         HReg cond;
+      } CSEL;
+      /* Call pseudoinstruction. Call a target (an absolute address), on a given
+         condition register. */
+      struct {
+         RetLoc rloc;      /* Where the return value will be. */
+         Addr64 target;    /* Target address of the call. */
+         HReg   cond;      /* Condition, can be INVALID_HREG for "always". */
+         UChar  nArgRegs;  /* # regs carrying integer args: 0 .. 8 */
+         UChar  nFArgRegs; /* # regs carrying floating-point args: 0 .. 8 */
+      } Call;
+      /* Update the guest pc value, then exit requesting to chain to it. May be
+         conditional. */
+      struct {
+         Addr64 dstGA;    /* Next guest address. */
+         HReg   base;     /* Base to access the guest state. */
+         Int    soff12;   /* Offset from the base register to access pc. */
+         HReg   cond;     /* Condition, can be INVALID_HREG for "always". */
+         Bool   toFastEP; /* Chain to the slow or fast point? */
+      } XDirect;
+      /* Boring transfer to a guest address not known at JIT time. Not
+         chainable. May be conditional. */
+      struct {
+         HReg dstGA;  /* Next guest address. */
+         HReg base;   /* Base to access the guest state. */
+         Int  soff12; /* Offset from the base register to access pc. */
+         HReg cond;   /* Condition, can be INVALID_HREG for "always". */
+      } XIndir;
+      /* Assisted transfer to a guest address, most general case. Not chainable.
+         May be conditional. */
+      struct {
+         HReg       dstGA;  /* Next guest address. */
+         HReg       base;   /* Base to access the guest state. */
+         Int        soff12; /* Offset from the base register to access pc. */
+         HReg       cond;   /* Condition, can be INVALID_HREG for "always". */
+         IRJumpKind jk;
+      } XAssisted;
+      /* Event check. */
+      struct {
+         HReg base_amCounter;   /* Base to access the guest state for
+                                   host_EvC_Counter. */
+         Int soff12_amCounter;  /* Offset from the base register to access
+                                   host_EvC_COUNTER. */
+         HReg base_amFailAddr;  /* Base to access the guest state for for
+                                   host_EvC_FAILADDR. */
+         Int soff12_amFailAddr; /* Offset from the base register to access
+                                   host_EvC_FAILADDR. */
+      } EvCheck;
+   } RISCV64in;
+} RISCV64Instr;
+
+RISCV64Instr* RISCV64Instr_LI(HReg dst, ULong imm64);
+RISCV64Instr* RISCV64Instr_MV(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_ADD(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_ADDI(HReg dst, HReg src, Int simm12);
+RISCV64Instr* RISCV64Instr_ADDW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_ADDIW(HReg dst, HReg src, Int simm12);
+RISCV64Instr* RISCV64Instr_SUB(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SUBW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_XOR(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_XORI(HReg dst, HReg src, Int simm12);
+RISCV64Instr* RISCV64Instr_OR(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_AND(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_ANDI(HReg dst, HReg src, Int simm12);
+RISCV64Instr* RISCV64Instr_SLL(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SRL(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SRA(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SLLI(HReg dst, HReg src, UInt uimm6);
+RISCV64Instr* RISCV64Instr_SRLI(HReg dst, HReg src, UInt uimm6);
+RISCV64Instr* RISCV64Instr_SRAI(HReg dst, HReg src, UInt uimm6);
+RISCV64Instr* RISCV64Instr_SLLW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SRLW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SRAW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SLT(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SLTU(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_SLTIU(HReg dst, HReg src, Int simm12);
+RISCV64Instr* RISCV64Instr_CSRRW(HReg dst, HReg src, UInt csr);
+RISCV64Instr* RISCV64Instr_MUL(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_MULH(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_MULHU(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_DIV(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_DIVU(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_REM(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_REMU(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_MULW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_DIVW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_DIVUW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_REMW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_REMUW(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_LD(HReg dst, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_LW(HReg dst, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_LH(HReg dst, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_LB(HReg dst, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_SD(HReg src, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_SW(HReg src, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_SH(HReg src, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_SB(HReg src, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_LR_W(HReg dst, HReg addr);
+RISCV64Instr* RISCV64Instr_FMADD_S(HReg dst, HReg src1, HReg src2, HReg src3);
+RISCV64Instr* RISCV64Instr_FADD_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMUL_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FDIV_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FSQRT_S(HReg dst, HReg src1);
+RISCV64Instr* RISCV64Instr_FSGNJN_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FSGNJX_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMIN_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMAX_S(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMV_X_W(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FMV_W_X(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FMV_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FMADD_D(HReg dst, HReg src1, HReg src2, HReg src3);
+RISCV64Instr* RISCV64Instr_FADD_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FSUB_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMUL_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FDIV_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FSQRT_D(HReg dst, HReg src1);
+RISCV64Instr* RISCV64Instr_FSGNJN_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FSGNJX_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMIN_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FMAX_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FEQ_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FLT_D(HReg dst, HReg src1, HReg src2);
+RISCV64Instr* RISCV64Instr_FCVT_S_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_D_S(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_W_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_WU_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_D_W(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_D_WU(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_L_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_LU_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_D_L(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FCVT_D_LU(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FMV_X_D(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FMV_D_X(HReg dst, HReg src);
+RISCV64Instr* RISCV64Instr_FLD(HReg dst, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_FLW(HReg dst, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_FSD(HReg src, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_FSW(HReg src, HReg base, Int soff12);
+RISCV64Instr* RISCV64Instr_SC_W(HReg res, HReg src, HReg addr);
+RISCV64Instr* RISCV64Instr_CAS_W(HReg old, HReg addr, HReg expd, HReg data);
+RISCV64Instr* RISCV64Instr_CAS_D(HReg old, HReg addr, HReg expd, HReg data);
+RISCV64Instr* RISCV64Instr_FENCE(void);
+RISCV64Instr* RISCV64Instr_CSEL(HReg dst, HReg iftrue, HReg iffalse, HReg cond);
+RISCV64Instr* RISCV64Instr_Call(
+   RetLoc rloc, Addr64 target, HReg cond, UChar nArgRegs, UChar nFArgRegs);
+RISCV64Instr* RISCV64Instr_XDirect(
+   Addr64 dstGA, HReg base, Int soff12, HReg cond, Bool toFastEP);
+RISCV64Instr* RISCV64Instr_XIndir(HReg dstGA, HReg base, Int soff12, HReg cond);
+RISCV64Instr* RISCV64Instr_XAssisted(
+   HReg dstGA, HReg base, Int soff12, HReg cond, IRJumpKind jk);
+RISCV64Instr* RISCV64Instr_EvCheck(HReg base_amCounter,
+                                   Int  soff12_amCounter,
+                                   HReg base_amFailAddr,
+                                   Int  soff12_amFailAddr);
+
+/*------------------------------------------------------------*/
+/*--- Misc helpers                                         ---*/
+/*------------------------------------------------------------*/
+
+static inline HReg get_baseblock_register(void) { return hregRISCV64_x8(); }
+#define BASEBLOCK_OFFSET_ADJUSTMENT 2048
+
+/*------------------------------------------------------------*/
+/* --- Interface exposed to VEX                           --- */
+/*------------------------------------------------------------*/
+
+UInt ppHRegRISCV64(HReg reg);
+
+void ppRISCV64Instr(const RISCV64Instr* i, Bool mode64);
+
+const RRegUniverse* getRRegUniverse_RISCV64(void);
+
+/* Some functions that insulate the register allocator from details of the
+   underlying instruction set. */
+void getRegUsage_RISCV64Instr(HRegUsage* u, const RISCV64Instr* i, Bool mode64);
+void mapRegs_RISCV64Instr(HRegRemap* m, RISCV64Instr* i, Bool mode64);
+
+void genSpill_RISCV64(
+   /*OUT*/ HInstr** i1, /*OUT*/ HInstr** i2, HReg rreg, Int offset, Bool);
+void genReload_RISCV64(
+   /*OUT*/ HInstr** i1, /*OUT*/ HInstr** i2, HReg rreg, Int offset, Bool);
+RISCV64Instr* genMove_RISCV64(HReg from, HReg to, Bool);
+
+Int emit_RISCV64Instr(/*MB_MOD*/ Bool*    is_profInc,
+                      UChar*              buf,
+                      Int                 nbuf,
+                      const RISCV64Instr* i,
+                      Bool                mode64,
+                      VexEndness          endness_host,
+                      const void*         disp_cp_chain_me_to_slowEP,
+                      const void*         disp_cp_chain_me_to_fastEP,
+                      const void*         disp_cp_xindir,
+                      const void*         disp_cp_xassisted);
+
+/* Return the number of bytes of code needed for an event check. */
+Int evCheckSzB_RISCV64(void);
+
+/* Perform a chaining and unchaining of an XDirect jump. */
+VexInvalRange chainXDirect_RISCV64(VexEndness  endness_host,
+                                   void*       place_to_chain,
+                                   const void* disp_cp_chain_me_EXPECTED,
+                                   const void* place_to_jump_to);
+
+VexInvalRange unchainXDirect_RISCV64(VexEndness  endness_host,
+                                     void*       place_to_unchain,
+                                     const void* place_to_jump_to_EXPECTED,
+                                     const void* disp_cp_chain_me);
+
+/* Patch the counter location into an existing ProfInc point. */
+VexInvalRange patchProfInc_RISCV64(VexEndness   endness_host,
+                                   void*        place_to_patch,
+                                   const ULong* location_of_counter);
+
+HInstrArray* iselSB_RISCV64(const IRSB*        bb,
+                            VexArch            arch_host,
+                            const VexArchInfo* archinfo_host,
+                            const VexAbiInfo*  vbi,
+                            Int                offs_Host_EvC_Counter,
+                            Int                offs_Host_EvC_FailAddr,
+                            Bool               chainingAllowed,
+                            Bool               addProfInc,
+                            Addr               max_ga);
+
+#endif /* ndef __VEX_HOST_RISCV64_DEFS_H */
+
+/*--------------------------------------------------------------------*/
+/*--- end                                      host_riscv64_defs.h ---*/
+/*--------------------------------------------------------------------*/
--- /dev/null
+++ b/VEX/priv/host_riscv64_isel.c
@@ -0,0 +1,2083 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                    host_riscv64_isel.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "host_riscv64_defs.h"
+#include "main_globals.h"
+#include "main_util.h"
+
+/*------------------------------------------------------------*/
+/*--- ISelEnv                                              ---*/
+/*------------------------------------------------------------*/
+
+/* This carries around:
+
+   - A mapping from IRTemp to IRType, giving the type of any IRTemp we might
+     encounter. This is computed before insn selection starts, and does not
+     change.
+
+   - A mapping from IRTemp to HReg. This tells the insn selector which virtual
+     register is associated with each IRTemp temporary. This is computed before
+     insn selection starts, and does not change. We expect this mapping to map
+     precisely the same set of IRTemps as the type mapping does.
+
+     - vregmap   holds the primary register for the IRTemp.
+     - vregmapHI is only used for 128-bit integer-typed IRTemps. It holds the
+                 identity of a second 64-bit virtual HReg, which holds the high
+                 half of the value.
+
+   - The code array, that is, the insns selected so far.
+
+   - A counter, for generating new virtual registers.
+
+   - The host hardware capabilities word. This is set at the start and does not
+     change.
+
+   - A Bool for indicating whether we may generate chain-me instructions for
+     control flow transfers, or whether we must use XAssisted.
+
+   - The maximum guest address of any guest insn in this block. Actually, the
+     address of the highest-addressed byte from any insn in this block. Is set
+     at the start and does not change. This is used for detecting jumps which
+     are definitely forward-edges from this block, and therefore can be made
+     (chained) to the fast entry point of the destination, thereby avoiding the
+     destination's event check.
+
+   - An IRExpr*, which may be NULL, holding the IR expression (an
+     IRRoundingMode-encoded value) to which the FPU's rounding mode was most
+     recently set. Setting to NULL is always safe. Used to avoid redundant
+     settings of the FPU's rounding mode, as described in
+     set_fcsr_rounding_mode() below.
+
+   Note, this is all (well, mostly) host-independent.
+*/
+
+typedef struct {
+   /* Constant -- are set at the start and do not change. */
+   IRTypeEnv* type_env;
+
+   HReg* vregmap;
+   HReg* vregmapHI;
+   Int   n_vregmap;
+
+   UInt hwcaps;
+
+   Bool   chainingAllowed;
+   Addr64 max_ga;
+
+   /* These are modified as we go along. */
+   HInstrArray* code;
+   Int          vreg_ctr;
+
+   IRExpr* previous_rm;
+} ISelEnv;
+
+static HReg lookupIRTemp(ISelEnv* env, IRTemp tmp)
+{
+   vassert(tmp >= 0);
+   vassert(tmp < env->n_vregmap);
+   return env->vregmap[tmp];
+}
+
+static void addInstr(ISelEnv* env, RISCV64Instr* instr)
+{
+   addHInstr(env->code, instr);
+   if (vex_traceflags & VEX_TRACE_VCODE) {
+      ppRISCV64Instr(instr, True /*mode64*/);
+      vex_printf("\n");
+   }
+}
+
+static HReg newVRegI(ISelEnv* env)
+{
+   HReg reg = mkHReg(True /*virtual*/, HRcInt64, 0, env->vreg_ctr);
+   env->vreg_ctr++;
+   return reg;
+}
+
+static HReg newVRegF(ISelEnv* env)
+{
+   HReg reg = mkHReg(True /*virtual*/, HRcFlt64, 0, env->vreg_ctr);
+   env->vreg_ctr++;
+   return reg;
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Forward declarations                           ---*/
+/*------------------------------------------------------------*/
+
+/* These are organised as iselXXX and iselXXX_wrk pairs. The iselXXX_wrk do the
+   real work, but are not to be called directly. For each XXX, iselXXX calls its
+   iselXXX_wrk counterpart, then checks that all returned registers are virtual.
+   You should not call the _wrk version directly. */
+
+static HReg iselIntExpr_R(ISelEnv* env, IRExpr* e);
+static void iselInt128Expr(HReg* rHi, HReg* rLo, ISelEnv* env, IRExpr* e);
+static HReg iselFltExpr(ISelEnv* env, IRExpr* e);
+
+/*------------------------------------------------------------*/
+/*--- ISEL: FP rounding mode helpers                       ---*/
+/*------------------------------------------------------------*/
+
+/* Set the FP rounding mode: 'mode' is an I32-typed expression denoting a value
+   of IRRoundingMode. Set the fcsr RISC-V register to have the same rounding.
+
+   All attempts to set the rounding mode have to be routed through this
+   function for things to work properly. Refer to the comment in the AArch64
+   backend for set_FPCR_rounding_mode() how the mechanism relies on the SSA
+   property of IR and CSE.
+*/
+static void set_fcsr_rounding_mode(ISelEnv* env, IRExpr* mode)
+{
+   vassert(typeOfIRExpr(env->type_env, mode) == Ity_I32);
+
+   /* Do we need to do anything? */
+   if (env->previous_rm && env->previous_rm->tag == Iex_RdTmp &&
+       mode->tag == Iex_RdTmp &&
+       env->previous_rm->Iex.RdTmp.tmp == mode->Iex.RdTmp.tmp) {
+      /* No - setting it to what it was before.  */
+      vassert(typeOfIRExpr(env->type_env, env->previous_rm) == Ity_I32);
+      return;
+   }
+
+   /* No luck - we better set it, and remember what we set it to. */
+   env->previous_rm = mode;
+
+   /*
+      rounding mode                 |  IR  | RISC-V
+      ---------------------------------------------
+      to nearest, ties to even      | 0000 |   000
+      to -infinity                  | 0001 |   011
+      to +infinity                  | 0010 |   010
+      to zero                       | 0011 |   001
+      to nearest, ties away from 0  | 0100 |   100
+      prepare for shorter precision | 0101 |   111
+      to away from 0                | 0110 |   111
+      to nearest, ties towards 0    | 0111 |   111
+      invalid                       | 1000 |   111
+
+      All rounding modes not supported on RISC-V are mapped to 111 which is the
+      dynamic mode that is always invalid in fcsr and raises an illegal
+      instruction exception.
+
+      The mapping can be implemented using the following transformation:
+         t0 = 30 >> rm_IR
+         t1 = t0 & 19
+         t2 = t0 + 7
+         t3 = t1 + t2
+         fcsr_rm_RISCV = t3 >> t1
+   */
+   HReg rm_IR  = iselIntExpr_R(env, mode);
+   HReg imm_30 = newVRegI(env);
+   addInstr(env, RISCV64Instr_LI(imm_30, 30));
+   HReg t0 = newVRegI(env);
+   addInstr(env, RISCV64Instr_SRL(t0, imm_30, rm_IR));
+   HReg t1 = newVRegI(env);
+   addInstr(env, RISCV64Instr_ANDI(t1, t0, 19));
+   HReg t2 = newVRegI(env);
+   addInstr(env, RISCV64Instr_ADDI(t2, t0, 7));
+   HReg t3 = newVRegI(env);
+   addInstr(env, RISCV64Instr_ADD(t3, t1, t2));
+   HReg fcsr_rm_RISCV = newVRegI(env);
+   addInstr(env, RISCV64Instr_SRL(fcsr_rm_RISCV, t3, t1));
+   addInstr(env,
+            RISCV64Instr_CSRRW(hregRISCV64_x0(), fcsr_rm_RISCV, 0x002 /*frm*/));
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Function call helpers                          ---*/
+/*------------------------------------------------------------*/
+
+/* Used only in doHelperCall(). See the big comment in doHelperCall() regarding
+   handling of register-parameter arguments. This function figures out whether
+   evaluation of an expression might require use of a fixed register. If in
+   doubt return True (safe but suboptimal).
+*/
+static Bool mightRequireFixedRegs(IRExpr* e)
+{
+   if (UNLIKELY(is_IRExpr_VECRET_or_GSPTR(e))) {
+      /* These are always "safe" -- either a copy of x2/sp in some arbitrary
+         vreg, or a copy of x8/s0, respectively. */
+      return False;
+   }
+   /* Else it's a "normal" expression. */
+   switch (e->tag) {
+   case Iex_RdTmp:
+   case Iex_Const:
+   case Iex_Get:
+      return False;
+   default:
+      return True;
+   }
+}
+
+/* Do a complete function call. |guard| is a Ity_Bit expression indicating
+   whether or not the call happens. If guard==NULL, the call is unconditional.
+   |retloc| is set to indicate where the return value is after the call. The
+   caller (of this fn) must generate code to add |stackAdjustAfterCall| to the
+   stack pointer after the call is done. Returns True iff it managed to handle
+   this combination of arg/return types, else returns False. */
+static Bool doHelperCall(/*OUT*/ UInt*   stackAdjustAfterCall,
+                         /*OUT*/ RetLoc* retloc,
+                         ISelEnv*        env,
+                         IRExpr*         guard,
+                         IRCallee*       cee,
+                         IRType          retTy,
+                         IRExpr**        args)
+{
+   /* Set default returns. We'll update them later if needed. */
+   *stackAdjustAfterCall = 0;
+   *retloc               = mk_RetLoc_INVALID();
+
+   /* Marshal args for a call and do the call.
+
+      This function only deals with a limited set of possibilities, which cover
+      all helpers in practice. The restrictions are that only the following
+      arguments are supported:
+      * RISCV64_N_REGPARMS x Ity_I32/Ity_I64 values, passed in x10/a0 .. x17/a7,
+      * RISCV64_N_FREGPARMS x Ity_F32/Ity_F64 values, passed in f10/fa0 ..
+        f17/fa7.
+
+      Note that the cee->regparms field is meaningless on riscv64 hosts (since
+      we only implement one calling convention) and so we always ignore it.
+
+      The return type can be I{8,16,32,64} or V128. In the V128 case, it is
+      expected that |args| will contain the special node IRExpr_VECRET(), in
+      which case this routine generates code to allocate space on the stack for
+      the vector return value.  Since we are not passing any scalars on the
+      stack, it is enough to preallocate the return space before marshalling any
+      arguments, in this case.
+
+      |args| may also contain IRExpr_GSPTR(), in which case the value in the
+      guest state pointer register minus BASEBLOCK_OFFSET_ADJUSTMENT is passed
+      as the corresponding argument.
+
+      Generating code which is both efficient and correct when parameters are to
+      be passed in registers is difficult, for the reasons elaborated in detail
+      in comments attached to doHelperCall() in VEX/priv/host_x86_isel.c. Here,
+      we use a variant of the method described in those comments.
+
+      The problem is split into two cases: the fast scheme and the slow scheme.
+      In the fast scheme, arguments are computed directly into the target (real)
+      registers. This is only safe when we can be sure that computation of each
+      argument will not trash any real registers set by computation of any other
+      argument.
+
+      In the slow scheme, all args are first computed into vregs, and once they
+      are all done, they are moved to the relevant real regs. This always gives
+      correct code, but it also gives a bunch of vreg-to-rreg moves which are
+      usually redundant but are hard for the register allocator to get rid of.
+
+      To decide which scheme to use, all argument expressions are first
+      examined. If they are all so simple that it is clear they will be
+      evaluated without use of any fixed registers, use the fast scheme, else
+      use the slow scheme. Note also that only unconditional calls may use the
+      fast scheme, since having to compute a condition expression could itself
+      trash real registers.
+
+      Note this requires being able to examine an expression and determine
+      whether or not evaluation of it might use a fixed register. That requires
+      knowledge of how the rest of this insn selector works. Currently just the
+      following 3 are regarded as safe -- hopefully they cover the majority of
+      arguments in practice: IRExpr_RdTmp, IRExpr_Const, IRExpr_Get.
+   */
+
+   /* These are used for cross-checking that IR-level constraints on the use of
+      IRExpr_VECRET() and IRExpr_GSPTR() are observed. */
+   UInt nVECRETs = 0;
+   UInt nGSPTRs  = 0;
+
+   UInt n_args = 0;
+   for (UInt i = 0; args[i] != NULL; i++) {
+      IRExpr* arg = args[i];
+      if (UNLIKELY(arg->tag == Iex_VECRET))
+         nVECRETs++;
+      else if (UNLIKELY(arg->tag == Iex_GSPTR))
+         nGSPTRs++;
+      n_args++;
+   }
+
+   /* If this fails, the IR is ill-formed. */
+   vassert(nGSPTRs == 0 || nGSPTRs == 1);
+
+   /* If we have a VECRET, allocate space on the stack for the return value, and
+      record the stack pointer after that. */
+   HReg r_vecRetAddr = INVALID_HREG;
+   if (nVECRETs == 1) {
+      vassert(retTy == Ity_V128 || retTy == Ity_V256);
+      r_vecRetAddr = newVRegI(env);
+      addInstr(env, RISCV64Instr_ADDI(hregRISCV64_x2(), hregRISCV64_x2(),
+                                      retTy == Ity_V128 ? -16 : -32));
+      addInstr(env, RISCV64Instr_MV(r_vecRetAddr, hregRISCV64_x2()));
+   } else {
+      /* If either of these fail, the IR is ill-formed. */
+      vassert(retTy != Ity_V128 && retTy != Ity_V256);
+      vassert(nVECRETs == 0);
+   }
+
+   /* First decide which scheme (slow or fast) is to be used. First assume the
+      fast scheme, and select slow if any contraindications (wow) appear. */
+   Bool go_fast = True;
+
+   /* We'll need space on the stack for the return value. Avoid possible
+      complications with nested calls by using the slow scheme. */
+   if (retTy == Ity_V128 || retTy == Ity_V256)
+      go_fast = False;
+
+   if (go_fast && guard != NULL) {
+      if (guard->tag == Iex_Const && guard->Iex.Const.con->tag == Ico_U1 &&
+          guard->Iex.Const.con->Ico.U1 == True) {
+         /* Unconditional. */
+      } else {
+         /* Not manifestly unconditional -- be conservative. */
+         go_fast = False;
+      }
+   }
+
+   if (go_fast)
+      for (UInt i = 0; i < n_args; i++) {
+         if (mightRequireFixedRegs(args[i])) {
+            go_fast = False;
+            break;
+         }
+      }
+
+   /* At this point the scheme to use has been established. Generate code to get
+      the arg values into the argument regs. If we run out of arg regs, give up.
+    */
+
+   HReg argregs[RISCV64_N_ARGREGS];
+   HReg fargregs[RISCV64_N_FARGREGS];
+
+   vassert(RISCV64_N_ARGREGS == 8);
+   vassert(RISCV64_N_FARGREGS == 8);
+
+   argregs[0] = hregRISCV64_x10();
+   argregs[1] = hregRISCV64_x11();
+   argregs[2] = hregRISCV64_x12();
+   argregs[3] = hregRISCV64_x13();
+   argregs[4] = hregRISCV64_x14();
+   argregs[5] = hregRISCV64_x15();
+   argregs[6] = hregRISCV64_x16();
+   argregs[7] = hregRISCV64_x17();
+
+   fargregs[0] = hregRISCV64_f10();
+   fargregs[1] = hregRISCV64_f11();
+   fargregs[2] = hregRISCV64_f12();
+   fargregs[3] = hregRISCV64_f13();
+   fargregs[4] = hregRISCV64_f14();
+   fargregs[5] = hregRISCV64_f15();
+   fargregs[6] = hregRISCV64_f16();
+   fargregs[7] = hregRISCV64_f17();
+
+   HReg tmpregs[RISCV64_N_ARGREGS];
+   HReg ftmpregs[RISCV64_N_FARGREGS];
+   Int  nextArgReg = 0, nextFArgReg = 0;
+   HReg cond;
+
+   if (go_fast) {
+      /* FAST SCHEME */
+      for (UInt i = 0; i < n_args; i++) {
+         IRExpr* arg = args[i];
+
+         IRType aTy = Ity_INVALID;
+         if (LIKELY(!is_IRExpr_VECRET_or_GSPTR(arg)))
+            aTy = typeOfIRExpr(env->type_env, args[i]);
+
+         if (aTy == Ity_I32 || aTy == Ity_I64) {
+            if (nextArgReg >= RISCV64_N_ARGREGS)
+               return False; /* Out of argregs. */
+            addInstr(env, RISCV64Instr_MV(argregs[nextArgReg],
+                                          iselIntExpr_R(env, args[i])));
+            nextArgReg++;
+         } else if (aTy == Ity_F32 || aTy == Ity_F64) {
+            if (nextFArgReg >= RISCV64_N_FARGREGS)
+               return False; /* Out of fargregs. */
+            addInstr(env, RISCV64Instr_FMV_D(fargregs[nextFArgReg],
+                                             iselFltExpr(env, args[i])));
+            nextFArgReg++;
+         } else if (arg->tag == Iex_GSPTR) {
+            if (nextArgReg >= RISCV64_N_ARGREGS)
+               return False; /* Out of argregs. */
+            addInstr(env,
+                     RISCV64Instr_MV(argregs[nextArgReg], hregRISCV64_x8()));
+            nextArgReg++;
+         } else if (arg->tag == Iex_VECRET) {
+            /* Because of the go_fast logic above, we can't get here, since
+               vector return values make us use the slow path instead. */
+            vassert(0);
+         } else
+            return False; /* Unhandled arg type. */
+      }
+
+      /* Fast scheme only applies for unconditional calls. Hence: */
+      cond = INVALID_HREG;
+
+   } else {
+      /* SLOW SCHEME; move via temporaries. */
+      for (UInt i = 0; i < n_args; i++) {
+         IRExpr* arg = args[i];
+
+         IRType aTy = Ity_INVALID;
+         if (LIKELY(!is_IRExpr_VECRET_or_GSPTR(arg)))
+            aTy = typeOfIRExpr(env->type_env, args[i]);
+
+         if (aTy == Ity_I32 || aTy == Ity_I64) {
+            if (nextArgReg >= RISCV64_N_ARGREGS)
+               return False; /* Out of argregs. */
+            tmpregs[nextArgReg] = iselIntExpr_R(env, args[i]);
+            nextArgReg++;
+         } else if (aTy == Ity_F32 || aTy == Ity_F64) {
+            if (nextFArgReg >= RISCV64_N_FARGREGS)
+               return False; /* Out of fargregs. */
+            ftmpregs[nextFArgReg] = iselFltExpr(env, args[i]);
+            nextFArgReg++;
+         } else if (arg->tag == Iex_GSPTR) {
+            if (nextArgReg >= RISCV64_N_ARGREGS)
+               return False; /* Out of argregs. */
+            tmpregs[nextArgReg] = hregRISCV64_x8();
+            nextArgReg++;
+         } else if (arg->tag == Iex_VECRET) {
+            vassert(!hregIsInvalid(r_vecRetAddr));
+            tmpregs[nextArgReg] = r_vecRetAddr;
+            nextArgReg++;
+         } else
+            return False; /* Unhandled arg type. */
+      }
+
+      /* Compute the condition. Be a bit clever to handle the common case where
+         the guard is 1:Bit. */
+      cond = INVALID_HREG;
+      if (guard) {
+         if (guard->tag == Iex_Const && guard->Iex.Const.con->tag == Ico_U1 &&
+             guard->Iex.Const.con->Ico.U1 == True) {
+            /* Unconditional -- do nothing. */
+         } else {
+            cond = iselIntExpr_R(env, guard);
+         }
+      }
+
+      /* Move the args to their final destinations. */
+      for (UInt i = 0; i < nextArgReg; i++) {
+         vassert(!(hregIsInvalid(tmpregs[i])));
+         addInstr(env, RISCV64Instr_MV(argregs[i], tmpregs[i]));
+      }
+      for (UInt i = 0; i < nextFArgReg; i++) {
+         vassert(!(hregIsInvalid(ftmpregs[i])));
+         addInstr(env, RISCV64Instr_FMV_D(fargregs[i], ftmpregs[i]));
+      }
+   }
+
+   /* Should be assured by checks above. */
+   vassert(nextArgReg <= RISCV64_N_ARGREGS);
+   vassert(nextFArgReg <= RISCV64_N_FARGREGS);
+
+   /* Do final checks, set the return values, and generate the call instruction
+      proper. */
+   vassert(nGSPTRs == 0 || nGSPTRs == 1);
+   vassert(nVECRETs == ((retTy == Ity_V128 || retTy == Ity_V256) ? 1 : 0));
+   vassert(*stackAdjustAfterCall == 0);
+   vassert(is_RetLoc_INVALID(*retloc));
+   switch (retTy) {
+   case Ity_INVALID:
+      /* Function doesn't return a value. */
+      *retloc = mk_RetLoc_simple(RLPri_None);
+      break;
+   case Ity_I8:
+   case Ity_I16:
+   case Ity_I32:
+   case Ity_I64:
+      *retloc = mk_RetLoc_simple(RLPri_Int);
+      break;
+   case Ity_V128:
+      *retloc               = mk_RetLoc_spRel(RLPri_V128SpRel, 0);
+      *stackAdjustAfterCall = 16;
+      break;
+   case Ity_V256:
+      *retloc               = mk_RetLoc_spRel(RLPri_V256SpRel, 0);
+      *stackAdjustAfterCall = 32;
+      break;
+   default:
+      /* IR can denote other possible return types, but we don't handle those
+         here. */
+      return False;
+   }
+
+   /* Finally, generate the call itself. This needs the *retloc value set in the
+      switch above, which is why it's at the end. */
+
+   /* nextArgReg doles out argument registers. Since these are assigned in the
+      order x10/a0 .. x17/a7, its numeric value at this point, which must be
+      between 0 and 8 inclusive, is going to be equal to the number of arg regs
+      in use for the call. Hence bake that number into the call (we'll need to
+      know it when doing register allocation, to know what regs the call reads.)
+
+      The same applies to nextFArgReg which records a number of used
+      floating-point registers f10/fa0 .. f17/fa7.
+    */
+   addInstr(env, RISCV64Instr_Call(*retloc, (Addr64)cee->addr, cond, nextArgReg,
+                                   nextFArgReg));
+
+   return True;
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Integer expressions (64/32/16/8/1 bit)         ---*/
+/*------------------------------------------------------------*/
+
+/* Select insns for an integer-typed expression, and add them to the code list.
+   Return a reg holding the result. This reg will be a virtual register. THE
+   RETURNED REG MUST NOT BE MODIFIED. If you want to modify it, ask for a new
+   vreg, copy it in there, and modify the copy. The register allocator will do
+   its best to map both vregs to the same real register, so the copies will
+   often disappear later in the game.
+
+   This should handle expressions of 64, 32, 16, 8 and 1-bit type. All results
+   are returned in a 64-bit register. For an N-bit expression, the upper 64-N
+   bits are arbitrary, so you should mask or sign-extend partial values if
+   necessary.
+
+   The riscv64 backend however internally always extends the values as follows:
+   * a 32/16/8-bit integer result is sign-extended to 64 bits,
+   * a 1-bit logical result is zero-extended to 64 bits.
+
+   This schema follows the approach taken by the RV64 ISA which by default
+   sign-extends any 32/16/8-bit operation result to 64 bits. Matching the isel
+   with the ISA generally results in requiring less instructions. For instance,
+   it allows that any Ico_U32 immediate can be always materialized at maximum
+   using two instructions (LUI+ADDIW).
+
+   An important consequence of this design is that any Iop_<N>Sto64 extension is
+   a no-op. On the other hand, any Iop_64to<N> operation must additionally
+   perform an N-bit sign-extension. This is the opposite situation than in most
+   other VEX backends.
+*/
+
+/* -------------------------- Reg --------------------------- */
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static HReg iselIntExpr_R_wrk(ISelEnv* env, IRExpr* e)
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(ty == Ity_I64 || ty == Ity_I32 || ty == Ity_I16 || ty == Ity_I8 ||
+           ty == Ity_I1);
+
+   switch (e->tag) {
+   /* ------------------------ TEMP ------------------------- */
+   case Iex_RdTmp: {
+      return lookupIRTemp(env, e->Iex.RdTmp.tmp);
+   }
+
+   /* ------------------------ LOAD ------------------------- */
+   case Iex_Load: {
+      if (e->Iex.Load.end != Iend_LE)
+         goto irreducible;
+
+      HReg dst = newVRegI(env);
+      /* TODO Optimize the cases with small imm Add64/Sub64. */
+      HReg addr = iselIntExpr_R(env, e->Iex.Load.addr);
+
+      if (ty == Ity_I64)
+         addInstr(env, RISCV64Instr_LD(dst, addr, 0));
+      else if (ty == Ity_I32)
+         addInstr(env, RISCV64Instr_LW(dst, addr, 0));
+      else if (ty == Ity_I16)
+         addInstr(env, RISCV64Instr_LH(dst, addr, 0));
+      else if (ty == Ity_I8)
+         addInstr(env, RISCV64Instr_LB(dst, addr, 0));
+      else
+         goto irreducible;
+      return dst;
+   }
+
+   /* ---------------------- BINARY OP ---------------------- */
+   case Iex_Binop: {
+      /* TODO Optimize for small imms by generating <instr>i. */
+      switch (e->Iex.Binop.op) {
+      case Iop_Add64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_ADD(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Add32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_ADDW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Sub64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SUB(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Sub32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SUBW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Xor64:
+      case Iop_Xor32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_XOR(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Or64:
+      case Iop_Or32:
+      case Iop_Or1: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_OR(dst, argL, argR));
+         return dst;
+      }
+      case Iop_And64:
+      case Iop_And32:
+      case Iop_And1: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_AND(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Shl64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SLL(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Shl32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SLLW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Shr64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SRL(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Shr32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SRLW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Sar64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SRA(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Sar32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SRAW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Mul64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_MUL(dst, argL, argR));
+         return dst;
+      }
+      case Iop_Mul32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_MULW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_DivU64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_DIVU(dst, argL, argR));
+         return dst;
+      }
+      case Iop_DivU32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_DIVUW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_DivS64: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_DIV(dst, argL, argR));
+         return dst;
+      }
+      case Iop_DivS32: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_DIVW(dst, argL, argR));
+         return dst;
+      }
+      case Iop_CmpEQ64:
+      case Iop_CmpEQ32:
+      case Iop_CasCmpEQ64:
+      case Iop_CasCmpEQ32: {
+         HReg tmp  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SUB(tmp, argL, argR));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLTIU(dst, tmp, 1));
+         return dst;
+      }
+      case Iop_CmpNE64:
+      case Iop_CmpNE32:
+      case Iop_CasCmpNE64:
+      case Iop_CasCmpNE32: {
+         HReg tmp  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SUB(tmp, argL, argR));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLTU(dst, hregRISCV64_x0(), tmp));
+         return dst;
+      }
+      case Iop_CmpLT64S:
+      case Iop_CmpLT32S: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SLT(dst, argL, argR));
+         return dst;
+      }
+      case Iop_CmpLE64S:
+      case Iop_CmpLE32S: {
+         HReg tmp  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SLT(tmp, argR, argL));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLTIU(dst, tmp, 1));
+         return dst;
+      }
+      case Iop_CmpLT64U:
+      case Iop_CmpLT32U: {
+         HReg dst  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SLTU(dst, argL, argR));
+         return dst;
+      }
+      case Iop_CmpLE64U:
+      case Iop_CmpLE32U: {
+         HReg tmp  = newVRegI(env);
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_SLTU(tmp, argR, argL));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLTIU(dst, tmp, 1));
+         return dst;
+      }
+      case Iop_Max32U: {
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         HReg cond = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLTU(cond, argL, argR));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_CSEL(dst, argR, argL, cond));
+         return dst;
+      }
+      case Iop_32HLto64: {
+         HReg hi32s = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg lo32s = iselIntExpr_R(env, e->Iex.Binop.arg2);
+
+         HReg lo32_tmp = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(lo32_tmp, lo32s, 32));
+         HReg lo32 = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRLI(lo32, lo32_tmp, 32));
+
+         HReg hi32 = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(hi32, hi32s, 32));
+
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_OR(dst, hi32, lo32));
+         return dst;
+      }
+      case Iop_DivModS32to32: {
+         /* TODO Improve in conjunction with Iop_64HIto32. */
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+
+         HReg remw = newVRegI(env);
+         addInstr(env, RISCV64Instr_REMW(remw, argL, argR));
+         HReg remw_hi = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(remw_hi, remw, 32));
+
+         HReg divw = newVRegI(env);
+         addInstr(env, RISCV64Instr_DIVW(divw, argL, argR));
+         HReg divw_hi = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(divw_hi, divw, 32));
+         HReg divw_lo = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRLI(divw_lo, divw_hi, 32));
+
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_OR(dst, remw_hi, divw_lo));
+         return dst;
+      }
+      case Iop_DivModU32to32: {
+         /* TODO Improve in conjunction with Iop_64HIto32. */
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+
+         HReg remuw = newVRegI(env);
+         addInstr(env, RISCV64Instr_REMUW(remuw, argL, argR));
+         HReg remuw_hi = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(remuw_hi, remuw, 32));
+
+         HReg divuw = newVRegI(env);
+         addInstr(env, RISCV64Instr_DIVUW(divuw, argL, argR));
+         HReg divuw_hi = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(divuw_hi, divuw, 32));
+         HReg divuw_lo = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRLI(divuw_lo, divuw_hi, 32));
+
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_OR(dst, remuw_hi, divuw_lo));
+         return dst;
+      }
+      case Iop_CmpF64: {
+         HReg argL = iselFltExpr(env, e->Iex.Binop.arg1);
+         HReg argR = iselFltExpr(env, e->Iex.Binop.arg2);
+
+         HReg lt = newVRegI(env);
+         addInstr(env, RISCV64Instr_FLT_D(lt, argL, argR));
+         HReg gt = newVRegI(env);
+         addInstr(env, RISCV64Instr_FLT_D(gt, argR, argL));
+         HReg eq = newVRegI(env);
+         addInstr(env, RISCV64Instr_FEQ_D(eq, argL, argR));
+
+         /*
+            t0 = Ircr_UN
+            t1 = Ircr_LT
+            t2 = csel t1, t0, lt
+            t3 = Ircr_GT
+            t4 = csel t3, t2, gt
+            t5 = Ircr_EQ
+            dst = csel t5, t4, eq
+         */
+         HReg t0 = newVRegI(env);
+         addInstr(env, RISCV64Instr_LI(t0, Ircr_UN));
+         HReg t1 = newVRegI(env);
+         addInstr(env, RISCV64Instr_LI(t1, Ircr_LT));
+         HReg t2 = newVRegI(env);
+         addInstr(env, RISCV64Instr_CSEL(t2, t1, t0, lt));
+         HReg t3 = newVRegI(env);
+         addInstr(env, RISCV64Instr_LI(t3, Ircr_GT));
+         HReg t4 = newVRegI(env);
+         addInstr(env, RISCV64Instr_CSEL(t4, t3, t2, gt));
+         HReg t5 = newVRegI(env);
+         addInstr(env, RISCV64Instr_LI(t5, Ircr_EQ));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_CSEL(dst, t5, t4, eq));
+         return dst;
+      }
+      case Iop_F64toI32S: {
+         HReg dst = newVRegI(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_W_D(dst, src));
+         return dst;
+      }
+      case Iop_F64toI32U: {
+         HReg dst = newVRegI(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_WU_D(dst, src));
+         return dst;
+      }
+      case Iop_F64toI64S: {
+         HReg dst = newVRegI(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_L_D(dst, src));
+         return dst;
+      }
+      case Iop_F64toI64U: {
+         HReg dst = newVRegI(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_LU_D(dst, src));
+         return dst;
+      }
+      default:
+         break;
+      }
+
+      break;
+   }
+
+   /* ---------------------- UNARY OP ----------------------- */
+   case Iex_Unop: {
+      switch (e->Iex.Unop.op) {
+      case Iop_Not64:
+      case Iop_Not32: {
+         HReg dst = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_XORI(dst, src, -1));
+         return dst;
+      }
+      case Iop_Not1: {
+         HReg dst = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SLTIU(dst, src, 1));
+         return dst;
+      }
+      case Iop_8Uto32:
+      case Iop_8Uto64:
+      case Iop_16Uto64:
+      case Iop_32Uto64: {
+         UInt shift =
+            64 - 8 * sizeofIRType(typeOfIRExpr(env->type_env, e->Iex.Unop.arg));
+         HReg tmp = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SLLI(tmp, src, shift));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRLI(dst, tmp, shift));
+         return dst;
+      }
+      case Iop_1Sto32:
+      case Iop_1Sto64: {
+         HReg tmp = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SLLI(tmp, src, 63));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRAI(dst, tmp, 63));
+         return dst;
+      }
+      case Iop_1Uto64:
+      case Iop_8Sto64:
+      case Iop_16Sto64:
+      case Iop_32Sto64:
+         /* These are no-ops. */
+         return iselIntExpr_R(env, e->Iex.Unop.arg);
+      case Iop_32to8:
+      case Iop_32to16:
+      case Iop_64to8:
+      case Iop_64to16:
+      case Iop_64to32: {
+         UInt shift = 64 - 8 * sizeofIRType(ty);
+         HReg tmp   = newVRegI(env);
+         HReg src   = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SLLI(tmp, src, shift));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRAI(dst, tmp, shift));
+         return dst;
+      }
+      case Iop_128HIto64: {
+         HReg rHi, rLo;
+         iselInt128Expr(&rHi, &rLo, env, e->Iex.Unop.arg);
+         return rHi; /* and abandon rLo */
+      }
+      case Iop_64HIto32: {
+         HReg dst = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SRAI(dst, src, 32));
+         return dst;
+      }
+      case Iop_ReinterpF32asI32: {
+         HReg dst = newVRegI(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FMV_X_W(dst, src));
+         return dst;
+      }
+      case Iop_ReinterpF64asI64: {
+         HReg dst = newVRegI(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FMV_X_D(dst, src));
+         return dst;
+      }
+      case Iop_CmpNEZ8:
+      case Iop_CmpNEZ32:
+      case Iop_CmpNEZ64: {
+         HReg dst = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SLTU(dst, hregRISCV64_x0(), src));
+         return dst;
+      }
+      case Iop_CmpwNEZ32:
+      case Iop_CmpwNEZ64: {
+         /* Use the fact that x | -x == 0 iff x == 0. Otherwise, either X or -X
+            will have a 1 in the MSB. */
+         HReg neg = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SUB(neg, hregRISCV64_x0(), src));
+         HReg or = newVRegI(env);
+         addInstr(env, RISCV64Instr_OR(or, src, neg));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_SRAI(dst, or, 63));
+         return dst;
+      }
+      case Iop_Left32:
+      case Iop_Left64: {
+         /* Left32/64(src) = src | -src. */
+         HReg neg = newVRegI(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_SUB(neg, hregRISCV64_x0(), src));
+         HReg dst = newVRegI(env);
+         addInstr(env, RISCV64Instr_OR(dst, src, neg));
+         return dst;
+      }
+      default:
+         break;
+      }
+
+      break;
+   }
+
+   /* ------------------------- GET ------------------------- */
+   case Iex_Get: {
+      HReg dst  = newVRegI(env);
+      HReg base = get_baseblock_register();
+      Int  off  = e->Iex.Get.offset - BASEBLOCK_OFFSET_ADJUSTMENT;
+      vassert(off >= -2048 && off < 2048);
+
+      if (ty == Ity_I64)
+         addInstr(env, RISCV64Instr_LD(dst, base, off));
+      else if (ty == Ity_I32)
+         addInstr(env, RISCV64Instr_LW(dst, base, off));
+      else if (ty == Ity_I16)
+         addInstr(env, RISCV64Instr_LH(dst, base, off));
+      else if (ty == Ity_I8)
+         addInstr(env, RISCV64Instr_LB(dst, base, off));
+      else
+         goto irreducible;
+      return dst;
+   }
+
+   /* ------------------------ CCALL ------------------------ */
+   case Iex_CCall: {
+      vassert(ty == e->Iex.CCall.retty);
+
+      /* Be very restrictive for now. Only 32 and 64-bit ints are allowed for
+         the return type. */
+      if (e->Iex.CCall.retty != Ity_I32 && e->Iex.CCall.retty != Ity_I64)
+         goto irreducible;
+
+      /* Marshal args and do the call. */
+      UInt   addToSp = 0;
+      RetLoc rloc    = mk_RetLoc_INVALID();
+      Bool   ok =
+         doHelperCall(&addToSp, &rloc, env, NULL /*guard*/, e->Iex.CCall.cee,
+                      e->Iex.CCall.retty, e->Iex.CCall.args);
+      if (!ok)
+         goto irreducible;
+      vassert(is_sane_RetLoc(rloc));
+      vassert(rloc.pri == RLPri_Int);
+      vassert(addToSp == 0);
+
+      HReg dst = newVRegI(env);
+      switch (e->Iex.CCall.retty) {
+      case Ity_I32:
+         /* Sign-extend the value returned from the helper as is expected by the
+            rest of the backend. */
+         addInstr(env, RISCV64Instr_ADDIW(dst, hregRISCV64_x10(), 0));
+         break;
+      case Ity_I64:
+         addInstr(env, RISCV64Instr_MV(dst, hregRISCV64_x10()));
+         break;
+      default:
+         vassert(0);
+      }
+      return dst;
+   }
+
+   /* ----------------------- LITERAL ----------------------- */
+   /* 64/32/16/8-bit literals. */
+   case Iex_Const: {
+      ULong u;
+      HReg  dst = newVRegI(env);
+      switch (e->Iex.Const.con->tag) {
+      case Ico_U64:
+         u = e->Iex.Const.con->Ico.U64;
+         break;
+      case Ico_U32:
+         vassert(ty == Ity_I32);
+         u = vex_sx_to_64(e->Iex.Const.con->Ico.U32, 32);
+         break;
+      case Ico_U16:
+         vassert(ty == Ity_I16);
+         u = vex_sx_to_64(e->Iex.Const.con->Ico.U16, 16);
+         break;
+      case Ico_U8:
+         vassert(ty == Ity_I8);
+         u = vex_sx_to_64(e->Iex.Const.con->Ico.U8, 8);
+         break;
+      default:
+         goto irreducible;
+      }
+      addInstr(env, RISCV64Instr_LI(dst, u));
+      return dst;
+   }
+
+   /* ---------------------- MULTIPLEX ---------------------- */
+   case Iex_ITE: {
+      /* ITE(ccexpr, iftrue, iffalse) */
+      if (ty == Ity_I64 || ty == Ity_I32) {
+         HReg dst     = newVRegI(env);
+         HReg iftrue  = iselIntExpr_R(env, e->Iex.ITE.iftrue);
+         HReg iffalse = iselIntExpr_R(env, e->Iex.ITE.iffalse);
+         HReg cond    = iselIntExpr_R(env, e->Iex.ITE.cond);
+         addInstr(env, RISCV64Instr_CSEL(dst, iftrue, iffalse, cond));
+         return dst;
+      }
+      break;
+   }
+
+   default:
+      break;
+   }
+
+   /* We get here if no pattern matched. */
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselIntExpr_R(riscv64)");
+}
+
+static HReg iselIntExpr_R(ISelEnv* env, IRExpr* e)
+{
+   HReg r = iselIntExpr_R_wrk(env, e);
+
+   /* Sanity checks ... */
+   vassert(hregClass(r) == HRcInt64);
+   vassert(hregIsVirtual(r));
+
+   return r;
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Integer expressions (128 bit)                  ---*/
+/*------------------------------------------------------------*/
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static void iselInt128Expr_wrk(HReg* rHi, HReg* rLo, ISelEnv* env, IRExpr* e)
+{
+   vassert(typeOfIRExpr(env->type_env, e) == Ity_I128);
+
+   /* ---------------------- BINARY OP ---------------------- */
+   if (e->tag == Iex_Binop) {
+      switch (e->Iex.Binop.op) {
+      /* 64 x 64 -> 128 multiply */
+      case Iop_MullS64:
+      case Iop_MullU64: {
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         *rHi      = newVRegI(env);
+         *rLo      = newVRegI(env);
+         if (e->Iex.Binop.op == Iop_MullS64)
+            addInstr(env, RISCV64Instr_MULH(*rHi, argL, argR));
+         else
+            addInstr(env, RISCV64Instr_MULHU(*rHi, argL, argR));
+         addInstr(env, RISCV64Instr_MUL(*rLo, argL, argR));
+         return;
+      }
+
+      /* 64 x 64 -> (64(rem),64(div)) division */
+      case Iop_DivModS64to64:
+      case Iop_DivModU64to64: {
+         HReg argL = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         HReg argR = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         *rHi      = newVRegI(env);
+         *rLo      = newVRegI(env);
+         if (e->Iex.Binop.op == Iop_DivModS64to64) {
+            addInstr(env, RISCV64Instr_REM(*rHi, argL, argR));
+            addInstr(env, RISCV64Instr_DIV(*rLo, argL, argR));
+         } else {
+            addInstr(env, RISCV64Instr_REMU(*rHi, argL, argR));
+            addInstr(env, RISCV64Instr_DIVU(*rLo, argL, argR));
+         }
+         return;
+      }
+
+      /* 64HLto128(e1,e2) */
+      case Iop_64HLto128:
+         *rHi = iselIntExpr_R(env, e->Iex.Binop.arg1);
+         *rLo = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         return;
+
+      default:
+         break;
+      }
+   }
+
+   ppIRExpr(e);
+   vpanic("iselInt128Expr(riscv64)");
+}
+
+/* Compute a 128-bit value into a register pair, which is returned as the first
+   two parameters. As with iselIntExpr_R, these will be virtual registers and
+   they must not be changed by subsequent code emitted by the caller. */
+static void iselInt128Expr(HReg* rHi, HReg* rLo, ISelEnv* env, IRExpr* e)
+{
+   iselInt128Expr_wrk(rHi, rLo, env, e);
+
+   /* Sanity checks ... */
+   vassert(hregClass(*rHi) == HRcInt64);
+   vassert(hregIsVirtual(*rHi));
+   vassert(hregClass(*rLo) == HRcInt64);
+   vassert(hregIsVirtual(*rLo));
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Floating point expressions                     ---*/
+/*------------------------------------------------------------*/
+
+/* DO NOT CALL THIS DIRECTLY ! */
+static HReg iselFltExpr_wrk(ISelEnv* env, IRExpr* e)
+{
+   IRType ty = typeOfIRExpr(env->type_env, e);
+   vassert(ty == Ity_F32 || ty == Ity_F64);
+
+   switch (e->tag) {
+   /* ------------------------ TEMP ------------------------- */
+   case Iex_RdTmp: {
+      return lookupIRTemp(env, e->Iex.RdTmp.tmp);
+   }
+
+   /* ------------------------ LOAD ------------------------- */
+   case Iex_Load: {
+      if (e->Iex.Load.end != Iend_LE)
+         goto irreducible;
+
+      HReg dst = newVRegF(env);
+      /* TODO Optimize the cases with small imm Add64/Sub64. */
+      HReg addr = iselIntExpr_R(env, e->Iex.Load.addr);
+
+      if (ty == Ity_F32)
+         addInstr(env, RISCV64Instr_FLW(dst, addr, 0));
+      else if (ty == Ity_F64)
+         addInstr(env, RISCV64Instr_FLD(dst, addr, 0));
+      else
+         vassert(0);
+      return dst;
+   }
+
+   /* -------------------- QUATERNARY OP -------------------- */
+   case Iex_Qop: {
+      switch (e->Iex.Qop.details->op) {
+      case Iop_MAddF32: {
+         HReg dst  = newVRegF(env);
+         HReg argN = iselFltExpr(env, e->Iex.Qop.details->arg2);
+         HReg argM = iselFltExpr(env, e->Iex.Qop.details->arg3);
+         HReg argA = iselFltExpr(env, e->Iex.Qop.details->arg4);
+         set_fcsr_rounding_mode(env, e->Iex.Qop.details->arg1);
+         addInstr(env, RISCV64Instr_FMADD_S(dst, argN, argM, argA));
+         return dst;
+      }
+      case Iop_MAddF64: {
+         HReg dst  = newVRegF(env);
+         HReg argN = iselFltExpr(env, e->Iex.Qop.details->arg2);
+         HReg argM = iselFltExpr(env, e->Iex.Qop.details->arg3);
+         HReg argA = iselFltExpr(env, e->Iex.Qop.details->arg4);
+         set_fcsr_rounding_mode(env, e->Iex.Qop.details->arg1);
+         addInstr(env, RISCV64Instr_FMADD_D(dst, argN, argM, argA));
+         return dst;
+      }
+      default:
+         break;
+      }
+
+      break;
+   }
+
+   /* --------------------- TERNARY OP ---------------------- */
+   case Iex_Triop: {
+      switch (e->Iex.Triop.details->op) {
+      case Iop_AddF32: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FADD_S(dst, argL, argR));
+         return dst;
+      }
+      case Iop_AddF64: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FADD_D(dst, argL, argR));
+         return dst;
+      }
+      case Iop_SubF64: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FSUB_D(dst, argL, argR));
+         return dst;
+      }
+      case Iop_MulF32: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FMUL_S(dst, argL, argR));
+         return dst;
+      }
+      case Iop_MulF64: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FMUL_D(dst, argL, argR));
+         return dst;
+      }
+      case Iop_DivF32: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FDIV_S(dst, argL, argR));
+         return dst;
+      }
+      case Iop_DivF64: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Triop.details->arg2);
+         HReg argR = iselFltExpr(env, e->Iex.Triop.details->arg3);
+         set_fcsr_rounding_mode(env, e->Iex.Triop.details->arg1);
+         addInstr(env, RISCV64Instr_FDIV_D(dst, argL, argR));
+         return dst;
+      }
+      default:
+         break;
+      }
+
+      break;
+   }
+
+   /* ---------------------- BINARY OP ---------------------- */
+   case Iex_Binop: {
+      switch (e->Iex.Binop.op) {
+      case Iop_SqrtF32: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FSQRT_S(dst, src));
+         return dst;
+      }
+      case Iop_SqrtF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FSQRT_D(dst, src));
+         return dst;
+      }
+      case Iop_I64StoF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_D_L(dst, src));
+         return dst;
+      }
+      case Iop_I64UtoF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_D_LU(dst, src));
+         return dst;
+      }
+      case Iop_F64toF32: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Binop.arg2);
+         set_fcsr_rounding_mode(env, e->Iex.Binop.arg1);
+         addInstr(env, RISCV64Instr_FCVT_S_D(dst, src));
+         return dst;
+      }
+      case Iop_MinNumF32: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Binop.arg1);
+         HReg argR = iselFltExpr(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_FMIN_S(dst, argL, argR));
+         return dst;
+      }
+      case Iop_MinNumF64: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Binop.arg1);
+         HReg argR = iselFltExpr(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_FMIN_D(dst, argL, argR));
+         return dst;
+      }
+      case Iop_MaxNumF32: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Binop.arg1);
+         HReg argR = iselFltExpr(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_FMAX_S(dst, argL, argR));
+         return dst;
+      }
+      case Iop_MaxNumF64: {
+         HReg dst  = newVRegF(env);
+         HReg argL = iselFltExpr(env, e->Iex.Binop.arg1);
+         HReg argR = iselFltExpr(env, e->Iex.Binop.arg2);
+         addInstr(env, RISCV64Instr_FMAX_D(dst, argL, argR));
+         return dst;
+      }
+      default:
+         break;
+      }
+
+      break;
+   }
+
+   /* ---------------------- UNARY OP ----------------------- */
+   case Iex_Unop: {
+      switch (e->Iex.Unop.op) {
+      case Iop_NegF32: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FSGNJN_S(dst, src, src));
+         return dst;
+      }
+      case Iop_NegF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FSGNJN_D(dst, src, src));
+         return dst;
+      }
+      case Iop_AbsF32: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FSGNJX_S(dst, src, src));
+         return dst;
+      }
+      case Iop_AbsF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FSGNJX_D(dst, src, src));
+         return dst;
+      }
+      case Iop_I32StoF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FCVT_D_W(dst, src));
+         return dst;
+      }
+      case Iop_I32UtoF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FCVT_D_WU(dst, src));
+         return dst;
+      }
+      case Iop_F32toF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselFltExpr(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FCVT_D_S(dst, src));
+         return dst;
+      }
+      case Iop_ReinterpI32asF32: {
+         HReg dst = newVRegF(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FMV_W_X(dst, src));
+         return dst;
+      }
+      case Iop_ReinterpI64asF64: {
+         HReg dst = newVRegF(env);
+         HReg src = iselIntExpr_R(env, e->Iex.Unop.arg);
+         addInstr(env, RISCV64Instr_FMV_D_X(dst, src));
+         return dst;
+      }
+      default:
+         break;
+      }
+
+      break;
+   }
+
+   /* ------------------------- GET ------------------------- */
+   case Iex_Get: {
+      HReg dst  = newVRegF(env);
+      HReg base = get_baseblock_register();
+      Int  off  = e->Iex.Get.offset - BASEBLOCK_OFFSET_ADJUSTMENT;
+      vassert(off >= -2048 && off < 2048);
+
+      if (ty == Ity_F32)
+         addInstr(env, RISCV64Instr_FLW(dst, base, off));
+      else if (ty == Ity_F64)
+         addInstr(env, RISCV64Instr_FLD(dst, base, off));
+      else
+         vassert(0);
+      return dst;
+   }
+
+   default:
+      break;
+   }
+
+irreducible:
+   ppIRExpr(e);
+   vpanic("iselFltExpr(riscv64)");
+}
+
+/* Compute a floating-point value into a register, the identity of which is
+   returned. As with iselIntExpr_R, the register will be virtual and must not be
+   changed by subsequent code emitted by the caller. */
+static HReg iselFltExpr(ISelEnv* env, IRExpr* e)
+{
+   HReg r = iselFltExpr_wrk(env, e);
+
+   /* Sanity checks ... */
+   vassert(hregClass(r) == HRcFlt64);
+   vassert(hregIsVirtual(r));
+
+   return r;
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Statements                                     ---*/
+/*------------------------------------------------------------*/
+
+static void iselStmt(ISelEnv* env, IRStmt* stmt)
+{
+   if (vex_traceflags & VEX_TRACE_VCODE) {
+      vex_printf("\n-- ");
+      ppIRStmt(stmt);
+      vex_printf("\n");
+   }
+
+   switch (stmt->tag) {
+   /* ------------------------ STORE ------------------------ */
+   /* Little-endian write to memory. */
+   case Ist_Store: {
+      IRType tyd = typeOfIRExpr(env->type_env, stmt->Ist.Store.data);
+      if (tyd == Ity_I64 || tyd == Ity_I32 || tyd == Ity_I16 || tyd == Ity_I8) {
+         HReg src = iselIntExpr_R(env, stmt->Ist.Store.data);
+         /* TODO Optimize the cases with small imm Add64/Sub64. */
+         HReg addr = iselIntExpr_R(env, stmt->Ist.Store.addr);
+
+         if (tyd == Ity_I64)
+            addInstr(env, RISCV64Instr_SD(src, addr, 0));
+         else if (tyd == Ity_I32)
+            addInstr(env, RISCV64Instr_SW(src, addr, 0));
+         else if (tyd == Ity_I16)
+            addInstr(env, RISCV64Instr_SH(src, addr, 0));
+         else if (tyd == Ity_I8)
+            addInstr(env, RISCV64Instr_SB(src, addr, 0));
+         else
+            vassert(0);
+         return;
+      }
+      if (tyd == Ity_F32 || tyd == Ity_F64) {
+         HReg src  = iselFltExpr(env, stmt->Ist.Store.data);
+         HReg addr = iselIntExpr_R(env, stmt->Ist.Store.addr);
+
+         if (tyd == Ity_F32)
+            addInstr(env, RISCV64Instr_FSW(src, addr, 0));
+         else if (tyd == Ity_F64)
+            addInstr(env, RISCV64Instr_FSD(src, addr, 0));
+         else
+            vassert(0);
+         return;
+      }
+      break;
+   }
+
+   /* ------------------------- PUT ------------------------- */
+   /* Write guest state, fixed offset. */
+   case Ist_Put: {
+      IRType tyd = typeOfIRExpr(env->type_env, stmt->Ist.Put.data);
+      if (tyd == Ity_I64 || tyd == Ity_I32 || tyd == Ity_I16 || tyd == Ity_I8) {
+         HReg src  = iselIntExpr_R(env, stmt->Ist.Put.data);
+         HReg base = get_baseblock_register();
+         Int  off  = stmt->Ist.Put.offset - BASEBLOCK_OFFSET_ADJUSTMENT;
+         vassert(off >= -2048 && off < 2048);
+
+         if (tyd == Ity_I64)
+            addInstr(env, RISCV64Instr_SD(src, base, off));
+         else if (tyd == Ity_I32)
+            addInstr(env, RISCV64Instr_SW(src, base, off));
+         else if (tyd == Ity_I16)
+            addInstr(env, RISCV64Instr_SH(src, base, off));
+         else if (tyd == Ity_I8)
+            addInstr(env, RISCV64Instr_SB(src, base, off));
+         else
+            vassert(0);
+         return;
+      }
+      if (tyd == Ity_F32 || tyd == Ity_F64) {
+         HReg src  = iselFltExpr(env, stmt->Ist.Put.data);
+         HReg base = get_baseblock_register();
+         Int  off  = stmt->Ist.Put.offset - BASEBLOCK_OFFSET_ADJUSTMENT;
+         vassert(off >= -2048 && off < 2048);
+
+         if (tyd == Ity_F32)
+            addInstr(env, RISCV64Instr_FSW(src, base, off));
+         else if (tyd == Ity_F64)
+            addInstr(env, RISCV64Instr_FSD(src, base, off));
+         else
+            vassert(0);
+         return;
+      }
+      break;
+   }
+
+   /* ------------------------- TMP ------------------------- */
+   /* Assign value to temporary. */
+   case Ist_WrTmp: {
+      IRType ty = typeOfIRTemp(env->type_env, stmt->Ist.WrTmp.tmp);
+      if (ty == Ity_I64 || ty == Ity_I32 || ty == Ity_I16 || ty == Ity_I8 ||
+          ty == Ity_I1) {
+         HReg dst = lookupIRTemp(env, stmt->Ist.WrTmp.tmp);
+         HReg src = iselIntExpr_R(env, stmt->Ist.WrTmp.data);
+         addInstr(env, RISCV64Instr_MV(dst, src));
+         return;
+      }
+      if (ty == Ity_F32 || ty == Ity_F64) {
+         HReg dst = lookupIRTemp(env, stmt->Ist.WrTmp.tmp);
+         HReg src = iselFltExpr(env, stmt->Ist.WrTmp.data);
+         addInstr(env, RISCV64Instr_FMV_D(dst, src));
+         return;
+      }
+      break;
+   }
+
+   /* ---------------- Call to DIRTY helper ----------------- */
+   /* Call complex ("dirty") helper function. */
+   case Ist_Dirty: {
+      IRDirty* d = stmt->Ist.Dirty.details;
+
+      /* Figure out the return type, if any. */
+      IRType retty = Ity_INVALID;
+      if (d->tmp != IRTemp_INVALID)
+         retty = typeOfIRTemp(env->type_env, d->tmp);
+
+      if (retty != Ity_INVALID && retty != Ity_I8 && retty != Ity_I16 &&
+          retty != Ity_I32 && retty != Ity_I64)
+         goto stmt_fail;
+
+      /* Marshal args and do the call. */
+      UInt   addToSp = 0;
+      RetLoc rloc    = mk_RetLoc_INVALID();
+      Bool   ok =
+         doHelperCall(&addToSp, &rloc, env, d->guard, d->cee, retty, d->args);
+      if (!ok)
+         goto stmt_fail;
+      vassert(is_sane_RetLoc(rloc));
+      vassert(addToSp == 0);
+
+      /* Now figure out what to do with the returned value, if any. */
+      switch (retty) {
+      case Ity_INVALID: {
+         /* No return value. Nothing to do. */
+         vassert(d->tmp == IRTemp_INVALID);
+         vassert(rloc.pri == RLPri_None);
+         return;
+      }
+      /* The returned value is for Ity_I<x> in x10/a0. Park it in the register
+         associated with tmp. */
+      case Ity_I8:
+      case Ity_I16: {
+         vassert(rloc.pri == RLPri_Int);
+         /* Sign-extend the value returned from the helper as is expected by the
+            rest of the backend. */
+         HReg dst   = lookupIRTemp(env, d->tmp);
+         UInt shift = 64 - 8 * sizeofIRType(retty);
+         HReg tmp   = newVRegI(env);
+         addInstr(env, RISCV64Instr_SLLI(tmp, hregRISCV64_x10(), shift));
+         addInstr(env, RISCV64Instr_SRAI(dst, tmp, shift));
+         return;
+      }
+      case Ity_I32: {
+         vassert(rloc.pri == RLPri_Int);
+         HReg dst = lookupIRTemp(env, d->tmp);
+         addInstr(env, RISCV64Instr_ADDIW(dst, hregRISCV64_x10(), 0));
+         return;
+      }
+      case Ity_I64: {
+         vassert(rloc.pri == RLPri_Int);
+         HReg dst = lookupIRTemp(env, d->tmp);
+         addInstr(env, RISCV64Instr_MV(dst, hregRISCV64_x10()));
+         return;
+      }
+      default:
+         vassert(0);
+      }
+      break;
+   }
+
+   /* ---------- Load Linked and Store Conditional ---------- */
+   case Ist_LLSC: {
+      if (stmt->Ist.LLSC.storedata == NULL) {
+         /* LL */
+         IRTemp res = stmt->Ist.LLSC.result;
+         IRType ty  = typeOfIRTemp(env->type_env, res);
+         if (ty == Ity_I32) {
+            HReg r_dst  = lookupIRTemp(env, res);
+            HReg r_addr = iselIntExpr_R(env, stmt->Ist.LLSC.addr);
+            addInstr(env, RISCV64Instr_LR_W(r_dst, r_addr));
+            return;
+         }
+      } else {
+         /* SC */
+         IRType tyd = typeOfIRExpr(env->type_env, stmt->Ist.LLSC.storedata);
+         if (tyd == Ity_I32) {
+            HReg r_tmp  = newVRegI(env);
+            HReg r_src  = iselIntExpr_R(env, stmt->Ist.LLSC.storedata);
+            HReg r_addr = iselIntExpr_R(env, stmt->Ist.LLSC.addr);
+            addInstr(env, RISCV64Instr_SC_W(r_tmp, r_src, r_addr));
+
+            /* Now r_tmp is non-zero if failed, 0 if success. Change to IR
+               conventions (0 is fail, 1 is success). */
+            IRTemp res   = stmt->Ist.LLSC.result;
+            HReg   r_res = lookupIRTemp(env, res);
+            IRType ty    = typeOfIRTemp(env->type_env, res);
+            vassert(ty == Ity_I1);
+            addInstr(env, RISCV64Instr_SLTIU(r_res, r_tmp, 1));
+            return;
+         }
+      }
+      break;
+   }
+
+   /* ------------------------ ACAS ------------------------- */
+   case Ist_CAS: {
+      if (stmt->Ist.CAS.details->oldHi == IRTemp_INVALID) {
+         /* "Normal" singleton CAS. */
+         IRCAS* cas = stmt->Ist.CAS.details;
+         IRType tyd = typeOfIRTemp(env->type_env, cas->oldLo);
+         if (tyd == Ity_I64 || tyd == Ity_I32) {
+            HReg old  = lookupIRTemp(env, cas->oldLo);
+            HReg addr = iselIntExpr_R(env, cas->addr);
+            HReg expd = iselIntExpr_R(env, cas->expdLo);
+            HReg data = iselIntExpr_R(env, cas->dataLo);
+            if (tyd == Ity_I64)
+               addInstr(env, RISCV64Instr_CAS_D(old, addr, expd, data));
+            else
+               addInstr(env, RISCV64Instr_CAS_W(old, addr, expd, data));
+            return;
+         }
+      }
+      break;
+   }
+
+   /* ---------------------- MEM FENCE ---------------------- */
+   case Ist_MBE:
+      switch (stmt->Ist.MBE.event) {
+      case Imbe_Fence:
+         addInstr(env, RISCV64Instr_FENCE());
+         return;
+      default:
+         break;
+      }
+      break;
+
+   /* --------------------- INSTR MARK ---------------------- */
+   /* Doesn't generate any executable code ... */
+   case Ist_IMark:
+      return;
+
+   /* ------------------------ NO-OP ------------------------ */
+   case Ist_NoOp:
+      return;
+
+   /* ------------------------ EXIT ------------------------- */
+   case Ist_Exit: {
+      if (stmt->Ist.Exit.dst->tag != Ico_U64)
+         vpanic("iselStmt(riscv64): Ist_Exit: dst is not a 64-bit value");
+
+      HReg cond   = iselIntExpr_R(env, stmt->Ist.Exit.guard);
+      HReg base   = get_baseblock_register();
+      Int  soff12 = stmt->Ist.Exit.offsIP - BASEBLOCK_OFFSET_ADJUSTMENT;
+      vassert(soff12 >= -2048 && soff12 < 2048);
+
+      /* Case: boring transfer to known address. */
+      if (stmt->Ist.Exit.jk == Ijk_Boring) {
+         if (env->chainingAllowed) {
+            /* .. almost always true .. */
+            /* Skip the event check at the dst if this is a forwards edge. */
+            Bool toFastEP = (Addr64)stmt->Ist.Exit.dst->Ico.U64 > env->max_ga;
+            if (0)
+               vex_printf("%s", toFastEP ? "Y" : ",");
+            addInstr(env, RISCV64Instr_XDirect(stmt->Ist.Exit.dst->Ico.U64,
+                                               base, soff12, cond, toFastEP));
+         } else {
+            /* .. very occasionally .. */
+            /* We can't use chaining, so ask for an assisted transfer, as
+               that's the only alternative that is allowable. */
+            HReg r = iselIntExpr_R(env, IRExpr_Const(stmt->Ist.Exit.dst));
+            addInstr(env,
+                     RISCV64Instr_XAssisted(r, base, soff12, cond, Ijk_Boring));
+         }
+         return;
+      }
+
+      /* Case: assisted transfer to arbitrary address. */
+      switch (stmt->Ist.Exit.jk) {
+      /* Keep this list in sync with that for iselNext below. */
+      case Ijk_ClientReq:
+      case Ijk_NoDecode:
+      case Ijk_NoRedir:
+      case Ijk_Sys_syscall:
+      case Ijk_InvalICache:
+      case Ijk_FlushDCache:
+      case Ijk_SigTRAP:
+      case Ijk_Yield: {
+         HReg r = iselIntExpr_R(env, IRExpr_Const(stmt->Ist.Exit.dst));
+         addInstr(env, RISCV64Instr_XAssisted(r, base, soff12, cond,
+                                              stmt->Ist.Exit.jk));
+         return;
+      }
+      default:
+         break;
+      }
+
+      /* Do we ever expect to see any other kind? */
+      goto stmt_fail;
+   }
+
+   default:
+      break;
+   }
+
+stmt_fail:
+   ppIRStmt(stmt);
+   vpanic("iselStmt");
+}
+
+/*------------------------------------------------------------*/
+/*--- ISEL: Basic block terminators (Nexts)                ---*/
+/*------------------------------------------------------------*/
+
+static void iselNext(ISelEnv* env, IRExpr* next, IRJumpKind jk, Int offsIP)
+{
+   if (vex_traceflags & VEX_TRACE_VCODE) {
+      vex_printf("\n-- PUT(%d) = ", offsIP);
+      ppIRExpr(next);
+      vex_printf("; exit-");
+      ppIRJumpKind(jk);
+      vex_printf("\n");
+   }
+
+   HReg base   = get_baseblock_register();
+   Int  soff12 = offsIP - BASEBLOCK_OFFSET_ADJUSTMENT;
+   vassert(soff12 >= -2048 && soff12 < 2048);
+
+   /* Case: boring transfer to known address. */
+   if (next->tag == Iex_Const) {
+      IRConst* cdst = next->Iex.Const.con;
+      vassert(cdst->tag == Ico_U64);
+      if (jk == Ijk_Boring || jk == Ijk_Call) {
+         /* Boring transfer to known address. */
+         if (env->chainingAllowed) {
+            /* .. almost always true .. */
+            /* Skip the event check at the dst if this is a forwards edge. */
+            Bool toFastEP = (Addr64)cdst->Ico.U64 > env->max_ga;
+            if (0)
+               vex_printf("%s", toFastEP ? "X" : ".");
+            addInstr(env, RISCV64Instr_XDirect(cdst->Ico.U64, base, soff12,
+                                               INVALID_HREG, toFastEP));
+         } else {
+            /* .. very occasionally .. */
+            /* We can't use chaining, so ask for an assisted transfer, as that's
+               the only alternative that is allowable. */
+            HReg r = iselIntExpr_R(env, next);
+            addInstr(env, RISCV64Instr_XAssisted(r, base, soff12, INVALID_HREG,
+                                                 Ijk_Boring));
+         }
+         return;
+      }
+   }
+
+   /* Case: call/return (==boring) transfer to any address. */
+   switch (jk) {
+   case Ijk_Boring:
+   case Ijk_Ret:
+   case Ijk_Call: {
+      HReg r = iselIntExpr_R(env, next);
+      if (env->chainingAllowed)
+         addInstr(env, RISCV64Instr_XIndir(r, base, soff12, INVALID_HREG));
+      else
+         addInstr(env, RISCV64Instr_XAssisted(r, base, soff12, INVALID_HREG,
+                                              Ijk_Boring));
+      return;
+   }
+   default:
+      break;
+   }
+
+   /* Case: assisted transfer to arbitrary address. */
+   switch (jk) {
+   /* Keep this list in sync with that for Ist_Exit above. */
+   case Ijk_ClientReq:
+   case Ijk_NoDecode:
+   case Ijk_NoRedir:
+   case Ijk_Sys_syscall:
+   case Ijk_InvalICache:
+   case Ijk_FlushDCache:
+   case Ijk_SigTRAP:
+   case Ijk_Yield: {
+      HReg r = iselIntExpr_R(env, next);
+      addInstr(env, RISCV64Instr_XAssisted(r, base, soff12, INVALID_HREG, jk));
+      return;
+   }
+   default:
+      break;
+   }
+
+   vex_printf("\n-- PUT(%d) = ", offsIP);
+   ppIRExpr(next);
+   vex_printf("; exit-");
+   ppIRJumpKind(jk);
+   vex_printf("\n");
+   vassert(0); /* Are we expecting any other kind? */
+}
+
+/*------------------------------------------------------------*/
+/*--- Insn selector top-level                              ---*/
+/*------------------------------------------------------------*/
+
+/* Translate an entire SB to riscv64 code. */
+
+HInstrArray* iselSB_RISCV64(const IRSB*        bb,
+                            VexArch            arch_host,
+                            const VexArchInfo* archinfo_host,
+                            const VexAbiInfo*  vbi /*UNUSED*/,
+                            Int                offs_Host_EvC_Counter,
+                            Int                offs_Host_EvC_FailAddr,
+                            Bool               chainingAllowed,
+                            Bool               addProfInc,
+                            Addr               max_ga)
+{
+   Int      i, j;
+   HReg     hreg, hregHI;
+   ISelEnv* env;
+
+   /* Do some sanity checks. */
+   vassert(arch_host == VexArchRISCV64);
+
+   /* Check that the host's endianness is as expected. */
+   vassert(archinfo_host->endness == VexEndnessLE);
+
+   /* Guard against unexpected space regressions. */
+   vassert(sizeof(RISCV64Instr) <= 32);
+
+   /* Make up an initial environment to use. */
+   env           = LibVEX_Alloc_inline(sizeof(ISelEnv));
+   env->vreg_ctr = 0;
+
+   /* Set up output code array. */
+   env->code = newHInstrArray();
+
+   /* Copy BB's type env. */
+   env->type_env = bb->tyenv;
+
+   /* Make up an IRTemp -> virtual HReg mapping. This doesn't change as we go
+      along. */
+   env->n_vregmap = bb->tyenv->types_used;
+   env->vregmap   = LibVEX_Alloc_inline(env->n_vregmap * sizeof(HReg));
+   env->vregmapHI = LibVEX_Alloc_inline(env->n_vregmap * sizeof(HReg));
+
+   /* and finally ... */
+   env->chainingAllowed = chainingAllowed;
+   env->hwcaps          = archinfo_host->hwcaps;
+   env->previous_rm     = NULL;
+   env->max_ga          = max_ga;
+
+   /* For each IR temporary, allocate a suitably-kinded virtual register. */
+   j = 0;
+   for (i = 0; i < env->n_vregmap; i++) {
+      hregHI = hreg = INVALID_HREG;
+      switch (bb->tyenv->types[i]) {
+      case Ity_I1:
+      case Ity_I8:
+      case Ity_I16:
+      case Ity_I32:
+      case Ity_I64:
+         hreg = mkHReg(True, HRcInt64, 0, j++);
+         break;
+      case Ity_I128:
+         hreg   = mkHReg(True, HRcInt64, 0, j++);
+         hregHI = mkHReg(True, HRcInt64, 0, j++);
+         break;
+      case Ity_F32:
+      case Ity_F64:
+         hreg = mkHReg(True, HRcFlt64, 0, j++);
+         break;
+      default:
+         ppIRType(bb->tyenv->types[i]);
+         vpanic("iselBB(riscv64): IRTemp type");
+      }
+      env->vregmap[i]   = hreg;
+      env->vregmapHI[i] = hregHI;
+   }
+   env->vreg_ctr = j;
+
+   /* The very first instruction must be an event check. */
+   HReg base             = get_baseblock_register();
+   Int  soff12_amCounter = offs_Host_EvC_Counter - BASEBLOCK_OFFSET_ADJUSTMENT;
+   vassert(soff12_amCounter >= -2048 && soff12_amCounter < 2048);
+   Int soff12_amFailAddr = offs_Host_EvC_FailAddr - BASEBLOCK_OFFSET_ADJUSTMENT;
+   vassert(soff12_amFailAddr >= -2048 && soff12_amFailAddr < 2048);
+   addInstr(env, RISCV64Instr_EvCheck(base, soff12_amCounter, base,
+                                      soff12_amFailAddr));
+
+   /* TODO */
+#if 0
+   /* Possibly a block counter increment (for profiling). At this point we don't
+      know the address of the counter, so just pretend it is zero. It will have
+      to be patched later, but before this translation is used, by a call to
+      LibVEX_patchProfCtr(). */
+   if (addProfInc)
+      addInstr(env, ARM64Instr_ProfInc());
+#endif
+
+   /* Ok, finally we can iterate over the statements. */
+   for (i = 0; i < bb->stmts_used; i++)
+      iselStmt(env, bb->stmts[i]);
+
+   iselNext(env, bb->next, bb->jumpkind, bb->offsIP);
+
+   /* Record the number of vregs we used. */
+   env->code->n_vregs = env->vreg_ctr;
+   return env->code;
+}
+
+/*--------------------------------------------------------------------*/
+/*--- end                                      host_riscv64_isel.c ---*/
+/*--------------------------------------------------------------------*/
--- a/VEX/priv/main_main.c
+++ b/VEX/priv/main_main.c
@@ -43,6 +43,7 @@
 #include "libvex_guest_s390x.h"
 #include "libvex_guest_mips32.h"
 #include "libvex_guest_mips64.h"
+#include "libvex_guest_riscv64.h"
 
 #include "main_globals.h"
 #include "main_util.h"
@@ -57,6 +58,7 @@
 #include "host_s390_defs.h"
 #include "host_mips_defs.h"
 #include "host_nanomips_defs.h"
+#include "host_riscv64_defs.h"
 
 #include "guest_generic_bb_to_IR.h"
 #include "guest_x86_defs.h"
@@ -67,6 +69,7 @@
 #include "guest_s390_defs.h"
 #include "guest_mips_defs.h"
 #include "guest_nanomips_defs.h"
+#include "guest_riscv64_defs.h"
 
 #include "host_generic_simd128.h"
 
@@ -163,6 +166,14 @@
 #define NANOMIPSST(f) vassert(0)
 #endif
 
+#if defined(VGA_riscv64) || defined(VEXMULTIARCH)
+#define RISCV64FN(f) f
+#define RISCV64ST(f) f
+#else
+#define RISCV64FN(f) NULL
+#define RISCV64ST(f) vassert(0)
+#endif
+
 /* This file contains the top level interface to the library. */
 
 /* --------- fwds ... --------- */
@@ -523,6 +534,22 @@
          vassert(sizeof( ((VexGuestMIPS64State*)0)->guest_NRADDR ) == 8);
          break;
 
+      case VexArchRISCV64:
+         preciseMemExnsFn
+            = RISCV64FN(guest_riscv64_state_requires_precise_mem_exns);
+         disInstrFn              = RISCV64FN(disInstr_RISCV64);
+         specHelper              = RISCV64FN(guest_riscv64_spechelper);
+         guest_layout            = RISCV64FN(&riscv64guest_layout);
+         offB_CMSTART            = offsetof(VexGuestRISCV64State,guest_CMSTART);
+         offB_CMLEN              = offsetof(VexGuestRISCV64State,guest_CMLEN);
+         offB_GUEST_IP           = offsetof(VexGuestRISCV64State,guest_pc);
+         szB_GUEST_IP            = sizeof( ((VexGuestRISCV64State*)0)->guest_pc );
+         vassert(vta->archinfo_guest.endness == VexEndnessLE);
+         vassert(0 == sizeof(VexGuestRISCV64State) % LibVEX_GUEST_STATE_ALIGN);
+         vassert(sizeof( ((VexGuestRISCV64State*)0)->guest_CMSTART ) == 8);
+         vassert(sizeof( ((VexGuestRISCV64State*)0)->guest_CMLEN   ) == 8);
+         vassert(sizeof( ((VexGuestRISCV64State*)0)->guest_NRADDR  ) == 8);
+         break;
       case VexArchNANOMIPS:
          preciseMemExnsFn
             = NANOMIPSFN(guest_mips32_state_requires_precise_mem_exns);
@@ -878,6 +905,14 @@
          offB_HOST_EvC_FAILADDR = offsetof(VexGuestMIPS32State,host_EvC_FAILADDR);
          break;
 
+      case VexArchRISCV64:
+         preciseMemExnsFn
+            = RISCV64FN(guest_riscv64_state_requires_precise_mem_exns);
+         guest_sizeB            = sizeof(VexGuestRISCV64State);
+         offB_HOST_EvC_COUNTER  = offsetof(VexGuestRISCV64State,host_EvC_COUNTER);
+         offB_HOST_EvC_FAILADDR = offsetof(VexGuestRISCV64State,host_EvC_FAILADDR);
+         break;
+
       default:
          vpanic("LibVEX_Codegen: unsupported guest insn set");
    }
@@ -1052,6 +1087,22 @@
                  || vta->archinfo_host.endness == VexEndnessBE);
          break;
 
+      case VexArchRISCV64:
+         mode64       = True;
+         rRegUniv     = RISCV64FN(getRRegUniverse_RISCV64());
+         getRegUsage
+            = CAST_TO_TYPEOF(getRegUsage) RISCV64FN(getRegUsage_RISCV64Instr);
+         mapRegs      = CAST_TO_TYPEOF(mapRegs) RISCV64FN(mapRegs_RISCV64Instr);
+         genSpill     = CAST_TO_TYPEOF(genSpill) RISCV64FN(genSpill_RISCV64);
+         genReload    = CAST_TO_TYPEOF(genReload) RISCV64FN(genReload_RISCV64);
+         genMove      = CAST_TO_TYPEOF(genMove) RISCV64FN(genMove_RISCV64);
+         ppInstr      = CAST_TO_TYPEOF(ppInstr) RISCV64FN(ppRISCV64Instr);
+         ppReg        = CAST_TO_TYPEOF(ppReg) RISCV64FN(ppHRegRISCV64);
+         iselSB       = RISCV64FN(iselSB_RISCV64);
+         emit         = CAST_TO_TYPEOF(emit) RISCV64FN(emit_RISCV64Instr);
+         vassert(vta->archinfo_host.endness == VexEndnessLE);
+         break;
+
       default:
          vpanic("LibVEX_Translate: unsupported host insn set");
    }
@@ -1297,6 +1348,11 @@
                                                  place_to_chain,
                                                  disp_cp_chain_me_EXPECTED,
                                                  place_to_jump_to));
+       case VexArchRISCV64:
+         RISCV64ST(return chainXDirect_RISCV64(endness_host,
+                                               place_to_chain,
+                                               disp_cp_chain_me_EXPECTED,
+                                               place_to_jump_to));
       default:
          vassert(0);
    }
@@ -1359,6 +1415,11 @@
                                                  place_to_unchain,
                                                  place_to_jump_to_EXPECTED,
                                                  disp_cp_chain_me));
+      case VexArchRISCV64:
+         RISCV64ST(return unchainXDirect_RISCV64(endness_host,
+                                                 place_to_unchain,
+                                                 place_to_jump_to_EXPECTED,
+                                                 disp_cp_chain_me));
       default:
          vassert(0);
    }
@@ -1387,8 +1448,10 @@
             MIPS32ST(cached = evCheckSzB_MIPS()); break;
          case VexArchMIPS64:
             MIPS64ST(cached = evCheckSzB_MIPS()); break;
-        case VexArchNANOMIPS:
+         case VexArchNANOMIPS:
             NANOMIPSST(cached = evCheckSzB_NANOMIPS()); break;
+	 case VexArchRISCV64:
+            RISCV64ST(cached = evCheckSzB_RISCV64()); break;
          default:
             vassert(0);
       }
@@ -1432,6 +1495,9 @@
       case VexArchNANOMIPS:
          NANOMIPSST(return patchProfInc_NANOMIPS(endness_host, place_to_patch,
                                                  location_of_counter));
+      case VexArchRISCV64:
+         RISCV64ST(return patchProfInc_RISCV64(endness_host, place_to_patch,
+                                               location_of_counter));
       default:
          vassert(0);
    }
@@ -1515,6 +1581,7 @@
       case VexArchMIPS32:   return "MIPS32";
       case VexArchMIPS64:   return "MIPS64";
       case VexArchNANOMIPS: return "NANOMIPS";
+      case VexArchRISCV64:  return "RISCV64";
       default:              return "VexArch???";
    }
 }
@@ -1584,6 +1651,7 @@
       case VexArchMIPS64:
       case VexArchPPC64:
       case VexArchS390X:
+      case VexArchRISCV64:
          return Ity_I64;
 
       default:
@@ -1599,7 +1667,7 @@
 /* Return a string showing the hwcaps in a nice way.  The string will
    be NULL for unrecognised hardware capabilities. */
 
-static const HChar* show_hwcaps_x86 ( UInt hwcaps ) 
+static const HChar* show_hwcaps_x86 ( UInt hwcaps )
 {
    static const HChar prefix[] = "x86";
    static const struct {
@@ -1613,7 +1681,7 @@
       { VEX_HWCAPS_X86_LZCNT,  "lzcnt"  },
    };
    /* Allocate a large enough buffer */
-   static HChar buf[sizeof prefix + 
+   static HChar buf[sizeof prefix +
                     NUM_HWCAPS * (sizeof hwcaps_list[0].name + 1) + 1]; // '\0'
    if (buf[0] != '\0') return buf;  /* already constructed */
 
@@ -1621,7 +1689,7 @@
 
    if (hwcaps == 0) {
       vex_sprintf(p, "-%s", "sse0");
-   } else {      
+   } else {
       UInt i;
       for (i = 0 ; i < NUM_HWCAPS; ++i) {
          if (hwcaps & hwcaps_list[i].hwcaps_bit)
@@ -1924,6 +1992,11 @@
    return "Unsupported baseline";
 }
 
+static const HChar* show_hwcaps_riscv64 ( UInt hwcaps )
+{
+   return "riscv64";
+}
+
 #undef NUM_HWCAPS
 
 /* Thie function must not return NULL. */
@@ -1940,6 +2013,7 @@
       case VexArchS390X:  return show_hwcaps_s390x(hwcaps);
       case VexArchMIPS32: return show_hwcaps_mips32(hwcaps);
       case VexArchMIPS64: return show_hwcaps_mips64(hwcaps);
+      case VexArchRISCV64: return show_hwcaps_riscv64(hwcaps);
       default: return NULL;
    }
 }
@@ -2207,6 +2281,11 @@
             return;
          invalid_hwcaps(arch, hwcaps, "Unsupported baseline\n");
 
+      case VexArchRISCV64:
+         if (hwcaps == 0)
+            return;
+         invalid_hwcaps(arch, hwcaps, "Cannot handle capabilities\n");
+
       default:
          vpanic("unknown architecture");
    }
--- a/VEX/priv/main_util.h
+++ b/VEX/priv/main_util.h
@@ -99,6 +99,16 @@
 extern SizeT vex_strlen ( const HChar* str );
 extern void vex_bzero ( void* s, SizeT n );
 
+/* Math ops */
+
+/* Sign extend an N-bit value up to 64 bits, by copying bit N-1 into all higher
+   positions. */
+static inline ULong vex_sx_to_64( ULong x, UInt n )
+{
+   vassert(n >= 1 && n < 64);
+   return (ULong)((Long)(x << (64 - n)) >> (64 - n));
+}
+
 
 /* Storage management: clear the area, and allocate from it. */
 
--- a/VEX/pub/libvex.h
+++ b/VEX/pub/libvex.h
@@ -60,6 +60,7 @@
       VexArchMIPS32,
       VexArchMIPS64,
       VexArchNANOMIPS,
+      VexArchRISCV64,
    }
    VexArch;
 
@@ -1018,6 +1019,16 @@
    ~~~~~
    r21 is GSP.
 
+   riscv64
+   ~~~~~~~
+   On entry, x8/s0 should point to the guest state + 2048. RISC-V has
+   load/store instructions with immediate (offset from the base
+   register) in range -2048 to 2047. The adjustment of 2048 allows
+   LibVEX to effectively use the full range. When translating
+   riscv64->riscv64, only a single instruction is then needed to
+   read/write values in the guest state (primary + 2x shadow state
+   areas) and most of the spill area.
+
    ALL GUEST ARCHITECTURES
    ~~~~~~~~~~~~~~~~~~~~~~~
    The guest state must contain two pseudo-registers, guest_CMSTART
--- a/VEX/pub/libvex_basictypes.h
+++ b/VEX/pub/libvex_basictypes.h
@@ -198,6 +198,10 @@
 #   define VEX_HOST_WORDSIZE 4
 #   define VEX_REGPARM(_n) /* */
 
+#elif defined(__riscv) && (__riscv_xlen == 64)
+#   define VEX_HOST_WORDSIZE 8
+#   define VEX_REGPARM(_n) /* */
+
 #else
 #   error "Vex: Fatal: Can't establish the host architecture"
 #endif
--- /dev/null
+++ b/VEX/pub/libvex_guest_riscv64.h
@@ -0,0 +1,148 @@
+
+/*--------------------------------------------------------------------*/
+/*--- begin                                 libvex_guest_riscv64.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef __LIBVEX_PUB_GUEST_RISCV64_H
+#define __LIBVEX_PUB_GUEST_RISCV64_H
+
+#include "libvex_basictypes.h"
+
+/*------------------------------------------------------------*/
+/*--- Vex's representation of the riscv64 CPU state.       ---*/
+/*------------------------------------------------------------*/
+
+typedef struct {
+   /*   0 */ ULong host_EvC_FAILADDR;
+   /*   8 */ UInt  host_EvC_COUNTER;
+   /*  12 */ UInt  pad0;
+   /*  16 */ ULong guest_x0;
+   /*  24 */ ULong guest_x1;
+   /*  32 */ ULong guest_x2;
+   /*  40 */ ULong guest_x3;
+   /*  48 */ ULong guest_x4;
+   /*  56 */ ULong guest_x5;
+   /*  64 */ ULong guest_x6;
+   /*  72 */ ULong guest_x7;
+   /*  80 */ ULong guest_x8;
+   /*  88 */ ULong guest_x9;
+   /*  96 */ ULong guest_x10;
+   /* 104 */ ULong guest_x11;
+   /* 112 */ ULong guest_x12;
+   /* 120 */ ULong guest_x13;
+   /* 128 */ ULong guest_x14;
+   /* 136 */ ULong guest_x15;
+   /* 144 */ ULong guest_x16;
+   /* 152 */ ULong guest_x17;
+   /* 160 */ ULong guest_x18;
+   /* 168 */ ULong guest_x19;
+   /* 176 */ ULong guest_x20;
+   /* 184 */ ULong guest_x21;
+   /* 192 */ ULong guest_x22;
+   /* 200 */ ULong guest_x23;
+   /* 208 */ ULong guest_x24;
+   /* 216 */ ULong guest_x25;
+   /* 224 */ ULong guest_x26;
+   /* 232 */ ULong guest_x27;
+   /* 240 */ ULong guest_x28;
+   /* 248 */ ULong guest_x29;
+   /* 256 */ ULong guest_x30;
+   /* 264 */ ULong guest_x31;
+   /* 272 */ ULong guest_pc;
+
+   /* Floating-point state. */
+   /* 280 */ ULong guest_f0;
+   /* 288 */ ULong guest_f1;
+   /* 296 */ ULong guest_f2;
+   /* 304 */ ULong guest_f3;
+   /* 312 */ ULong guest_f4;
+   /* 320 */ ULong guest_f5;
+   /* 328 */ ULong guest_f6;
+   /* 336 */ ULong guest_f7;
+   /* 344 */ ULong guest_f8;
+   /* 352 */ ULong guest_f9;
+   /* 360 */ ULong guest_f10;
+   /* 368 */ ULong guest_f11;
+   /* 376 */ ULong guest_f12;
+   /* 384 */ ULong guest_f13;
+   /* 392 */ ULong guest_f14;
+   /* 400 */ ULong guest_f15;
+   /* 408 */ ULong guest_f16;
+   /* 416 */ ULong guest_f17;
+   /* 424 */ ULong guest_f18;
+   /* 432 */ ULong guest_f19;
+   /* 440 */ ULong guest_f20;
+   /* 448 */ ULong guest_f21;
+   /* 456 */ ULong guest_f22;
+   /* 464 */ ULong guest_f23;
+   /* 472 */ ULong guest_f24;
+   /* 480 */ ULong guest_f25;
+   /* 488 */ ULong guest_f26;
+   /* 496 */ ULong guest_f27;
+   /* 504 */ ULong guest_f28;
+   /* 512 */ ULong guest_f29;
+   /* 520 */ ULong guest_f30;
+   /* 528 */ ULong guest_f31;
+   /* 536 */ UInt  guest_fcsr;
+
+   /* Various pseudo-regs mandated by Vex or Valgrind. */
+   /* Emulation notes. */
+   /* 540 */ UInt guest_EMNOTE;
+
+   /* For clflush/clinval: record start and length of area. */
+   /* 544 */ ULong guest_CMSTART;
+   /* 552 */ ULong guest_CMLEN;
+
+   /* Used to record the unredirected guest address at the start of a
+      translation whose start has been redirected. By reading this
+      pseudo-register shortly afterwards, the translation can find out what the
+      corresponding no-redirection address was. Note, this is only set for
+      wrap-style redirects, not for replace-style ones. */
+   /* 560 */ ULong guest_NRADDR;
+
+   /* Fallback LL/SC support. */
+   /* 568 */ ULong guest_LLSC_SIZE; /* 0==no transaction, else 4 or 8. */
+   /* 576 */ ULong guest_LLSC_ADDR; /* Address of the transaction. */
+   /* 584 */ ULong guest_LLSC_DATA; /* Original value at ADDR, sign-extended. */
+
+   /* Padding to 16 bytes. */
+   /* 592 */
+} VexGuestRISCV64State;
+
+/*------------------------------------------------------------*/
+/*--- Utility functions for riscv64 guest stuff.           ---*/
+/*------------------------------------------------------------*/
+
+/* ALL THE FOLLOWING ARE VISIBLE TO LIBRARY CLIENT */
+
+/* Initialise all guest riscv64 state. */
+void LibVEX_GuestRISCV64_initialise(/*OUT*/ VexGuestRISCV64State* vex_state);
+
+#endif /* ndef __LIBVEX_PUB_GUEST_RISCV64_H */
+
+/*--------------------------------------------------------------------*/
+/*---                                       libvex_guest_riscv64.h ---*/
+/*--------------------------------------------------------------------*/
--- a/cachegrind/cg_arch.c
+++ b/cachegrind/cg_arch.c
@@ -475,6 +475,14 @@
    *D1c = (cache_t) {  65536, 2, 64 };
    *LLc = (cache_t) { 262144, 8, 64 };
 
+#elif defined(VGA_riscv64)
+
+   /* TODO Implement. */
+   *I1c = (cache_t) {  16384, 4, 64 };
+   *D1c = (cache_t) {  16384, 4, 64 };
+   *LLc = (cache_t) { 262144, 8, 64 };
+   tl_assert(0);
+
 #else
 
 #error "Unknown arch"
--- a/cachegrind/cg_branchpred.c
+++ b/cachegrind/cg_branchpred.c
@@ -48,7 +48,7 @@
 #  define N_IADDR_LO_ZERO_BITS 2
 #elif defined(VGA_x86) || defined(VGA_amd64)
 #  define N_IADDR_LO_ZERO_BITS 0
-#elif defined(VGA_s390x) || defined(VGA_arm)
+#elif defined(VGA_s390x) || defined(VGA_arm) || defined(VGA_riscv64)
 #  define N_IADDR_LO_ZERO_BITS 1
 #else
 #  error "Unsupported architecture"
--- a/coregrind/Makefile.am
+++ b/coregrind/Makefile.am
@@ -392,6 +392,7 @@
 	m_dispatch/dispatch-nanomips-linux.S \
 	m_dispatch/dispatch-x86-freebsd.S \
 	m_dispatch/dispatch-amd64-freebsd.S \
+	m_dispatch/dispatch-riscv64-linux.S \
 	m_dispatch/dispatch-x86-darwin.S \
 	m_dispatch/dispatch-amd64-darwin.S \
 	m_dispatch/dispatch-x86-solaris.S \
@@ -414,6 +415,7 @@
 	m_gdbserver/valgrind-low-mips32.c \
 	m_gdbserver/valgrind-low-mips64.c \
 	m_gdbserver/valgrind-low-nanomips.c \
+	m_gdbserver/valgrind-low-riscv64.c \
 	m_gdbserver/version.c \
 	m_initimg/initimg-linux.c \
 	m_initimg/initimg-freebsd.c \
@@ -441,6 +443,7 @@
 	m_sigframe/sigframe-mips32-linux.c \
 	m_sigframe/sigframe-mips64-linux.c \
 	m_sigframe/sigframe-nanomips-linux.c \
+	m_sigframe/sigframe-riscv64-linux.c \
 	m_sigframe/sigframe-x86-darwin.c \
 	m_sigframe/sigframe-amd64-darwin.c \
 	m_sigframe/sigframe-solaris.c \
@@ -451,6 +454,7 @@
 	m_syswrap/syscall-ppc64le-linux.S \
 	m_syswrap/syscall-arm-linux.S \
 	m_syswrap/syscall-arm64-linux.S \
+	m_syswrap/syscall-riscv64-linux.S \
 	m_syswrap/syscall-s390x-linux.S \
 	m_syswrap/syscall-mips32-linux.S \
 	m_syswrap/syscall-mips64-linux.S \
@@ -480,6 +484,7 @@
 	m_syswrap/syswrap-mips32-linux.c \
 	m_syswrap/syswrap-mips64-linux.c \
 	m_syswrap/syswrap-nanomips-linux.c \
+	m_syswrap/syswrap-riscv64-linux.c \
 	m_syswrap/syswrap-x86-darwin.c \
 	m_syswrap/syswrap-amd64-darwin.c \
 	m_syswrap/syswrap-xen.c \
--- a/coregrind/launcher-linux.c
+++ b/coregrind/launcher-linux.c
@@ -75,6 +75,10 @@
 #define E_MIPS_ABI2    0x00000020
 #endif
 
+#ifndef EM_RISCV
+#define EM_RISCV 243
+#endif
+
 /* Report fatal errors */
 __attribute__((noreturn))
 static void barf ( const char *format, ... )
@@ -324,6 +328,10 @@
                 (header.ehdr64.e_ident[EI_OSABI] == ELFOSABI_SYSV ||
                  header.ehdr64.e_ident[EI_OSABI] == ELFOSABI_LINUX)) {
                platform = "ppc64be-linux";
+	    } else if (header.ehdr64.e_machine == EM_RISCV &&
+                (header.ehdr64.e_ident[EI_OSABI] == ELFOSABI_SYSV ||
+                 header.ehdr64.e_ident[EI_OSABI] == ELFOSABI_LINUX)) {
+               platform = "riscv64-linux";
             } 
             else 
             if (header.ehdr64.e_machine == EM_S390 &&
@@ -415,7 +423,8 @@
        (0==strcmp(VG_PLATFORM,"s390x-linux"))  ||
        (0==strcmp(VG_PLATFORM,"mips32-linux")) ||
        (0==strcmp(VG_PLATFORM,"mips64-linux")) ||
-       (0==strcmp(VG_PLATFORM,"nanomips-linux")))
+       (0==strcmp(VG_PLATFORM,"nanomips-linux")) ||
+       (0==strcmp(VG_PLATFORM,"riscv64-linux")))
       default_platform = VG_PLATFORM;
 #  elif defined(VGO_solaris)
    if ((0==strcmp(VG_PLATFORM,"x86-solaris")) ||
--- a/coregrind/m_aspacemgr/aspacemgr-common.c
+++ b/coregrind/m_aspacemgr/aspacemgr-common.c
@@ -157,7 +157,8 @@
 #  elif defined(VGP_amd64_linux) \
         || defined(VGP_ppc64be_linux)  || defined(VGP_ppc64le_linux) \
         || defined(VGP_s390x_linux) || defined(VGP_mips32_linux) \
-        || defined(VGP_mips64_linux) || defined(VGP_arm64_linux)
+        || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
+        defined(VGP_riscv64_linux)
    res = VG_(do_syscall6)(__NR_mmap, (UWord)start, length, 
                          prot, flags, fd, offset);
 #  elif defined(VGP_x86_darwin)
@@ -262,7 +263,7 @@
 
 SysRes ML_(am_open) ( const HChar* pathname, Int flags, Int mode )
 {
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    /* ARM64 wants to use __NR_openat rather than __NR_open. */
    SysRes res = VG_(do_syscall4)(__NR_openat,
                                  VKI_AT_FDCWD, (UWord)pathname, flags, mode);
@@ -291,7 +292,7 @@
 Int ML_(am_readlink)(const HChar* path, HChar* buf, UInt bufsiz)
 {
    SysRes res;
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    res = VG_(do_syscall4)(__NR_readlinkat, VKI_AT_FDCWD,
                                            (UWord)path, (UWord)buf, bufsiz);
 #  elif defined(VGO_linux) || defined(VGO_darwin) || defined(VGO_freebsd)
--- a/coregrind/m_cache.c
+++ b/coregrind/m_cache.c
@@ -539,12 +539,14 @@
 #elif defined(VGA_arm) || defined(VGA_ppc32)    || \
    defined(VGA_ppc64be) || defined(VGA_ppc64le) || \
    defined(VGA_mips32) || defined(VGA_mips64) || \
-   defined(VGA_arm64) || defined(VGA_nanomips)
+   defined(VGA_arm64) || defined(VGA_nanomips) || \
+   defined(VGA_riscv64)
+
 static Bool
 get_cache_info(VexArchInfo *vai)
 {
    vai->hwcache_info.icaches_maintain_coherence = False;
-
+   /* TODO Revisit for riscv64. */
    return False;   // not yet
 }
 
--- a/coregrind/m_coredump/coredump-elf.c
+++ b/coregrind/m_coredump/coredump-elf.c
@@ -277,7 +277,7 @@
    prs->pr_sid = VG_(getpgrp)();
 #endif
    
-#if defined(VGP_s390x_linux)
+#if defined(VGP_s390x_linux) || defined(VGP_riscv64_linux)
    /* prs->pr_reg has struct type. Need to take address. */
    regs = (struct vki_user_regs_struct *)&(prs->pr_reg);
 #elif defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
@@ -528,6 +528,9 @@
    regs->fs     = arch->vex.guest_FS;
    regs->gs     = arch->vex.guest_GS;
 
+#elif defined(VGP_riscv64_linux)
+   /* TODO Implement. */
+
 #else
 #  error Unknown ELF platform
 #endif
@@ -664,6 +667,9 @@
    DO(8);  DO(9);  DO(10); DO(11); DO(12); DO(13); DO(14); DO(15);
 #  undef DO
 
+#elif defined(VGP_riscv64_linux)
+   /* TODO Implement. */
+
 #else
 #  error Unknown ELF platform
 #endif
--- a/coregrind/m_debuginfo/d3basics.c
+++ b/coregrind/m_debuginfo/d3basics.c
@@ -524,6 +524,9 @@
 #  elif defined(VGP_arm64_linux)
    if (regno == 31) { *a = regs->sp; return True; }
    if (regno == 29) { *a = regs->fp; return True; }
+#  elif defined(VGP_riscv64_linux)
+   I_die_here;
+
 #  else
 #    error "Unknown platform"
 #  endif
--- a/coregrind/m_debuginfo/debuginfo.c
+++ b/coregrind/m_debuginfo/debuginfo.c
@@ -1178,7 +1178,8 @@
    is_rx_map = seg->hasR && seg->hasX;
    is_rw_map = seg->hasR && seg->hasW;
 #  elif defined(VGA_amd64) || defined(VGA_ppc64be) || defined(VGA_ppc64le)  \
-        || defined(VGA_arm) || defined(VGA_arm64)
+        || defined(VGA_arm) || defined(VGA_arm64) \
+	|| defined(VGA_riscv64)
    is_rx_map = seg->hasR && seg->hasX && !seg->hasW;
    is_rw_map = seg->hasR && seg->hasW && !seg->hasX;
 #  elif defined(VGP_s390x_linux)
@@ -2888,6 +2889,8 @@
             case Creg_ARM64_SP: return eec->uregs->sp;
             case Creg_ARM64_X30: return eec->uregs->x30;
             case Creg_ARM64_X29: return eec->uregs->x29;
+	    elif defined(VGP_riscv64_linux)
+            I_die_here;
 #           else
 #             error "Unsupported arch"
 #           endif
--- a/coregrind/m_debuginfo/priv_storage.h
+++ b/coregrind/m_debuginfo/priv_storage.h
@@ -235,6 +235,8 @@
               CFIR_S390X_F5  -> old value of %f5
               CFIR_S390X_F6  -> old value of %f6
               CFIR_S390X_F7  -> old value of %f7
+
+TODO riscv64
 */
 
 #define CFIC_IA_SPREL     ((UChar)1)
@@ -355,6 +357,21 @@
    }
    DiCfSI_m;
 #elif defined(VGA_mips32) || defined(VGA_mips64) || defined(VGA_nanomips)
+
+typedef
+   struct {
+      UChar cfa_how; /* a CFIC_ value */
+      UChar ra_how;  /* a CFIR_ value */
+      UChar sp_how;  /* a CFIR_ value */
+      UChar fp_how;  /* a CFIR_ value */
+      Int   cfa_off;
+      Int   ra_off;
+      Int   sp_off;
+      Int   fp_off;
+   }
+   DiCfSI_m;
+#elif defined(VGA_riscv64)
+
 typedef
    struct {
       UChar cfa_how; /* a CFIC_ value */
--- a/coregrind/m_debuginfo/readdwarf.c
+++ b/coregrind/m_debuginfo/readdwarf.c
@@ -2005,6 +2005,10 @@
 #  define FP_REG         30
 #  define SP_REG         29
 #  define RA_REG_DEFAULT 31
+#elif defined(VGP_riscv64_linux)
+#  define FP_REG         8
+#  define SP_REG         2
+#  define RA_REG_DEFAULT 1
 #else
 #  error "Unknown platform"
 #endif
@@ -2023,6 +2027,8 @@
 # define N_CFI_REGS 128
 #elif defined(VGP_s390x_linux)
 # define N_CFI_REGS 66
+#elif defined(VGP_riscv64_linux)
+# define N_CFI_REGS 128
 #else
 # define N_CFI_REGS 20
 #endif
@@ -2249,6 +2255,10 @@
          start out as RR_Same. */
       ctx->state[j].reg[29/*FP*/].tag = RR_Same;
       ctx->state[j].reg[30/*LR*/].tag = RR_Same;
+      elif defined(VGA_riscv64)
+      /* Registers fp and ra start out implicitly as RR_Same. */
+      ctx->state[j].reg[FP_REG].tag = RR_Same;
+      ctx->state[j].reg[RA_REG_DEFAULT].tag = RR_Same;
 #     endif
    }
 }
@@ -2331,7 +2341,7 @@
    if (ctxs->cfa_is_regoff && ctxs->cfa_reg == SP_REG) {
       si_m->cfa_off = ctxs->cfa_off;
 #     if defined(VGA_x86) || defined(VGA_amd64) || defined(VGA_s390x) \
-         || defined(VGA_mips32) || defined(VGA_nanomips) || defined(VGA_mips64)
+         || defined(VGA_mips32) || defined(VGA_nanomips) || defined(VGA_mips64) || defined(VGA_riscv64)
       si_m->cfa_how = CFIC_IA_SPREL;
 #     elif defined(VGA_arm)
       si_m->cfa_how = CFIC_ARM_R13REL;
@@ -2345,7 +2355,7 @@
    if (ctxs->cfa_is_regoff && ctxs->cfa_reg == FP_REG) {
       si_m->cfa_off = ctxs->cfa_off;
 #     if defined(VGA_x86) || defined(VGA_amd64) || defined(VGA_s390x) \
-         || defined(VGA_mips32) || defined(VGA_nanomips) || defined(VGA_mips64)
+         || defined(VGA_mips32) || defined(VGA_nanomips) || defined(VGA_mips64) || defined(VGA_riscv64)
       si_m->cfa_how = CFIC_IA_BPREL;
 #     elif defined(VGA_arm)
       si_m->cfa_how = CFIC_ARM_R12REL;
@@ -2725,6 +2735,30 @@
 #  elif defined(VGA_ppc32) || defined(VGA_ppc64be) || defined(VGA_ppc64le)
    /* These don't use CFI based unwinding (is that really true?) */
 
+#  elif defined(VGA_riscv64)
+
+   /* --- entire tail of this fn specialised for riscv64 --- */
+
+   SUMMARISE_HOW(si_m->ra_how, si_m->ra_off, ctxs->reg[ctx->ra_reg]);
+   SUMMARISE_HOW(si_m->fp_how, si_m->fp_off, ctxs->reg[FP_REG]);
+
+   /* on riscv64, it seems the old sp value before the call is always
+      the same as the CFA.  Therefore ... */
+   si_m->sp_how = CFIR_CFAREL;
+   si_m->sp_off = 0;
+
+   /* bogus looking range?  Note, we require that the difference is
+      representable in 32 bits. */
+   if (loc_start >= ctx->loc)
+      { why = 4; goto failed; }
+   if (ctx->loc - loc_start > 10000000 /* let's say */)
+      { why = 5; goto failed; }
+
+   *base = loc_start + ctx->initloc;
+   *len  = (UInt)(ctx->loc - loc_start);
+
+   return True;
+
 #  else
 #    error "Unknown arch"
 #  endif
@@ -2824,6 +2858,8 @@
             return ML_(CfiExpr_CfiReg)( dstxa, Creg_ARM64_X30 );
 #        elif defined(VGA_ppc32) || defined(VGA_ppc64be) \
             || defined(VGA_ppc64le)
+	 elif defined(VGA_riscv64)
+         I_die_here;
 #        else
 #           error "Unknown arch"
 #        endif
--- a/coregrind/m_debuginfo/readelf.c
+++ b/coregrind/m_debuginfo/readelf.c
@@ -1773,7 +1773,7 @@
 
    while (tries > 0) {
       SysRes res;
-#if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
       res = VG_(do_syscall4)(__NR_readlinkat, VKI_AT_FDCWD,
                                               (UWord)path, (UWord)buf, bufsiz);
 #elif defined(VGO_linux) || defined(VGO_darwin) || defined(VGO_freebsd)
@@ -2622,7 +2622,8 @@
          || defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
          || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) \
          || defined(VGP_x86_solaris) || defined(VGP_amd64_solaris) \
-         || defined(VGP_x86_freebsd) || defined(VGP_amd64_freebsd)
+         || defined(VGP_x86_freebsd) || defined(VGP_amd64_freebsd) \
+         || defined(VGP_riscv64_linux)
       /* Accept .plt where mapped as rx (code) */
       if (0 == VG_(strcmp)(name, ".plt")) {
          if (inrx && !di->plt_present) {
--- a/coregrind/m_debuginfo/storage.c
+++ b/coregrind/m_debuginfo/storage.c
@@ -260,6 +260,11 @@
    SHOW_HOW(si_m->x30_how, si_m->x30_off);
    VG_(printf)(" X29=");
    SHOW_HOW(si_m->x29_how, si_m->x29_off);
+   #  elif defined(VGA_riscv64)
+   VG_(printf)(" SP=");
+   SHOW_HOW(si_m->sp_how, si_m->sp_off);
+   VG_(printf)(" FP=");
+   SHOW_HOW(si_m->fp_how, si_m->fp_off);
 #  else
 #    error "Unknown arch"
 #  endif
--- a/coregrind/m_debuglog.c
+++ b/coregrind/m_debuglog.c
@@ -601,6 +601,34 @@
    return a0;
 }
 
+#elif defined(VGP_riscv64_linux)
+
+static UInt local_sys_write_stderr ( const HChar* buf, Int n )
+{
+   register RegWord a0 asm("a0") = 2; /* stderr */
+   register RegWord a1 asm("a1") = (RegWord)buf;
+   register RegWord a2 asm("a2") = n;
+   register RegWord a7 asm("a7") = __NR_write;
+   __asm__ volatile (
+      "ecall\n"
+      : "+r" (a0)
+      : "r" (a1), "r" (a2), "r" (a7)
+   );
+   return a0 >= 0 ? (UInt)a0 : -1;
+}
+
+static UInt local_sys_getpid ( void )
+{
+   register RegWord a0 asm("a0");
+   register RegWord a7 asm("a7") = __NR_getpid;
+   __asm__ volatile (
+      "ecall\n"
+      : "=r" (a0)
+      : "r" (a7)
+   );
+   return (UInt)a0;
+}
+
 #elif defined(VGP_x86_solaris)
 static UInt local_sys_write_stderr ( const HChar* buf, Int n )
 {
--- /dev/null
+++ b/coregrind/m_dispatch/dispatch-riscv64-linux.S
@@ -0,0 +1,298 @@
+
+/*--------------------------------------------------------------------*/
+/*--- The core dispatch loop, for jumping to a code address.       ---*/
+/*---                                     dispatch-riscv64-linux.S ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+  This file is part of Valgrind, a dynamic binary instrumentation
+  framework.
+
+  Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+  This program is free software; you can redistribute it and/or
+  modify it under the terms of the GNU General Public License as
+  published by the Free Software Foundation; either version 2 of the
+  License, or (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+  The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "pub_core_basics_asm.h"
+
+#if defined(VGP_riscv64_linux)
+
+#include "pub_core_dispatch_asm.h"
+#include "pub_core_transtab_asm.h"
+#include "libvex_guest_offsets.h"
+
+
+/*------------------------------------------------------------*/
+/*---                                                      ---*/
+/*--- The dispatch loop.  VG_(disp_run_translations) is    ---*/
+/*--- used to run all translations,                        ---*/
+/*--- including no-redir ones.                             ---*/
+/*---                                                      ---*/
+/*------------------------------------------------------------*/
+
+/*----------------------------------------------------*/
+/*--- Entry and preamble (set everything up)       ---*/
+/*----------------------------------------------------*/
+
+/* signature:
+void VG_(disp_run_translations)( UWord* two_words,
+                                 void*  guest_state,
+                                 Addr   host_addr );
+*/
+.text
+.global VG_(disp_run_translations)
+VG_(disp_run_translations):
+	/* a0 holds two_words
+	   a1 holds guest_state
+	   a2 holds host_addr
+	*/
+	/* Push the callee-saved registers. Note this sequence maintains
+	   16-alignment of sp. Also save a0 since it will be needed in the
+	   postamble. */
+	addi sp, sp, -112
+	sd ra, 104(sp)
+	sd s0, 96(sp)
+	sd s1, 88(sp)
+	sd s2, 80(sp)
+	sd s3, 72(sp)
+	sd s4, 64(sp)
+	sd s5, 56(sp)
+	sd s6, 48(sp)
+	sd s7, 40(sp)
+	sd s8, 32(sp)
+	sd s9, 24(sp)
+	sd s10, 16(sp)
+	sd s11, 8(sp)
+	sd a0, 0(sp)
+
+	/* Set up the guest state pointer. */
+	li t0, 2048
+	add s0, a1, t0
+
+	/* and jump into the code cache. Chained translations in the code cache
+	   run, until for whatever reason, they can't continue. When that
+	   happens, the translation in question will jump (or call) to one of
+	   the continuation points VG_(cp_...) below. */
+	jr a2
+	/* NOTREACHED */
+
+/*----------------------------------------------------*/
+/*--- Postamble and exit.                          ---*/
+/*----------------------------------------------------*/
+
+postamble:
+	/* At this point, t0 and t1 contain two words to be returned to the
+	   caller. t0 holds a TRC value, and t1 optionally may hold another
+	   word (for CHAIN_ME exits, the address of the place to patch.) */
+
+	/* Restore int regs, including importantly a0 (two_words). */
+	ld ra, 104(sp)
+	ld s0, 96(sp)
+	ld s1, 88(sp)
+	ld s2, 80(sp)
+	ld s3, 72(sp)
+	ld s4, 64(sp)
+	ld s5, 56(sp)
+	ld s6, 48(sp)
+	ld s7, 40(sp)
+	ld s8, 32(sp)
+	ld s9, 24(sp)
+	ld s10, 16(sp)
+	ld s11, 8(sp)
+	ld a0, 0(sp)
+	addi sp, sp, 112
+
+	/* Stash return values. */
+	sd t0, 0(a0)
+	sd t1, 8(a0)
+	ret
+
+/*----------------------------------------------------*/
+/*--- Continuation points                          ---*/
+/*----------------------------------------------------*/
+
+/* ------ Chain me to slow entry point ------ */
+.global VG_(disp_cp_chain_me_to_slowEP)
+VG_(disp_cp_chain_me_to_slowEP):
+	/* We got called. The return address indicates where the patching needs
+	   to happen. Collect the return address and, exit back to C land,
+	   handing the caller the pair (Chain_me_F, RA). */
+	li t0, VG_TRC_CHAIN_ME_TO_SLOW_EP
+	mv t1, ra
+	/* 4 = lui t0, disp_cp_chain_me_to_slowEP[47:28]'
+	   4 = addiw t0, t0, disp_cp_chain_me_to_slowEP[27:16]'
+	   2 = c.slli t0, 12
+	   4 = addi t0, t0, disp_cp_chain_me_to_slowEP[15:4]'
+	   2 = c.slli t0, 4
+	   2 = c.addi t0, disp_cp_chain_me_to_slowEP[3:0]'
+	   2 = c.jalr 0(t0)
+	*/
+	addi t1, t1, -(4+4+2+4+2+2+2)
+	j postamble
+
+/* ------ Chain me to fast entry point ------ */
+.global VG_(disp_cp_chain_me_to_fastEP)
+VG_(disp_cp_chain_me_to_fastEP):
+	/* We got called. The return address indicates where the patching needs
+	   to happen. Collect the return address and, exit back to C land,
+	   handing the caller the pair (Chain_me_F, RA). */
+	li t0, VG_TRC_CHAIN_ME_TO_FAST_EP
+	mv t1, ra
+	/* 4 = lui t0, disp_cp_chain_me_to_fastEP[47:28]'
+	   4 = addiw t0, t0, disp_cp_chain_me_to_fastEP[27:16]'
+	   2 = c.slli t0, 12
+	   4 = addi t0, t0, disp_cp_chain_me_to_fastEP[15:4]'
+	   2 = c.slli t0, 4
+	   2 = c.addi t0, disp_cp_chain_me_to_fastEP[3:0]'
+	   2 = c.jalr 0(t0)
+	*/
+	addi t1, t1, -(4+4+2+4+2+2+2)
+	j postamble
+
+/* ------ Indirect but boring jump ------ */
+.global VG_(disp_cp_xindir)
+VG_(disp_cp_xindir):
+	/* Where are we going? */
+	ld t0, OFFSET_riscv64_pc-2048(s0)
+
+	/* Stats only. */
+	lw t1, VG_(stats__n_xIndirs_32)
+	addi t1, t1, 1
+	sw t1, VG_(stats__n_xIndirs_32), t2
+
+	/* LIVE: s0 (guest state ptr), t0 (guest address to go to).
+	   We use 6 temporaries:
+	     t6 (to point at the relevant FastCacheSet),
+	     t1, t2, t3 (scratch, for swapping entries within a set)
+	     t4, t5 (other scratch) */
+
+	/* Try a fast lookup in the translation cache. This is pretty much
+	   a handcoded version of VG_(lookupInFastCache). */
+
+	/* Compute t6 = VG_TT_FAST_HASH(guest). */
+	srli t6, t0, 2                       /* g2 = guest >> 2 */
+	srli t4, t6, VG_TT_FAST_BITS         /* g2 >> VG_TT_FAST_BITS */
+	xor t6, t4, t6                       /* (g2 >> VG_TT_FAST_BITS) ^ g2 */
+	li t4, VG_TT_FAST_MASK               /* VG_TT_FAST_MASK */
+	and t6, t6, t4                       /* setNo */
+
+	/* Compute t6 = &VG_(tt_fast)[t6]. */
+	la t4, VG_(tt_fast)                  /* &VG_(tt_fast)[0] */
+	slli t6, t6, VG_FAST_CACHE_SET_BITS
+	add t6, t4, t6                       /* &VG_(tt_fast)[setNo] */
+
+	/* LIVE: s0 (guest state ptr), t0 (guest addr), t6 (cache set). */
+	/* Try way 0. */
+	ld t4, FCS_g0(t6)                    /* t4 = .guest0 */
+	bne t4, t0, 1f                       /* cmp against .guest0 */
+	/* Hit at way 0. */
+	/* Go to .host0. */
+	ld t5, FCS_h0(t6)                    /* t5 = .host0 */
+	jr t5
+	/*NOTREACHED*/
+
+1:	/* Try way 1. */
+	ld t4, FCS_g1(t6)
+	bne t4, t0, 2f                       /* cmp against .guest1 */
+	/* Hit at way 1; swap upwards. */
+	ld t1, FCS_g0(t6)                    /* t1 = old .guest0 */
+	ld t2, FCS_h0(t6)                    /* t2 = old .host0 */
+	ld t3, FCS_h1(t6)                    /* t3 = old .host1 */
+	sd t0, FCS_g0(t6)                    /* new .guest0 = guest */
+	sd t3, FCS_h0(t6)                    /* new .host0 = old .host1 */
+	sd t1, FCS_g1(t6)                    /* new .guest1 = old .guest0 */
+	sd t2, FCS_h1(t6)                    /* new .host1 = old .host0 */
+	/* Stats only. */
+	lw t4, VG_(stats__n_xIndir_hits1_32)
+	addi t4, t4, 1
+	sw t4, VG_(stats__n_xIndir_hits1_32), t5
+	/* Go to old .host1 a.k.a. new .host0. */
+	jr t3
+	/*NOTREACHED*/
+
+2:	/* Try way 2. */
+	ld t4, FCS_g2(t6)
+	bne t4, t0, 3f                       /* cmp against .guest2 */
+	/* Hit at way 2; swap upwards. */
+	ld t1, FCS_g1(t6)
+	ld t2, FCS_h1(t6)
+	ld t3, FCS_h2(t6)
+	sd t0, FCS_g1(t6)
+	sd t3, FCS_h1(t6)
+	sd t1, FCS_g2(t6)
+	sd t2, FCS_h2(t6)
+	/* Stats only. */
+	lw t4, VG_(stats__n_xIndir_hits2_32)
+	addi t4, t4, 1
+	sw t4, VG_(stats__n_xIndir_hits2_32), t5
+	/* Go to old .host2 a.k.a. new .host1. */
+	jr t3
+	/*NOTREACHED*/
+
+3:	/* Try way 3. */
+	ld t4, FCS_g3(t6)
+	bne t4, t0, 4f                       /* cmp against .guest3 */
+	/* Hit at way 3; swap upwards. */
+	ld t1, FCS_g2(t6)
+	ld t2, FCS_h2(t6)
+	ld t3, FCS_h3(t6)
+	sd t0, FCS_g2(t6)
+	sd t3, FCS_h2(t6)
+	sd t1, FCS_g3(t6)
+	sd t2, FCS_h3(t6)
+	/* Stats only. */
+	lw t4, VG_(stats__n_xIndir_hits3_32)
+	addi t4, t4, 1
+	sw t4, VG_(stats__n_xIndir_hits3_32), t5
+	/* Go to old .host3 a.k.a. new .host2. */
+	jr t3
+	/*NOTREACHED*/
+
+4:	/* Fast lookup failed. */
+	lw t4, VG_(stats__n_xIndir_misses_32)
+	addi t4, t4, 1
+	sw t4, VG_(stats__n_xIndir_misses_32), t5
+
+	li t0, VG_TRC_INNER_FASTMISS
+	li t1, 0
+	j postamble
+
+/* ------ Assisted jump ------ */
+.global VG_(disp_cp_xassisted)
+VG_(disp_cp_xassisted):
+	/* s0 contains the TRC. */
+	mv t0, s0
+	li t1, 0
+	j postamble
+
+/* ------ Event check failed ------ */
+.global VG_(disp_cp_evcheck_fail)
+VG_(disp_cp_evcheck_fail):
+	li t0, VG_TRC_INNER_COUNTERZERO
+	li t1, 0
+	j postamble
+
+.size VG_(disp_run_translations), .-VG_(disp_run_translations)
+
+#endif // defined(VGP_riscv64_linux)
+
+/* Let the linker know we don't need an executable stack */
+MARK_STACK_NO_EXEC
+
+/*--------------------------------------------------------------------*/
+/*--- end                                 dispatch-riscv64-linux.S ---*/
+/*--------------------------------------------------------------------*/
--- a/coregrind/m_gdbserver/target.c
+++ b/coregrind/m_gdbserver/target.c
@@ -902,6 +902,9 @@
    mips64_init_architecture(&the_low_target);
 #elif defined(VGA_nanomips)
    nanomips_init_architecture(&the_low_target);
+#elif defined(VGA_riscv64)
+   /* TODO Implement. */
+   /*I_die_here;*/
 #else
    #error "architecture missing in target.c valgrind_initialize_target"
 #endif
--- /dev/null
+++ b/coregrind/m_gdbserver/valgrind-low-riscv64.c
@@ -0,0 +1 @@
+/* TODO */
--- a/coregrind/m_initimg/initimg-linux.c
+++ b/coregrind/m_initimg/initimg-linux.c
@@ -892,9 +892,14 @@
 #        if !defined(VGP_ppc32_linux) && !defined(VGP_ppc64be_linux) \
             && !defined(VGP_ppc64le_linux) \
             && !defined(VGP_mips32_linux) && !defined(VGP_mips64_linux) \
-            && !defined(VGP_nanomips_linux)
+            && !defined(VGP_nanomips_linux) && !defined(VGP_riscv64_linux)
          case AT_SYSINFO_EHDR: {
             /* Trash this, because we don't reproduce it */
+	   /* riscv64-linux: Keep the VDSO mapping on this platform present.
+               It contains __kernel_rt_sigreturn() which the kernel sets the ra
+               register to point to on a signal delivery. */
+            /* TODO (riscv64-linux): Export this mapping to the client? Can its
+               code be translated? */
             const NSegment* ehdrseg = VG_(am_find_nsegment)((Addr)auxv->u.a_ptr);
             vg_assert(ehdrseg);
             VG_(am_munmap_valgrind)(ehdrseg->start, ehdrseg->end - ehdrseg->start);
@@ -1303,6 +1308,25 @@
    arch->vex.guest_PC = iifii.initial_client_IP;
    arch->vex.guest_r31 = iifii.initial_client_SP;
 
+#  elif defined(VGP_riscv64_linux)
+   vg_assert(0 == sizeof(VexGuestRISCV64State) % LibVEX_GUEST_STATE_ALIGN);
+
+   /* Zero out the initial state. */
+   LibVEX_GuestRISCV64_initialise(&arch->vex);
+
+   /* Mark all registers as undefined ... */
+   VG_(memset)(&arch->vex_shadow1, 0xFF, sizeof(VexGuestRISCV64State));
+   VG_(memset)(&arch->vex_shadow2, 0x00, sizeof(VexGuestRISCV64State));
+
+   arch->vex.guest_x2 = iifii.initial_client_SP;
+   arch->vex.guest_pc = iifii.initial_client_IP;
+
+   /* Tell the tool about the registers we just wrote. */
+   VG_TRACK(post_reg_write, Vg_CoreStartup, /*tid*/1, VG_O_STACK_PTR, 8);
+   VG_TRACK(post_reg_write, Vg_CoreStartup, /*tid*/1, VG_O_INSTR_PTR, 8);
+
+#define PRECISE_GUEST_REG_DEFINEDNESS_AT_STARTUP 1
+
 #  else
 #    error Unknown platform
 #  endif
--- a/coregrind/m_libcassert.c
+++ b/coregrind/m_libcassert.c
@@ -264,6 +264,27 @@
         (srP)->misc.MIPS32.r31 = (UInt)ra;                \
         (srP)->misc.MIPS32.r28 = (UInt)gp;                \
       }
+
+#elif defined(VGP_riscv64_linux)
+#  define GET_STARTREGS(srP)                              \
+      { ULong pc, sp, fp, ra;                             \
+        __asm__ __volatile__(                             \
+           "jal %0, 0f;"                                  \
+           "0:\n"                                         \
+           "mv %1, sp;"                                   \
+           "mv %2, fp;"                                   \
+           "mv %3, ra;"                                   \
+           : "=r" (pc),                                   \
+             "=r" (sp),                                   \
+             "=r" (fp),                                   \
+             "=r" (ra)                                    \
+        );                                                \
+        (srP)->r_pc = pc;                                 \
+        (srP)->r_sp = sp;                                 \
+        (srP)->misc.RISCV64.r_fp = fp;                    \
+        (srP)->misc.RISCV64.r_ra = ra;                    \
+      }
+
 #else
 #  error Unknown platform
 #endif
--- a/coregrind/m_libcfile.c
+++ b/coregrind/m_libcfile.c
@@ -181,7 +181,8 @@
 
 SysRes VG_(mknod) ( const HChar* pathname, Int mode, UWord dev )
 {
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
+   /* More recent Linux platforms have only __NR_mknodat and no __NR_mknod. */
    /* ARM64 wants to use __NR_mknodat rather than __NR_mknod. */
    SysRes res = VG_(do_syscall4)(__NR_mknodat,
                                  VKI_AT_FDCWD, (UWord)pathname, mode, dev);
@@ -202,7 +203,8 @@
 
 SysRes VG_(open) ( const HChar* pathname, Int flags, Int mode )
 {
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
+   /* More recent Linux platforms have only __NR_openat and no __NR_open. */
    /* ARM64 wants to use __NR_openat rather than __NR_open. */
    SysRes res = VG_(do_syscall4)(__NR_openat,
                                  VKI_AT_FDCWD, (UWord)pathname, flags, mode);
@@ -296,7 +298,7 @@
    } else {
       return -1;
    }
-#  elif defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  elif defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    SysRes res = VG_(do_syscall2)(__NR_pipe2, (UWord)fd, 0);
    return sr_isError(res) ? -1 : 0;
 #  elif defined(VGO_linux)
@@ -615,7 +617,8 @@
 
 SysRes VG_(dup2) ( Int oldfd, Int newfd )
 {
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) \
+      || defined(VGP_riscv64_linux)
    /* We only have dup3, that means we have to mimic dup2.
       The only real difference is when oldfd == newfd.
       dup3 always returns an error, but dup2 returns only an
@@ -657,7 +660,7 @@
 #  if defined(VGO_solaris) || defined(VGP_arm64_linux)
    SysRes res = VG_(do_syscall4)(__NR_renameat, VKI_AT_FDCWD, (UWord)old_name,
                                  VKI_AT_FDCWD, (UWord)new_name);
-#  elif defined(VGP_nanomips_linux)
+#  elif defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    SysRes res = VG_(do_syscall5)(__NR_renameat2, VKI_AT_FDCWD, (UWord)old_name,
                                  VKI_AT_FDCWD, (UWord)new_name, 0);
 
@@ -671,7 +674,7 @@
 
 Int VG_(unlink) ( const HChar* file_name )
 {
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    SysRes res = VG_(do_syscall2)(__NR_unlinkat, VKI_AT_FDCWD,
                                                 (UWord)file_name);
 #  elif defined(VGO_linux) || defined(VGO_darwin) || defined(VGO_freebsd)
@@ -795,7 +798,7 @@
 {
    SysRes res;
    /* res = readlink( path, buf, bufsiz ); */
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    res = VG_(do_syscall4)(__NR_readlinkat, VKI_AT_FDCWD,
                                            (UWord)path, (UWord)buf, bufsiz);
 #  elif defined(VGO_linux) || defined(VGO_darwin) || defined(VGO_freebsd)
@@ -874,7 +877,7 @@
    UWord w = (irusr ? VKI_R_OK : 0)
              | (iwusr ? VKI_W_OK : 0)
              | (ixusr ? VKI_X_OK : 0);
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    SysRes res = VG_(do_syscall3)(__NR_faccessat, VKI_AT_FDCWD, (UWord)path, w);
 #  elif defined(VGO_linux) || defined(VGO_darwin) || defined(VGO_freebsd)
    SysRes res = VG_(do_syscall2)(__NR_access, (UWord)path, w);
@@ -1020,7 +1023,8 @@
    return res;
 #  elif defined(VGP_amd64_linux) || defined(VGP_s390x_linux) \
       || defined(VGP_ppc64be_linux)  || defined(VGP_ppc64le_linux) \
-      || defined(VGP_mips64_linux) || defined(VGP_arm64_linux)
+      || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
+      || defined(VGP_riscv64_linux)
    res = VG_(do_syscall4)(__NR_pread64, fd, (UWord)buf, count, offset);
    return res;
 #  elif defined(VGP_amd64_freebsd)
@@ -1284,7 +1288,8 @@
 
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
-        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGO_freebsd)
+        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGO_freebsd) \
+        || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall3)(__NR_socket, domain, type, protocol );
    return sr_isError(res) ? -1 : sr_Res(res);
@@ -1339,7 +1344,8 @@
 
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
-        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGO_freebsd)
+        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGO_freebsd) \
+        || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall3)(__NR_connect, sockfd, (UWord)serv_addr, addrlen);
    return sr_isError(res) ? -1 : sr_Res(res);
@@ -1386,7 +1392,8 @@
 
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
-        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGO_freebsd)
+        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGO_freebsd) \
+	|| defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall6)(__NR_sendto, sd, (UWord)msg, 
                                        count, VKI_MSG_NOSIGNAL, 0,0);
@@ -1424,7 +1431,8 @@
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
         || defined(VGP_nanomips_linux) || defined(VGO_freebsd) \
-        || defined(VGP_mips64_linux) || defined(VGP_arm64_linux)
+        || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
+        || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall3)( __NR_getsockname,
                            (UWord)sd, (UWord)name, (UWord)namelen );
@@ -1463,7 +1471,8 @@
 
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
-        || defined(VGP_nanomips_linux) || defined(VGO_freebsd)
+        || defined(VGP_nanomips_linux) || defined(VGO_freebsd) \
+        || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall3)( __NR_getpeername,
                            (UWord)sd, (UWord)name, (UWord)namelen );
@@ -1505,7 +1514,7 @@
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
         || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) \
-        || defined(VGO_freebsd)
+        || defined(VGO_freebsd) || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall5)( __NR_getsockopt,
                            (UWord)sd, (UWord)level, (UWord)optname, 
@@ -1549,7 +1558,8 @@
 
 #  elif defined(VGP_amd64_linux) || defined(VGP_arm_linux) \
         || defined(VGP_mips32_linux) || defined(VGP_mips64_linux) \
-        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+        || defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) \
+        || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall5)( __NR_setsockopt,
                            (UWord)sd, (UWord)level, (UWord)optname, 
--- a/coregrind/m_libcproc.c
+++ b/coregrind/m_libcproc.c
@@ -697,7 +697,7 @@
        * the /proc/self link is pointing...
        */
 
-#     if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#     if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
       res = VG_(do_syscall4)(__NR_readlinkat, VKI_AT_FDCWD,
                              (UWord)"/proc/self",
                              (UWord)pid, sizeof(pid));
@@ -752,7 +752,7 @@
 Int VG_(getpgrp) ( void )
 {
    /* ASSUMES SYSCALL ALWAYS SUCCEEDS */
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    return sr_Res( VG_(do_syscall1)(__NR_getpgid, 0) );
 #  elif defined(VGO_linux) || defined(VGO_darwin) || defined(VGO_freebsd)
    return sr_Res( VG_(do_syscall0)(__NR_getpgrp) );
@@ -849,7 +849,7 @@
         || defined(VGO_darwin) || defined(VGP_s390x_linux)    \
         || defined(VGP_mips32_linux) || defined(VGP_arm64_linux) \
         || defined(VGO_solaris) || defined(VGP_nanomips_linux) \
-        || defined(VGO_freebsd)
+        || defined(VGO_freebsd) || defined(VGP_riscv64_linux)
    SysRes sres;
    sres = VG_(do_syscall2)(__NR_getgroups, size, (Addr)list);
    if (sr_isError(sres))
@@ -890,7 +890,7 @@
 
 Int VG_(fork) ( void )
 {
-#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux)
+#  if defined(VGP_arm64_linux) || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    SysRes res;
    res = VG_(do_syscall5)(__NR_clone, VKI_SIGCHLD,
                           (UWord)NULL, (UWord)NULL, (UWord)NULL, (UWord)NULL);
@@ -1356,6 +1356,19 @@
 
    __builtin___clear_cache(ptr, (char*)ptr + nbytes);
 
+#  elif defined(VGP_riscv64_linux)
+   /* Make data stores to the area visible to all RISC-V harts. */
+   __asm__ __volatile__("fence w,r");
+
+   /* Ask the kernel to execute fence.i on all harts to guarantee that an
+      instruction fetch on each hart will see any previous data stores visible
+      to the same hart. */
+   Addr   startaddr = (Addr)ptr;
+   Addr   endaddr   = startaddr + nbytes;
+   SysRes sres = VG_(do_syscall3)(__NR_riscv_flush_icache, startaddr, endaddr,
+                                  0 /*flags*/);
+   vg_assert(!sr_isError(sres));
+
 #  endif
 }
 
@@ -1386,6 +1399,7 @@
    }
    __asm__ __volatile__("dsb ish");
 #  endif
+/* TODO Needed on riscv64? */
 }
 
 /*--------------------------------------------------------------------*/
--- a/coregrind/m_machine.c
+++ b/coregrind/m_machine.c
@@ -152,6 +152,11 @@
       = VG_(threads)[tid].arch.vex.guest_r31;
    regs->misc.MIPS64.r28
       = VG_(threads)[tid].arch.vex.guest_r28;
+#  elif defined(VGA_riscv64)
+   regs->r_pc = VG_(threads)[tid].arch.vex.guest_pc;
+   regs->r_sp = VG_(threads)[tid].arch.vex.guest_x2;
+   regs->misc.RISCV64.r_fp = VG_(threads)[tid].arch.vex.guest_x8;
+   regs->misc.RISCV64.r_ra = VG_(threads)[tid].arch.vex.guest_x1;
 #  else
 #    error "Unknown arch"
 #  endif
@@ -369,6 +374,39 @@
    (*f)(tid, "x28", vex->guest_X28);
    (*f)(tid, "x29", vex->guest_X29);
    (*f)(tid, "x30", vex->guest_X30);
+#elif defined(VGA_riscv64)
+   (*f)(tid, "x0" , vex->guest_x0 );
+   (*f)(tid, "x1" , vex->guest_x1 );
+   (*f)(tid, "x2" , vex->guest_x2 );
+   (*f)(tid, "x3" , vex->guest_x3 );
+   (*f)(tid, "x4" , vex->guest_x4 );
+   (*f)(tid, "x5" , vex->guest_x5 );
+   (*f)(tid, "x6" , vex->guest_x6 );
+   (*f)(tid, "x7" , vex->guest_x7 );
+   (*f)(tid, "x8" , vex->guest_x8 );
+   (*f)(tid, "x9" , vex->guest_x9 );
+   (*f)(tid, "x10", vex->guest_x10);
+   (*f)(tid, "x11", vex->guest_x11);
+   (*f)(tid, "x12", vex->guest_x12);
+   (*f)(tid, "x13", vex->guest_x13);
+   (*f)(tid, "x14", vex->guest_x14);
+   (*f)(tid, "x15", vex->guest_x15);
+   (*f)(tid, "x16", vex->guest_x16);
+   (*f)(tid, "x17", vex->guest_x17);
+   (*f)(tid, "x18", vex->guest_x18);
+   (*f)(tid, "x19", vex->guest_x19);
+   (*f)(tid, "x20", vex->guest_x20);
+   (*f)(tid, "x21", vex->guest_x21);
+   (*f)(tid, "x22", vex->guest_x22);
+   (*f)(tid, "x23", vex->guest_x23);
+   (*f)(tid, "x24", vex->guest_x24);
+   (*f)(tid, "x25", vex->guest_x25);
+   (*f)(tid, "x26", vex->guest_x26);
+   (*f)(tid, "x27", vex->guest_x27);
+   (*f)(tid, "x28", vex->guest_x28);
+   (*f)(tid, "x29", vex->guest_x29);
+   (*f)(tid, "x30", vex->guest_x30);
+   (*f)(tid, "x31", vex->guest_x31);
 #else
 #  error Unknown arch
 #endif
@@ -2224,6 +2262,22 @@
 
      return True;
    }
+
+#elif defined(VGA_riscv64)
+   {
+     va = VexArchRISCV64;
+     vai.endness = VexEndnessLE;
+
+     /* Hardware baseline is RV64GC. */
+     vai.hwcaps = 0;
+
+     VG_(debugLog)(1, "machine", "hwcaps = 0x%x\n", vai.hwcaps);
+
+     VG_(machine_get_cache_info)(&vai);
+
+     return True;
+   }
+
 #else
 #  error "Unknown arch"
 #endif
@@ -2357,6 +2411,10 @@
 #  elif defined(VGA_mips64)
    return 8;
 
+#  elif defined(VGA_riscv64)
+   /* TODO Correct when SIMD instructions are added. */
+   return 8;
+
 #  else
 #    error "Unknown arch"
 #  endif
@@ -2373,7 +2431,7 @@
       || defined(VGP_s390x_linux) || defined(VGP_mips32_linux) \
       || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
       || defined(VGP_x86_solaris) || defined(VGP_amd64_solaris) \
-      || defined(VGP_nanomips_linux)
+      || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    return f;
 #  elif defined(VGP_ppc64be_linux)
    /* ppc64-linux uses the AIX scheme, in which f is a pointer to a
--- a/coregrind/m_main.c
+++ b/coregrind/m_main.c
@@ -2481,6 +2481,11 @@
    VG_TRACK(post_reg_write, Vg_CoreClientReq, tid,
             offsetof(VexGuestPPC64State, guest_GPR3),
             sizeof(VG_(threads)[tid].arch.vex.guest_GPR3));
+#  elif defined(VGA_riscv64)
+   VG_(threads)[tid].arch.vex.guest_x10 = to_run;
+   VG_TRACK(post_reg_write, Vg_CoreClientReq, tid,
+            offsetof(VexGuestRISCV64State, guest_x10),
+            sizeof(VG_(threads)[tid].arch.vex.guest_x10));
 #  elif defined(VGA_s390x)
    VG_(threads)[tid].arch.vex.guest_r2 = to_run;
    VG_TRACK(post_reg_write, Vg_CoreClientReq, tid,
@@ -3014,6 +3019,33 @@
     ".set pop                                           \n\t"
 ".previous                                              \n\t"
 );
+#elif defined(VGP_riscv64_linux)
+asm("\n"
+    "\t.text\n"
+    "\t.type _start,@function\n"
+    "\t.global _start\n"
+    "_start:\n"
+    /* establish the global pointer in gp */
+    ".option push\n"
+    ".option norelax\n"
+    "\tla gp, __global_pointer$\n"
+    ".option pop\n"
+    /* set up the new stack in t0 */
+    "\tla t0, vgPlain_interim_stack\n"
+    "\tli t1, "VG_STRINGIFY(VG_STACK_GUARD_SZB)"\n"
+    "\tadd t0, t0, t1\n"
+    "\tli t1, "VG_STRINGIFY(VG_DEFAULT_STACK_ACTIVE_SZB)"\n"
+    "\tadd t0, t0, t1\n"
+    "\tli t1, 0xFFFFFF00\n"
+    "\tand t0, t0, t1\n"
+    /* install it, and collect the original one */
+    "\tmv a0, sp\n"
+    "\tmv sp, t0\n"
+    /* call _start_in_C_linux, passing it the startup sp */
+    "\tj _start_in_C_linux\n"
+    "\tunimp\n"
+    ".previous\n"
+);
 #else
 #  error "Unknown platform"
 #endif
--- a/coregrind/m_options.c
+++ b/coregrind/m_options.c
@@ -194,7 +194,8 @@
 VgSmc VG_(clo_smc_check) = Vg_SmcAllNonFile;
 #elif defined(VGA_ppc32) || defined(VGA_ppc64be) || defined(VGA_ppc64le) \
       || defined(VGA_arm) || defined(VGA_arm64) \
-      || defined(VGA_mips32) || defined(VGA_mips64) || defined(VGA_nanomips)
+      || defined(VGA_mips32) || defined(VGA_mips64) || defined(VGA_nanomips) \
+      || defined(VGA_riscv64)
 VgSmc VG_(clo_smc_check) = Vg_SmcStack;
 #else
 #  error "Unknown arch"
--- a/coregrind/m_redir.c
+++ b/coregrind/m_redir.c
@@ -1651,6 +1651,9 @@
       );
    }
 
+#  elif defined(VGP_riscv64_linux)
+   /* No early intercepts are needed at this time. */
+
 #  elif defined(VGP_x86_solaris)
    /* If we're using memcheck, use these intercepts right from
       the start, otherwise ld.so makes a lot of noise. */
--- a/coregrind/m_scheduler/scheduler.c
+++ b/coregrind/m_scheduler/scheduler.c
@@ -896,6 +896,10 @@
 #  if defined(VGA_mips32) || defined(VGA_mips64)
    /* no special requirements */
 #  endif
+
+#  if defined(VGA_riscv64)
+   /* no special requirements */
+#  endif
 }
 
 // NO_VGDB_POLL value ensures vgdb is not polled, while
@@ -1010,6 +1014,8 @@
    tst->arch.vex.guest_LLaddr = (RegWord)(-1);
 #  elif defined(VGP_arm64_linux)
    tst->arch.vex.guest_LLSC_SIZE = 0;
+#  elif defined(VGP_riscv64_linux)
+   tst->arch.vex.guest_LLSC_SIZE = 0;
 #  endif
 
    if (0) {
@@ -1798,6 +1804,9 @@
 #elif defined(VGA_mips32) || defined(VGA_mips64) || defined(VGA_nanomips)
 #  define VG_CLREQ_ARGS       guest_r12
 #  define VG_CLREQ_RET        guest_r11
+#elif defined(VGA_riscv64)
+#  define VG_CLREQ_ARGS       guest_x14
+#  define VG_CLREQ_RET        guest_x13
 #else
 #  error Unknown arch
 #endif
--- /dev/null
+++ b/coregrind/m_sigframe/sigframe-riscv64-linux.c
@@ -0,0 +1,346 @@
+
+/*--------------------------------------------------------------------*/
+/*--- Create/destroy signal delivery frames.                       ---*/
+/*---                                     sigframe-riscv64-linux.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#if defined(VGP_riscv64_linux)
+
+#include "libvex_guest_offsets.h"
+#include "priv_sigframe.h"
+#include "pub_core_aspacemgr.h"
+#include "pub_core_basics.h"
+#include "pub_core_libcassert.h"
+#include "pub_core_libcbase.h"
+#include "pub_core_libcprint.h"
+#include "pub_core_machine.h"
+#include "pub_core_options.h"
+#include "pub_core_sigframe.h"
+#include "pub_core_signals.h"
+#include "pub_core_threadstate.h"
+#include "pub_core_tooliface.h"
+#include "pub_core_trampoline.h"
+#include "pub_core_vki.h"
+
+/*------------------------------------------------------------*/
+/*--- Signal frame layout                                  ---*/
+/*------------------------------------------------------------*/
+
+/* Valgrind-specific parts of the signal frame. */
+struct vg_sigframe {
+   /* Sanity check word. */
+   UInt magicPI;
+
+   /* Safely-saved version of sigNo. */
+   Int sigNo_private;
+
+   /* Sanity check word. */
+   UInt magicE;
+};
+
+/* Complete signal frame. */
+struct rt_sigframe {
+   struct vki_siginfo info;
+   struct vki_ucontext uc;
+   struct vg_sigframe vg;
+};
+
+/*------------------------------------------------------------*/
+/*--- Creating a signal frame                              ---*/
+/*------------------------------------------------------------*/
+
+static void synth_ucontext(ThreadState*         tst,
+                           const vki_siginfo_t* si,
+                           const vki_sigset_t*  set,
+                           struct vki_ucontext* uc)
+{
+   VG_(memset)(uc, 0, sizeof(*uc));
+
+   /* Prepare common data. */
+   uc->uc_flags = 0;
+   VG_TRACK(post_mem_write, Vg_CoreSignal, tst->tid, (Addr)&uc->uc_flags,
+            sizeof(uc->uc_flags));
+   uc->uc_link = 0;
+   VG_TRACK(post_mem_write, Vg_CoreSignal, tst->tid, (Addr)&uc->uc_link,
+            sizeof(uc->uc_link));
+   uc->uc_sigmask = *set;
+   VG_TRACK(post_mem_write, Vg_CoreSignal, tst->tid, (Addr)&uc->uc_sigmask,
+            sizeof(uc->uc_sigmask));
+   uc->uc_stack = tst->altstack;
+   VG_TRACK(post_mem_write, Vg_CoreSignal, tst->tid, (Addr)&uc->uc_stack,
+            sizeof(uc->uc_stack));
+
+   struct vki_sigcontext* sc = &uc->uc_mcontext;
+
+   /* Save integer registers. */
+#define IREG_TO_CTX(ureg, vreg)                                                \
+   sc->sc_regs.ureg = tst->arch.vex.guest_##vreg;                              \
+   VG_TRACK(copy_reg_to_mem, Vg_CoreSignal, tst->tid, OFFSET_riscv64_##vreg,   \
+            (Addr)&sc->sc_regs.ureg, sizeof(UWord));
+   IREG_TO_CTX(pc, pc);
+   IREG_TO_CTX(ra, x1);
+   IREG_TO_CTX(sp, x2);
+   IREG_TO_CTX(gp, x3);
+   IREG_TO_CTX(tp, x4);
+   IREG_TO_CTX(t0, x5);
+   IREG_TO_CTX(t1, x6);
+   IREG_TO_CTX(t2, x7);
+   IREG_TO_CTX(s0, x8);
+   IREG_TO_CTX(s1, x9);
+   IREG_TO_CTX(a0, x10);
+   IREG_TO_CTX(a1, x11);
+   IREG_TO_CTX(a2, x12);
+   IREG_TO_CTX(a3, x13);
+   IREG_TO_CTX(a4, x14);
+   IREG_TO_CTX(a5, x15);
+   IREG_TO_CTX(a6, x16);
+   IREG_TO_CTX(a7, x17);
+   IREG_TO_CTX(s2, x18);
+   IREG_TO_CTX(s3, x19);
+   IREG_TO_CTX(s4, x20);
+   IREG_TO_CTX(s5, x21);
+   IREG_TO_CTX(s6, x22);
+   IREG_TO_CTX(s7, x23);
+   IREG_TO_CTX(s8, x24);
+   IREG_TO_CTX(s9, x25);
+   IREG_TO_CTX(s10, x26);
+   IREG_TO_CTX(s11, x27);
+   IREG_TO_CTX(t3, x28);
+   IREG_TO_CTX(t4, x29);
+   IREG_TO_CTX(t5, x30);
+   IREG_TO_CTX(t6, x31);
+#undef IREG_TO_CTX
+
+   /* TODO Save floating point registers. */
+}
+
+/* Build the Valgrind-specific part of a signal frame. */
+static void build_vg_sigframe(struct vg_sigframe* frame, Int sigNo)
+{
+   frame->magicPI       = 0x31415927;
+   frame->sigNo_private = sigNo;
+   frame->magicE        = 0x27182818;
+}
+
+static Addr build_rt_sigframe(ThreadState*         tst,
+                              Addr                 sp_top_of_frame,
+                              const vki_siginfo_t* siginfo,
+                              UInt                 flags,
+                              const vki_sigset_t*  mask)
+{
+   SizeT size = sizeof(struct rt_sigframe);
+   Addr  sp   = VG_ROUNDDN(sp_top_of_frame - size, 16);
+
+   if (!ML_(sf_maybe_extend_stack)(tst, sp, size, flags))
+      return sp_top_of_frame;
+
+   /* Tell the tools that the sigframe is to be written. */
+   VG_TRACK(pre_mem_write, Vg_CoreSignal, tst->tid, "signal handler frame", sp,
+            sizeof(struct rt_sigframe));
+
+   struct rt_sigframe* frame = (struct rt_sigframe*)sp;
+
+   /* Fill in the siginfo. */
+   frame->info = *siginfo;
+
+   /* SIGILL defines addr to be the faulting address. */
+   Int sigNo = siginfo->si_signo;
+   if (sigNo == VKI_SIGILL && siginfo->si_code > 0)
+      frame->info._sifields._sigfault._addr = (void*)VG_(get_IP)(tst->tid);
+
+   VG_TRACK(post_mem_write, Vg_CoreSignal, tst->tid, (Addr)&frame->info,
+            sizeof(frame->info));
+
+   /* Fill in the ucontext. */
+   synth_ucontext(tst, siginfo, mask, &frame->uc);
+
+   /* Fill in the Valgrind-specific part. */
+   build_vg_sigframe(&frame->vg, sigNo);
+
+   return sp;
+}
+
+void VG_(sigframe_create)(ThreadId                   tid,
+                          Bool                       on_altstack,
+                          Addr                       rsp_top_of_frame,
+                          const vki_siginfo_t*       siginfo,
+                          const struct vki_ucontext* siguc,
+                          void*                      handler,
+                          UInt                       flags,
+                          const vki_sigset_t*        mask,
+                          void*                      restorer)
+{
+   /* The restorer functionality (SA_RESTORER) is not used on riscv64-linux. */
+   vg_assert(restorer == NULL);
+
+   ThreadState* tst = VG_(get_ThreadState)(tid);
+
+   /* Build the signal frame on the stack. */
+   Addr sp = build_rt_sigframe(tst, rsp_top_of_frame, siginfo, flags, mask);
+   struct rt_sigframe* frame = (struct rt_sigframe*)sp;
+
+   /* Configure guest registers for the signal delivery. */
+   VG_(set_SP)(tid, sp);
+   VG_TRACK(post_reg_write, Vg_CoreSignal, tid, VG_O_STACK_PTR, sizeof(UWord));
+
+   tst->arch.vex.guest_x10 = siginfo->si_signo;
+   VG_TRACK(post_reg_write, Vg_CoreSignal, tst->tid, OFFSET_riscv64_x10,
+            sizeof(UWord));
+   tst->arch.vex.guest_x11 = (Addr)&frame->info;
+   VG_TRACK(post_reg_write, Vg_CoreSignal, tst->tid, OFFSET_riscv64_x11,
+            sizeof(UWord));
+   tst->arch.vex.guest_x12 = (Addr)&frame->uc;
+   VG_TRACK(post_reg_write, Vg_CoreSignal, tst->tid, OFFSET_riscv64_x12,
+            sizeof(UWord));
+
+   tst->arch.vex.guest_x1 = (Addr)&VG_(riscv64_linux_SUBST_FOR_rt_sigreturn);
+   VG_TRACK(post_reg_write, Vg_CoreSignal, tst->tid, OFFSET_riscv64_x1,
+            sizeof(UWord));
+
+   /* Set up the program counter. Note that it is not necessary to inform the
+      tools about this write because pc is always defined. */
+   VG_(set_IP)(tid, (Addr)handler);
+
+   if (VG_(clo_trace_signals))
+      VG_(message)(Vg_DebugMsg,
+                   "sigframe_create (thread %u): next pc=%#lx, next sp=%#lx\n",
+                   tid, (Addr)handler, sp);
+}
+
+/*------------------------------------------------------------*/
+/*--- Destroying a signal frame                            ---*/
+/*------------------------------------------------------------*/
+
+/* Restore the Valgrind-specific part of a signal frame. The returned value
+   indicates whether the frame is valid. If not then nothing is restored and the
+   client is set to take a segfault. */
+static Bool
+restore_vg_sigframe(ThreadState* tst, struct vg_sigframe* frame, Int* sigNo)
+{
+   if (frame->magicPI != 0x31415927 || frame->magicE != 0x27182818) {
+      VG_(message)(Vg_UserMsg, "Thread %u return signal frame "
+                               "corrupted.  Killing process.\n",
+                   tst->tid);
+      VG_(set_default_handler)(VKI_SIGSEGV);
+      VG_(synth_fault)(tst->tid);
+      *sigNo = VKI_SIGSEGV;
+      return False;
+   }
+   *sigNo = frame->sigNo_private;
+   return True;
+}
+
+static void restore_ucontext(ThreadState* tst, struct vki_ucontext* uc)
+{
+   /* Restore common data. */
+   VG_TRACK(pre_mem_read, Vg_CoreSignal, tst->tid, "signal frame mask",
+            (Addr)&uc->uc_sigmask, sizeof(uc->uc_sigmask));
+   tst->sig_mask     = uc->uc_sigmask;
+   tst->tmp_sig_mask = tst->sig_mask;
+
+   struct vki_sigcontext* sc = &uc->uc_mcontext;
+
+   /* Restore integer registers. */
+#define IREG_FROM_CTX(ureg, vreg)                                              \
+   tst->arch.vex.guest_##vreg = sc->sc_regs.ureg;                              \
+   VG_TRACK(copy_mem_to_reg, Vg_CoreSignal, tst->tid, (Addr)&sc->sc_regs.ureg, \
+            OFFSET_riscv64_##vreg, sizeof(UWord));
+   IREG_FROM_CTX(pc, pc);
+   IREG_FROM_CTX(ra, x1);
+   IREG_FROM_CTX(sp, x2);
+   IREG_FROM_CTX(gp, x3);
+   IREG_FROM_CTX(tp, x4);
+   IREG_FROM_CTX(t0, x5);
+   IREG_FROM_CTX(t1, x6);
+   IREG_FROM_CTX(t2, x7);
+   IREG_FROM_CTX(s0, x8);
+   IREG_FROM_CTX(s1, x9);
+   IREG_FROM_CTX(a0, x10);
+   IREG_FROM_CTX(a1, x11);
+   IREG_FROM_CTX(a2, x12);
+   IREG_FROM_CTX(a3, x13);
+   IREG_FROM_CTX(a4, x14);
+   IREG_FROM_CTX(a5, x15);
+   IREG_FROM_CTX(a6, x16);
+   IREG_FROM_CTX(a7, x17);
+   IREG_FROM_CTX(s2, x18);
+   IREG_FROM_CTX(s3, x19);
+   IREG_FROM_CTX(s4, x20);
+   IREG_FROM_CTX(s5, x21);
+   IREG_FROM_CTX(s6, x22);
+   IREG_FROM_CTX(s7, x23);
+   IREG_FROM_CTX(s8, x24);
+   IREG_FROM_CTX(s9, x25);
+   IREG_FROM_CTX(s10, x26);
+   IREG_FROM_CTX(s11, x27);
+   IREG_FROM_CTX(t3, x28);
+   IREG_FROM_CTX(t4, x29);
+   IREG_FROM_CTX(t5, x30);
+   IREG_FROM_CTX(t6, x31);
+#undef IREG_FROM_CTX
+
+   /* TODO Restore floating point registers. */
+}
+
+static void
+restore_rt_sigframe(ThreadState* tst, struct rt_sigframe* frame, Int* sigNo)
+{
+   if (restore_vg_sigframe(tst, &frame->vg, sigNo))
+      restore_ucontext(tst, &frame->uc);
+}
+
+void VG_(sigframe_destroy)(ThreadId tid, Bool isRT)
+{
+   /* Non-rt sigreturn does not exist on riscv64-linux. */
+   vg_assert(isRT);
+
+   ThreadState* tst = VG_(get_ThreadState)(tid);
+
+   /* Correctly reestablish the frame base address. */
+   Addr sp = VG_(get_SP)(tid);
+
+   /* Restore a state from the signal frame. */
+   Int sigNo;
+   restore_rt_sigframe(tst, (struct rt_sigframe*)sp, &sigNo);
+
+   VG_TRACK(die_mem_stack_signal, sp - VG_STACK_REDZONE_SZB,
+            sizeof(struct rt_sigframe) + VG_STACK_REDZONE_SZB);
+
+   /* Returning from a signal handler. */
+   if (VG_(clo_trace_signals))
+      VG_(message)(Vg_DebugMsg,
+                   "sigframe_return (thread %u): pc=%#lx\n",
+                   tid, VG_(get_IP)(tid));
+
+   /* Tell the tools. */
+   VG_TRACK(post_deliver_signal, tid, sigNo);
+}
+
+#endif // defined(VGP_riscv64_linux)
+
+/*--------------------------------------------------------------------*/
+/*--- end                                 sigframe-riscv64-linux.c ---*/
+/*--------------------------------------------------------------------*/
--- a/coregrind/m_signals.c
+++ b/coregrind/m_signals.c
@@ -651,6 +651,20 @@
         (srP)->r_sp = (uc)->uc_mcontext.gregs[VKI_REG_RSP];                  \
         (srP)->misc.AMD64.r_rbp = (uc)->uc_mcontext.gregs[VKI_REG_RBP];      \
       }
+
+#elif defined(VGP_riscv64_linux)
+#  define VG_UCONTEXT_INSTR_PTR(uc)       ((uc)->uc_mcontext.sc_regs.pc)
+#  define VG_UCONTEXT_STACK_PTR(uc)       ((uc)->uc_mcontext.sc_regs.sp)
+#  define VG_UCONTEXT_SYSCALL_SYSRES(uc)                               \
+      /* Convert the value in uc_mcontext.sc_regs.a0 into a SysRes. */ \
+      VG_(mk_SysRes_riscv64_linux)( (uc)->uc_mcontext.sc_regs.a0 )
+#  define VG_UCONTEXT_TO_UnwindStartRegs(srP, uc)                \
+      { (srP)->r_pc = (uc)->uc_mcontext.sc_regs.pc;              \
+        (srP)->r_sp = (uc)->uc_mcontext.sc_regs.sp;              \
+        (srP)->misc.RISCV64.r_fp = (uc)->uc_mcontext.sc_regs.s0; \
+        (srP)->misc.RISCV64.r_ra = (uc)->uc_mcontext.sc_regs.ra; \
+      }
+
 #else
 #  error Unknown platform
 #endif
@@ -895,9 +909,10 @@
       if (skss_handler != VKI_SIG_IGN && skss_handler != VKI_SIG_DFL)
          skss_flags |= VKI_SA_SIGINFO;
 
+#     if !defined(VGP_riscv64_linux)
       /* use our own restorer */
       skss_flags |= VKI_SA_RESTORER;
-
+#     endif
       /* Create SKSS entry for this signal. */
       if (sig != VKI_SIGKILL && sig != VKI_SIGSTOP)
          dst->skss_per_sig[sig].skss_handler = skss_handler;
@@ -1048,6 +1063,16 @@
    "   li $t4, " #name "\n" \
    "   syscall[32]\n" \
    ".previous\n"
+
+#elif defined(VGP_riscv64_linux)
+/* Not used on riscv64. */
+#  define _MY_SIGRETURN(name) \
+   ".text\n" \
+   ".globl my_sigreturn\n" \
+   "my_sigreturn:\n" \
+   "   unimp\n" \
+   ".previous\n"
+
 #elif defined(VGP_x86_solaris) || defined(VGP_amd64_solaris)
 /* Not used on Solaris. */
 #  define _MY_SIGRETURN(name) \
@@ -1107,7 +1132,7 @@
       ksa.sa_flags    = skss.skss_per_sig[sig].skss_flags;
 #     if !defined(VGP_ppc32_linux) && \
          !defined(VGP_x86_darwin) && !defined(VGP_amd64_darwin) && \
-         !defined(VGP_mips32_linux) && !defined(VGO_solaris) && !defined(VGO_freebsd)
+         !defined(VGP_mips32_linux) && !defined(VGP_riscv64_linux) && !defined(VGO_solaris) && !defined(VGO_freebsd)
       ksa.sa_restorer = my_sigreturn;
 #     endif
       /* Re above ifdef (also the assertion below), PaulM says:
@@ -1155,7 +1180,7 @@
             !defined(VGP_x86_darwin) && !defined(VGP_amd64_darwin) && \
             !defined(VGP_mips32_linux) && !defined(VGP_mips64_linux) && \
             !defined(VGP_nanomips_linux) && !defined(VGO_solaris) && \
-            !defined(VGO_freebsd)
+            !defined(VGO_freebsd) && !defined(VGP_riscv64_linux)
          vg_assert(ksa_old.sa_restorer == my_sigreturn);
 #        endif
          VG_(sigaddset)( &ksa_old.sa_mask, VKI_SIGKILL );
@@ -1276,7 +1301,7 @@
       old_act->sa_flags    = scss.scss_per_sig[signo].scss_flags;
       old_act->sa_mask     = scss.scss_per_sig[signo].scss_mask;
 #     if !defined(VGO_darwin) && !defined(VGO_freebsd) && \
-         !defined(VGO_solaris)
+         !defined(VGO_solaris) && !defined(VGP_riscv64_linux)
       old_act->sa_restorer = scss.scss_per_sig[signo].scss_restorer;
 #     endif
    }
@@ -1289,7 +1314,7 @@
 
       scss.scss_per_sig[signo].scss_restorer = NULL;
 #     if !defined(VGO_darwin) && !defined(VGO_freebsd) && \
-         !defined(VGO_solaris)
+         !defined(VGO_solaris) && !defined(VGP_riscv64_linux)
       scss.scss_per_sig[signo].scss_restorer = new_act->sa_restorer;
 #     endif
 
@@ -3017,7 +3042,7 @@
                sa->ksa_handler, 
                (UInt)sa->sa_flags, 
 #              if !defined(VGO_darwin) && !defined(VGO_freebsd) && \
-                  !defined(VGO_solaris)
+                  !defined(VGO_solaris) && !defined(VGP_riscv64_linux)
                   sa->sa_restorer
 #              else
                   (void*)0
@@ -3040,7 +3065,7 @@
    sa.ksa_handler = VKI_SIG_DFL;
    sa.sa_flags = 0;
 #  if !defined(VGO_darwin) && !defined(VGO_freebsd) && \
-      !defined(VGO_solaris)
+      !defined(VGO_solaris) && !defined(VGP_riscv64_linux)
    sa.sa_restorer = 0;
 #  endif
    VG_(sigemptyset)(&sa.sa_mask);
@@ -3162,7 +3187,7 @@
 	 tsa.ksa_handler = (void *)sync_signalhandler;
 	 tsa.sa_flags = VKI_SA_SIGINFO;
 #        if !defined(VGO_darwin) && !defined(VGO_freebsd) && \
-            !defined(VGO_solaris)
+            !defined(VGO_solaris) && !defined(VGP_riscv64_linux)
 	 tsa.sa_restorer = 0;
 #        endif
 	 VG_(sigfillset)(&tsa.sa_mask);
@@ -3190,7 +3215,7 @@
 
       scss.scss_per_sig[i].scss_restorer = NULL;
 #     if !defined(VGO_darwin) && !defined(VGO_freebsd) && \
-         !defined(VGO_solaris)
+         !defined(VGO_solaris) && !defined(VGP_riscv64_linux)
       scss.scss_per_sig[i].scss_restorer = sa.sa_restorer;
 #     endif
 
--- a/coregrind/m_stacktrace.c
+++ b/coregrind/m_stacktrace.c
@@ -1502,6 +1502,84 @@
 
 #endif
 
+/* ------------------------ riscv64 ------------------------- */
+
+#if defined(VGP_riscv64_linux)
+
+UInt VG_(get_StackTrace_wrk) ( ThreadId tid_if_known,
+                               /*OUT*/Addr* ips, UInt max_n_ips,
+                               /*OUT*/Addr* sps, /*OUT*/Addr* fps,
+                               const UnwindStartRegs* startRegs,
+                               Addr fp_max_orig )
+{
+   Bool  debug = False;
+   Int   i;
+   Addr  fp_max;
+   UInt  n_found = 0;
+   const Int cmrf = VG_(clo_merge_recursive_frames);
+
+   vg_assert(sizeof(Addr) == sizeof(UWord));
+   vg_assert(sizeof(Addr) == sizeof(void*));
+
+   D3UnwindRegs uregs;
+   uregs.pc = startRegs->r_pc;
+   uregs.sp = startRegs->r_sp;
+   uregs.fp = startRegs->misc.RISCV64.r_fp;
+   uregs.ra = startRegs->misc.RISCV64.r_ra;
+   Addr fp_min = uregs.sp - VG_STACK_REDZONE_SZB;
+
+   /* Snaffle IPs from the client's stack into ips[0 .. max_n_ips-1],
+      stopping when the trail goes cold, which we guess to be
+      when FP is not a reasonable stack location. */
+
+   /* TODO Is this needed? It shouldn't. */
+   /* fp_max = VG_PGROUNDUP(fp_max_orig); */
+   fp_max = fp_max_orig;
+   if (fp_max >= sizeof(Addr))
+      fp_max -= sizeof(Addr);
+
+   if (debug)
+      VG_(printf)("\nmax_n_ips=%u fp_min=0x%lx fp_max_orig=0x%lx, "
+                  "fp_max=0x%lx pc=0x%lx sp=0x%lx fp=0x%lx ra=0x%lx\n",
+                  max_n_ips, fp_min, fp_max_orig, fp_max,
+                  uregs.pc, uregs.sp, uregs.fp, uregs.ra);
+
+   if (sps) sps[0] = uregs.sp;
+   if (fps) fps[0] = uregs.fp;
+   ips[0] = uregs.pc;
+   i = 1;
+
+   /* Loop unwinding the stack, using CFI. */
+   while (True) {
+      if (debug)
+         VG_(printf)("i: %d, pc: 0x%lx, sp: 0x%lx, fp: 0x%lx, ra: 0x%lx\n",
+                     i, uregs.pc, uregs.sp, uregs.fp, uregs.ra);
+      if (i >= max_n_ips)
+         break;
+
+      if (VG_(use_CF_info)( &uregs, fp_min, fp_max )) {
+         if (sps) sps[i] = uregs.sp;
+         if (fps) fps[i] = uregs.fp;
+         ips[i++] = uregs.pc - 1;
+         if (debug)
+            VG_(printf)(
+               "USING CFI: pc: 0x%lx, sp: 0x%lx, fp: 0x%lx, ra: 0x%lx\n",
+               uregs.pc, uregs.sp, uregs.fp, uregs.ra);
+         uregs.pc = uregs.pc - 1;
+         RECURSIVE_MERGE(cmrf,ips,i);
+         continue;
+      }
+
+      /* No luck.  We have to give up. */
+      break;
+   }
+
+   n_found = i;
+   return n_found;
+}
+
+#endif
+
 /*------------------------------------------------------------*/
 /*---                                                      ---*/
 /*--- END platform-dependent unwinder worker functions     ---*/
--- a/coregrind/m_syscall.c
+++ b/coregrind/m_syscall.c
@@ -204,6 +204,17 @@
    return res;
 }
 
+SysRes VG_(mk_SysRes_riscv64_linux) ( Long val ) {
+   SysRes res;
+   res._isError = val >= -4095 && val <= -1;
+   if (res._isError) {
+      res._val = (ULong)(-val);
+   } else {
+      res._val = (ULong)val;
+   }
+   return res;
+}
+
 /* Generic constructors. */
 SysRes VG_(mk_SysRes_Success) ( UWord res ) {
    SysRes r;
@@ -1032,6 +1043,29 @@
    ".previous                              \n\t"
 );
 
+#elif defined(VGP_riscv64_linux)
+/* Calling convention is: args in a0-a5, sysno in a7, return value in a0.
+   Return value follows the usual convention that -4095 .. -1 (both inclusive)
+   is an error value. All other values are success values.
+   Registers a0 to a5 remain unchanged, but syscall_no is in a6 and needs to be
+   moved to a7.
+*/
+extern UWord do_syscall_WRK (
+          UWord a1, UWord a2, UWord a3,
+          UWord a4, UWord a5, UWord a6,
+          UWord syscall_no
+       );
+asm(
+".text\n"
+".globl do_syscall_WRK\n"
+"do_syscall_WRK:\n"
+"        mv a7, a6\n"
+"        li a6, 0\n"
+"        ecall\n"
+"        ret\n"
+".previous\n"
+);
+
 #elif defined(VGP_x86_solaris)
 
 extern ULong
@@ -1272,6 +1306,10 @@
    do_syscall_WRK(a1, a2, a3, a4, a5, a6, sysno, &reg_a0);
    return VG_(mk_SysRes_nanomips_linux)(reg_a0);
 
+#  elif defined(VGP_riscv64_linux)
+   UWord val = do_syscall_WRK(a1, a2, a3, a4, a5, a6, sysno);
+   return VG_(mk_SysRes_riscv64_linux)(val);
+
 #  elif defined(VGP_x86_solaris)
    UInt val, val2, err = False;
    Bool restart;
--- a/coregrind/m_syswrap/priv_syswrap-linux.h
+++ b/coregrind/m_syswrap/priv_syswrap-linux.h
@@ -501,6 +501,13 @@
                                               Int*  child_tid,      /* a4 - 8 */
                                               Int*  parent_tid,     /* a5 - 9 */
                                               void* tls_ptr);       /* a6 - 10 */
+extern UInt do_syscall_clone_riscv64_linux ( Word (*fn) (void *),
+                                             void* stack,
+                                             Int   flags,
+                                             void* arg,
+                                             Int*  child_tid,
+                                             Int*  parent_tid,
+                                             void* tls_ptr);
 #endif   // __PRIV_SYSWRAP_LINUX_H
 
 /*--------------------------------------------------------------------*/
--- a/coregrind/m_syswrap/priv_types_n_macros.h
+++ b/coregrind/m_syswrap/priv_types_n_macros.h
@@ -94,7 +94,7 @@
          || defined(VGP_ppc32_linux) \
          || defined(VGP_arm_linux) || defined(VGP_s390x_linux) \
          || defined(VGP_arm64_linux) \
-         || defined(VGP_nanomips_linux)
+         || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
       Int o_arg1;
       Int o_arg2;
       Int o_arg3;
--- /dev/null
+++ b/coregrind/m_syswrap/syscall-riscv64-linux.S
@@ -0,0 +1,198 @@
+
+/*--------------------------------------------------------------------*/
+/*--- Support for doing system calls.      syscall-riscv64-linux.S ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+  This file is part of Valgrind, a dynamic binary instrumentation
+  framework.
+
+  Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+  This program is free software; you can redistribute it and/or
+  modify it under the terms of the GNU General Public License as
+  published by the Free Software Foundation; either version 2 of the
+  License, or (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+  The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "pub_core_basics_asm.h"
+
+#if defined(VGP_riscv64_linux)
+
+#include "pub_core_vkiscnums_asm.h"
+#include "libvex_guest_offsets.h"
+
+
+/*----------------------------------------------------------------*/
+/*
+        Perform a syscall for the client.  This will run a syscall
+        with the client's specific per-thread signal mask.
+
+        The structure of this function is such that, if the syscall is
+        interrupted by a signal, we can determine exactly what
+        execution state we were in with respect to the execution of
+        the syscall by examining the value of pc in the signal
+        handler.  This means that we can always do the appropriate
+        thing to precisely emulate the kernel's signal/syscall
+        interactions.
+
+        The syscall number is taken from the argument, even though it
+        should also be in guest_state->guest_x17.  The syscall result
+	is written back to guest_state->guest_x10 on completion.
+
+        Returns 0 if the syscall was successfully called (even if the
+        syscall itself failed), or a nonzero error code in the lowest
+        8 bits if one of the sigprocmasks failed (there's no way to
+        determine which one failed).  And there's no obvious way to
+        recover from that either, but nevertheless we want to know.
+
+        VG_(fixup_guest_state_after_syscall_interrupted) does the
+        thread state fixup in the case where we were interrupted by a
+        signal.
+
+        Prototype:
+
+   UWord ML_(do_syscall_for_client_WRK)(
+              Int syscallno,                 // a0
+              void* guest_state,             // a1
+              const vki_sigset_t *sysmask,   // a2
+              const vki_sigset_t *postmask,  // a3
+              Int nsigwords)                 // a4
+*/
+/* from vki-riscv64-linux.h */
+#define VKI_SIG_SETMASK 2
+
+.globl ML_(do_syscall_for_client_WRK)
+ML_(do_syscall_for_client_WRK):
+
+   /* Stash callee-saves and our args on the stack */
+   addi sp, sp, -144
+   sd ra, 136(sp)
+   sd s0, 128(sp)
+   sd s1, 120(sp)
+   sd s2, 112(sp)
+   sd s3, 104(sp)
+   sd s4, 96(sp)
+   sd s5, 88(sp)
+   sd s6, 80(sp)
+   sd s7, 72(sp)
+   sd s8, 64(sp)
+   sd s9, 56(sp)
+   sd s10, 48(sp)
+   sd s11, 40(sp)
+   sd a0, 32(sp)
+   sd a1, 24(sp)
+   sd a2, 16(sp)
+   sd a3, 8(sp)
+   sd a4, 0(sp)
+
+1:
+
+   li a7, __NR_rt_sigprocmask
+   li a0, VKI_SIG_SETMASK
+   mv a1, a2 /* sysmask */
+   mv a2, a3 /* postmask */
+   mv a3, a4 /* nsigwords */
+   ecall
+
+
+   ld a5, 24(sp) /* saved a1 == guest_state */
+
+   ld a7, 32(sp) /* saved a0 == syscall# */
+   ld a0, OFFSET_riscv64_x10(a5)
+   ld a1, OFFSET_riscv64_x11(a5)
+   ld a2, OFFSET_riscv64_x12(a5)
+   ld a3, OFFSET_riscv64_x13(a5)
+   ld a4, OFFSET_riscv64_x14(a5)
+   ld a5, OFFSET_riscv64_x15(a5)
+
+2: ecall
+3:
+   ld a5, 24(sp) /* saved a1 == guest_state */
+   sd a0, OFFSET_riscv64_x10(a5)
+
+4:
+   li a7, __NR_rt_sigprocmask
+   li a0, VKI_SIG_SETMASK
+   ld a1, 8(sp) /* saved a3 == postmask */
+   li a2, 0
+   ld a3, 0(sp) /* saved a4 == nsigwords */
+   ecall
+
+   bltz x0, 7f
+
+5: /* Success: return zero */
+   li a0, 0
+   ld ra, 136(sp)
+   ld s0, 128(sp)
+   ld s1, 120(sp)
+   ld s2, 112(sp)
+   ld s3, 104(sp)
+   ld s4, 96(sp)
+   ld s5, 88(sp)
+   ld s6, 80(sp)
+   ld s7, 72(sp)
+   ld s8, 64(sp)
+   ld s9, 56(sp)
+   ld s10, 48(sp)
+   ld s11, 40(sp)
+   addi sp, sp, 144
+   ret
+
+7: /* Failure: return 0x8000 | error code */
+   li a1, 0x8000
+   or a0, a0, a1
+   ld ra, 136(sp)
+   ld s0, 128(sp)
+   ld s1, 120(sp)
+   ld s2, 112(sp)
+   ld s3, 104(sp)
+   ld s4, 96(sp)
+   ld s5, 88(sp)
+   ld s6, 80(sp)
+   ld s7, 72(sp)
+   ld s8, 64(sp)
+   ld s9, 56(sp)
+   ld s10, 48(sp)
+   ld s11, 40(sp)
+   addi sp, sp, 144
+   ret
+
+
+
+.section .rodata
+/* export the ranges so that
+   VG_(fixup_guest_state_after_syscall_interrupted) can do the
+   right thing */
+
+.align 3
+.globl ML_(blksys_setup)
+.globl ML_(blksys_restart)
+.globl ML_(blksys_complete)
+.globl ML_(blksys_committed)
+.globl ML_(blksys_finished)
+ML_(blksys_setup):      .quad 1b
+ML_(blksys_restart):    .quad 2b
+ML_(blksys_complete):   .quad 3b
+ML_(blksys_committed):  .quad 4b
+ML_(blksys_finished):   .quad 5b
+
+#endif // defined(VGP_riscv64_linux)
+
+/* Let the linker know we don't need an executable stack */
+MARK_STACK_NO_EXEC
+
+/*--------------------------------------------------------------------*/
+/*--- end                                                          ---*/
+/*--------------------------------------------------------------------*/
--- a/coregrind/m_syswrap/syswrap-generic.c
+++ b/coregrind/m_syswrap/syswrap-generic.c
@@ -3419,7 +3419,7 @@
 #endif
 
 #if !defined(VGO_solaris) && !defined(VGP_arm64_linux) && \
-    !defined(VGP_nanomips_linux)
+    !defined(VGP_nanomips_linux) && !defined(VGP_riscv64_linux)
 static vki_sigset_t fork_saved_mask;
 
 // In Linux, the sys_fork() function varies across architectures, but we
@@ -3470,7 +3470,7 @@
       VG_(sigprocmask)(VKI_SIG_SETMASK, &fork_saved_mask, NULL);
    }
 }
-#endif // !defined(VGO_solaris) && !defined(VGP_arm64_linux)
+#endif // !defined(VGO_solaris) && !defined(VGP_arm64_linux) && !defined(VGP_riscv64_linux)
 
 PRE(sys_ftruncate)
 {
--- a/coregrind/m_syswrap/syswrap-linux.c
+++ b/coregrind/m_syswrap/syswrap-linux.c
@@ -309,6 +309,18 @@
          : "r" (VgTs_Empty), "n" (__NR_exit), "m" (tst->os_state.exitcode)
          : "memory" , "$t4", "$a0"
       );
+#elif defined(VGP_riscv64_linux)
+      /* TODO Isn't this and other implementations racy because they load
+         tst->os_state.exitcode after setting VgTs_Empty? */
+      asm volatile (
+         "sw   %1, %0\n"      /* set tst->status = VgTs_Empty */
+         "li   a7, %2\n"      /* set a7 = __NR_exit */
+         "ld   a0, %3\n"      /* set a0 = tst->os_state.exitcode */
+         "ecall\n"            /* exit(tst->os_state.exitcode) */
+         : "=m" (tst->status)
+         : "r" (VgTs_Empty), "n" (__NR_exit), "m" (tst->os_state.exitcode)
+         : "a7", "a0"
+      );
 #else
 # error Unknown platform
 #endif
@@ -534,6 +546,13 @@
       (ML_(start_thread_NORETURN), stack, flags, ctst,
        child_tidptr, parent_tidptr, NULL);
    res = VG_ (mk_SysRes_nanomips_linux) (ret);
+#elif defined(VGP_riscv64_linux)
+   ULong a0;
+   ctst->arch.vex.guest_x10 = 0;
+   a0 = do_syscall_clone_riscv64_linux
+      (ML_(start_thread_NORETURN), stack, flags, ctst,
+       child_tidptr, parent_tidptr, NULL);
+   res = VG_(mk_SysRes_riscv64_linux)( a0 );
 #else
 # error Unknown platform
 #endif
@@ -596,6 +615,8 @@
 #elif defined(VGP_mips32_linux) || defined(VGP_nanomips_linux)
    ctst->arch.vex.guest_ULR = tlsaddr;
    ctst->arch.vex.guest_r27 = tlsaddr;
+#elif defined(VGP_riscv64_linux)
+   ctst->arch.vex.guest_x4 = tlsaddr;
 #else
 # error Unknown platform
 #endif
@@ -754,7 +775,7 @@
     || defined(VGP_ppc64be_linux) || defined(VGP_ppc64le_linux)	\
     || defined(VGP_arm_linux) || defined(VGP_mips32_linux) \
     || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
-    || defined(VGP_nanomips_linux)
+    || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
    res = VG_(do_syscall5)( __NR_clone, flags, 
                            (UWord)NULL, (UWord)parent_tidptr, 
                            (UWord)NULL, (UWord)child_tidptr );
@@ -822,7 +843,7 @@
     || defined(VGP_ppc64be_linux) || defined(VGP_ppc64le_linux)	\
     || defined(VGP_arm_linux) || defined(VGP_mips32_linux) \
     || defined(VGP_mips64_linux) || defined(VGP_arm64_linux) \
-    || defined(VGP_nanomips_linux)
+    || defined(VGP_nanomips_linux) || defined(VGP_riscv64_linux)
 #define ARG_CHILD_TIDPTR ARG5
 #define PRA_CHILD_TIDPTR PRA5
 #define ARG_TLS          ARG4
@@ -4208,6 +4229,9 @@
    sig* wrappers
    ------------------------------------------------------------------ */
 
+/* TODO Review which Linux platforms actually have this syscall and invert the
+   condition. */
+#if !defined(VGP_riscv64_linux)
 PRE(sys_sigpending)
 {
    PRINT( "sys_sigpending ( %#" FMT_REGWORD "x )", ARG1 );
@@ -4218,6 +4242,7 @@
 {
    POST_MEM_WRITE( ARG1, sizeof(vki_old_sigset_t) ) ;
 }
+#endif
 
 // This syscall is not used on amd64/Linux -- it only provides
 // sys_rt_sigprocmask, which uses sigset_t rather than old_sigset_t.
@@ -4295,9 +4320,13 @@
       PRE_MEM_READ( "sigaction(act->sa_handler)", (Addr)&sa->ksa_handler, sizeof(sa->ksa_handler));
       PRE_MEM_READ( "sigaction(act->sa_mask)", (Addr)&sa->sa_mask, sizeof(sa->sa_mask));
       PRE_MEM_READ( "sigaction(act->sa_flags)", (Addr)&sa->sa_flags, sizeof(sa->sa_flags));
+#     if !defined(VGP_riscv64_linux)
+      /* Check the sa_restorer field. More recent Linux platforms completely
+         drop this member. */
       if (ML_(safe_to_deref)(sa,sizeof(struct vki_old_sigaction))
           && (sa->sa_flags & VKI_SA_RESTORER))
          PRE_MEM_READ( "sigaction(act->sa_restorer)", (Addr)&sa->sa_restorer, sizeof(sa->sa_restorer));
+#     endif
    }
 
    if (ARG3 != 0) {
@@ -4413,9 +4442,11 @@
       PRE_MEM_READ( "rt_sigaction(act->sa_handler)", (Addr)&sa->ksa_handler, sizeof(sa->ksa_handler));
       PRE_MEM_READ( "rt_sigaction(act->sa_mask)", (Addr)&sa->sa_mask, sizeof(sa->sa_mask));
       PRE_MEM_READ( "rt_sigaction(act->sa_flags)", (Addr)&sa->sa_flags, sizeof(sa->sa_flags));
+#     if !defined(VGP_riscv64_linux)
       if (ML_(safe_to_deref)(sa,sizeof(vki_sigaction_toK_t))
           && (sa->sa_flags & VKI_SA_RESTORER))
          PRE_MEM_READ( "rt_sigaction(act->sa_restorer)", (Addr)&sa->sa_restorer, sizeof(sa->sa_restorer));
+#     endif
    }
    if (ARG3 != 0)
       PRE_MEM_WRITE( "rt_sigaction(oldact)", ARG3, sizeof(vki_sigaction_fromK_t));
--- a/coregrind/m_syswrap/syswrap-main.c
+++ b/coregrind/m_syswrap/syswrap-main.c
@@ -834,6 +834,18 @@
    canonical->arg7  = 0;
    canonical->arg8  = 0;
 
+#elif defined(VGP_riscv64_linux)
+   VexGuestRISCV64State* gst = (VexGuestRISCV64State*)gst_vanilla;
+   canonical->sysno = gst->guest_x17; /* a7 */
+   canonical->arg1  = gst->guest_x10; /* a0 */
+   canonical->arg2  = gst->guest_x11; /* a1 */
+   canonical->arg3  = gst->guest_x12; /* a2 */
+   canonical->arg4  = gst->guest_x13; /* a3 */
+   canonical->arg5  = gst->guest_x14; /* a4 */
+   canonical->arg6  = gst->guest_x15; /* a5 */
+   canonical->arg7  = 0;
+   canonical->arg8  = 0;
+
 #elif defined(VGP_x86_solaris)
    VexGuestX86State* gst = (VexGuestX86State*)gst_vanilla;
    UWord *stack = (UWord *)gst->guest_ESP;
@@ -1126,6 +1138,16 @@
    gst->guest_r10 = canonical->arg7;
    gst->guest_r11 = canonical->arg8;
 
+#elif defined(VGP_riscv64_linux)
+   VexGuestRISCV64State* gst = (VexGuestRISCV64State*)gst_vanilla;
+   gst->guest_x17 = canonical->sysno; /* a7 */
+   gst->guest_x10 = canonical->arg1;  /* a0 */
+   gst->guest_x11 = canonical->arg2;  /* a1 */
+   gst->guest_x12 = canonical->arg3;  /* a2 */
+   gst->guest_x13 = canonical->arg4;  /* a3 */
+   gst->guest_x14 = canonical->arg5;  /* a4 */
+   gst->guest_x15 = canonical->arg6;  /* a5 */
+
 #elif defined(VGP_x86_solaris)
    VexGuestX86State* gst = (VexGuestX86State*)gst_vanilla;
    UWord *stack = (UWord *)gst->guest_ESP;
@@ -1317,6 +1339,11 @@
    canonical->sres = VG_(mk_SysRes_s390x_linux)( gst->guest_r2 );
    canonical->what = SsComplete;
 
+#  elif defined(VGP_riscv64_linux)
+   VexGuestRISCV64State* gst = (VexGuestRISCV64State*)gst_vanilla;
+   canonical->sres = VG_(mk_SysRes_riscv64_linux)( gst->guest_x10 );
+   canonical->what = SsComplete;
+
 #  elif defined(VGP_x86_solaris)
    VexGuestX86State* gst = (VexGuestX86State*)gst_vanilla;
    UInt carry = 1 & LibVEX_GuestX86_get_eflags(gst);
@@ -1606,6 +1633,20 @@
    VG_TRACK( post_reg_write, Vg_CoreSysCall, tid,
              OFFSET_mips32_r4, sizeof(UWord) );
 
+#  elif defined(VGP_riscv64_linux)
+   VexGuestRISCV64State* gst = (VexGuestRISCV64State*)gst_vanilla;
+   vg_assert(canonical->what == SsComplete);
+   if (sr_isError(canonical->sres)) {
+      /* This isn't exactly right, in that really a Failure with res
+         not in the range 1 .. 4095 is unrepresentable in the
+         Linux-riscv64 scheme.  Oh well. */
+      gst->guest_x10 = - (Long)sr_Err(canonical->sres);
+   } else {
+      gst->guest_x10 = sr_Res(canonical->sres);
+   }
+   VG_TRACK( post_reg_write, Vg_CoreSysCall, tid,
+             OFFSET_riscv64_x10, sizeof(UWord) );
+
 #  elif defined(VGP_x86_solaris)
    VexGuestX86State* gst = (VexGuestX86State*)gst_vanilla;
    SysRes sres = canonical->sres;
@@ -1832,6 +1873,17 @@
    layout->uu_arg7  = -1; /* impossible value */
    layout->uu_arg8  = -1; /* impossible value */
 
+#elif defined(VGP_riscv64_linux)
+   layout->o_sysno  = OFFSET_riscv64_x17; /* a7 */
+   layout->o_arg1   = OFFSET_riscv64_x10; /* a0 */
+   layout->o_arg2   = OFFSET_riscv64_x11; /* a1 */
+   layout->o_arg3   = OFFSET_riscv64_x12; /* a2 */
+   layout->o_arg4   = OFFSET_riscv64_x13; /* a3 */
+   layout->o_arg5   = OFFSET_riscv64_x14; /* a4 */
+   layout->o_arg6   = OFFSET_riscv64_x15; /* a5 */
+   layout->uu_arg7  = -1; /* impossible value */
+   layout->uu_arg8  = -1; /* impossible value */
+
 #elif defined(VGP_x86_solaris)
    layout->o_sysno  = OFFSET_x86_EAX;
    /* Syscall parameters are on the stack. */
@@ -2893,6 +2945,25 @@
          arch->vex.guest_PC -= 2;
       }
    }
+#elif defined(VGP_riscv64_linux)
+   arch->vex.guest_pc -= 4;             // sizeof(ecall)
+
+   /* Make sure our caller is actually sane, and we're really backing
+      back over a syscall.
+      ecall == 73 00 00 00
+   */
+   {
+      UChar *p = (UChar *)arch->vex.guest_pc;
+
+      if (p[0] != 0x73 || p[1] != 0x00 || p[2] != 0x00 || p[3] != 0x00)
+         VG_(message)(
+            Vg_DebugMsg,
+            "?! restarting over syscall at %#llx %02x %02x %02x %02x\n",
+            arch->vex.guest_pc, p[0], p[1], p[2], p[3]
+          );
+
+      vg_assert(p[0] == 0x73 && p[1] == 0x00 && p[2] == 0x00 && p[3] == 0x00);
+   }
 #elif defined(VGP_x86_solaris)
    arch->vex.guest_EIP -= 2;   // sizeof(int $0x91) or sizeof(syscall)
 
--- /dev/null
+++ b/coregrind/m_syswrap/syswrap-riscv64-linux.c
@@ -0,0 +1,400 @@
+
+/*--------------------------------------------------------------------*/
+/*--- Platform-specific syscalls stuff.  syswrap-riscv64-linux.c -----*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#if defined(VGP_riscv64_linux)
+
+#include "pub_core_basics.h"
+#include "pub_core_vki.h"
+#include "pub_core_vkiscnums.h"
+#include "pub_core_threadstate.h"
+#include "pub_core_aspacemgr.h"
+#include "pub_core_libcbase.h"
+#include "pub_core_libcassert.h"
+#include "pub_core_libcprint.h"
+#include "pub_core_libcsignal.h"
+#include "pub_core_options.h"
+#include "pub_core_scheduler.h"
+#include "pub_core_sigframe.h"      // For VG_(sigframe_destroy)()
+#include "pub_core_syscall.h"
+#include "pub_core_syswrap.h"
+#include "pub_core_tooliface.h"
+#include "pub_core_transtab.h"      // VG_(discard_translations)
+
+#include "priv_types_n_macros.h"
+#include "priv_syswrap-generic.h"   /* for decls of generic wrappers */
+#include "priv_syswrap-linux.h"     /* for decls of linux-ish wrappers */
+/* TODO Review includes. */
+
+
+/* ---------------------------------------------------------------------
+   clone() handling
+   ------------------------------------------------------------------ */
+
+/* Call f(arg1), but first switch stacks, using 'stack' as the new
+   stack, and use 'retaddr' as f's return-to address.  Also, clear all
+   the integer registers before entering f.*/
+__attribute__((noreturn))
+void ML_(call_on_new_stack_0_1) ( Addr stack,
+                                  Addr retaddr,
+                                  void (*f)(Word),
+                                  Word arg1 );
+//    a0 = stack
+//    a1 = retaddr
+//    a2 = f
+//    a3 = arg1
+asm(
+".text\n"
+".globl vgModuleLocal_call_on_new_stack_0_1\n"
+"vgModuleLocal_call_on_new_stack_0_1:\n"
+"   mv    sp, a0\n\t" /* Stack pointer */
+"   mv    ra, a1\n\t" /* Return address */
+"   mv    a0, a3\n\t" /* First argument */
+"   li    t0, 0\n\t"  /* Clear our GPRs */
+"   li    t1, 0\n\t"
+"   li    t2, 0\n\t"
+"   li    s0, 0\n\t"
+"   li    s1, 0\n\t"
+/* don't zero out a0, already set to the first argument */
+"   li    a1, 0\n\t"
+/* don't zero out a2, holds the target function f() */
+"   li    a3, 0\n\t"
+"   li    a4, 0\n\t"
+"   li    a5, 0\n\t"
+"   li    a6, 0\n\t"
+"   li    a7, 0\n\t"
+"   li    s2, 0\n\t"
+"   li    s3, 0\n\t"
+"   li    s4, 0\n\t"
+"   li    s5, 0\n\t"
+"   li    s6, 0\n\t"
+"   li    s7, 0\n\t"
+"   li    s8, 0\n\t"
+"   li    s9, 0\n\t"
+"   li    s10, 0\n\t"
+"   li    s11, 0\n\t"
+"   li    t3, 0\n\t"
+"   li    t4, 0\n\t"
+"   li    t5, 0\n\t"
+"   li    t6, 0\n\t"
+"   jr    a2\n\t"
+".previous\n"
+);
+
+/* Perform a clone system call. Clone is strange because it has fork()-like
+   return-twice semantics, so it needs special handling here.
+
+   Upon entry, we have:
+
+      Word (*fn)(void*)   in a0
+      void*  child_stack  in a1
+      int    flags        in a2
+      void*  arg          in a3
+      pid_t* child_tid    in a4
+      pid_t* parent_tid   in a5
+      void*  tls_ptr      in a6
+
+   System call requires:
+
+      int    $__NR_clone  in a7
+      int    flags        in a0
+      void*  child_stack  in a1
+      pid_t* parent_tid   in a2
+      void*  tls_ptr      in a3
+      pid_t* child_tid    in a4
+
+   Returns a Long encoded in the linux-riscv64 way, not a SysRes.
+*/
+#define __NR_CLONE VG_STRINGIFY(__NR_clone)
+#define __NR_EXIT  VG_STRINGIFY(__NR_exit)
+
+/* See priv_syswrap-linux.h for arg profile. */
+asm(
+".text\n"
+".globl do_syscall_clone_riscv64_linux\n"
+"do_syscall_clone_riscv64_linux:\n"
+        // set up child stack, temporarily preserving fn and arg
+"       addi   a1, a1, -16\n"       // make space on stack
+"       sd     a3, 8(a1)\n"         // save arg
+"       sd     a0, 0(a1)\n"         // save fn
+
+        // setup syscall
+"       li     a7, "__NR_CLONE"\n"  // syscall number
+"       mv     a0, a2\n"            // syscall arg1: flags
+"       mv     a1, a1\n"            // syscall arg2: child_stack
+"       mv     a2, a5\n"            // syscall arg3: parent_tid
+"       mv     a3, a6\n"            // syscall arg4: tls_ptr
+"       mv     a4, a4\n"            // syscall arg5: child_tid
+
+"       ecall\n"                    // clone()
+
+"       bnez   a0, 1f\n"            // child if retval == 0
+
+        // CHILD - call thread function
+"       ld     a1, 0(sp)\n"         // pop fn
+"       ld     a0, 8(sp)\n"         // pop fn arg1: arg
+"       addi   sp, sp, 16\n"
+"       jalr   a1\n"                // call fn
+
+        // exit with result
+"       mv     a0, a0\n"            // arg1: return value from fn
+"       li     a7, "__NR_EXIT"\n"
+
+"       ecall\n"
+
+        // Exit returned?!
+"       unimp\n"
+
+"1:\n"  // PARENT or ERROR.  a0 holds return value from the clone syscall.
+"       ret\n"
+".previous\n"
+);
+
+#undef __NR_CLONE
+#undef __NR_EXIT
+
+/* ---------------------------------------------------------------------
+   More thread stuff
+   ------------------------------------------------------------------ */
+
+/* riscv64 doesn't have any architecture specific thread stuff that needs to be
+   cleaned up. */
+void VG_(cleanup_thread) ( ThreadArchState* arch )
+{
+}
+
+/* ---------------------------------------------------------------------
+   PRE/POST wrappers for riscv64/Linux-specific syscalls
+   ------------------------------------------------------------------ */
+
+#define PRE(name)  DEFN_PRE_TEMPLATE(riscv64_linux, name)
+#define POST(name) DEFN_POST_TEMPLATE(riscv64_linux, name)
+
+/* Add prototypes for the wrappers declared here, so that gcc doesn't
+   harass us for not having prototypes.  Really this is a kludge --
+   the right thing to do is to make these wrappers 'static' since they
+   aren't visible outside this file, but that requires even more macro
+   magic. */
+
+DECL_TEMPLATE(riscv64_linux, sys_rt_sigreturn);
+DECL_TEMPLATE(riscv64_linux, sys_mmap);
+DECL_TEMPLATE(riscv64_linux, sys_riscv_flush_icache);
+
+PRE(sys_rt_sigreturn)
+{
+   /* See comments on PRE(sys_rt_sigreturn) in syswrap-amd64-linux.c for
+      an explanation of what follows. */
+
+   PRINT("rt_sigreturn ( )");
+
+   vg_assert(VG_(is_valid_tid)(tid));
+   vg_assert(tid >= 1 && tid < VG_N_THREADS);
+   vg_assert(VG_(is_running_thread)(tid));
+
+   /* Restore register state from frame and remove it. */
+   VG_(sigframe_destroy)(tid, True);
+
+   /* Tell the driver not to update the guest state with the "result", and set
+      a bogus result to keep it happy. */
+   *flags |= SfNoWriteResult;
+   SET_STATUS_Success(0);
+
+   /* Check to see if any signals arose as a result of this. */
+   *flags |= SfPollAfter;
+}
+
+PRE(sys_mmap)
+{
+   PRINT("sys_mmap ( %#lx, %lu, %lu, %#lx, %lu, %lu )", ARG1, ARG2, ARG3, ARG4,
+         ARG5, ARG6);
+   PRE_REG_READ6(long, "mmap", unsigned long, start, unsigned long, length,
+                 unsigned long, prot, unsigned long, flags, unsigned long, fd,
+                 unsigned long, offset);
+
+   SysRes r =
+      ML_(generic_PRE_sys_mmap)(tid, ARG1, ARG2, ARG3, ARG4, ARG5, ARG6);
+   SET_STATUS_from_SysRes(r);
+}
+
+PRE(sys_riscv_flush_icache)
+{
+   PRINT("sys_riscv_flush_icache ( %#lx, %lx, %#lx )", ARG1, ARG2, ARG3);
+   PRE_REG_READ3(long, "riscv_flush_icache", unsigned long, start,
+                 unsigned long, end, unsigned long, flags);
+
+   VG_(discard_translations)
+   ((Addr)ARG1, (ULong)ARG2 - (ULong)ARG1, "PRE(sys_riscv_flush_icache)");
+   SET_STATUS_Success(0);
+}
+
+#undef PRE
+#undef POST
+
+/* ---------------------------------------------------------------------
+   The riscv64/Linux syscall table
+   ------------------------------------------------------------------ */
+
+/* Add a riscv64-linux specific wrapper to a syscall table. */
+#define PLAX_(sysno, name) WRAPPER_ENTRY_X_(riscv64_linux, sysno, name)
+#define PLAXY(sysno, name) WRAPPER_ENTRY_XY(riscv64_linux, sysno, name)
+
+/* This table maps from __NR_xxx syscall numbers to the appropriate PRE/POST
+   sys_foo() wrappers on riscv64. */
+static SyscallTableEntry syscall_main_table[] = {
+   GENXY(__NR_getcwd, sys_getcwd),                         /* 17 */
+   LINXY(__NR_eventfd2, sys_eventfd2),                     /* 19 */
+   LINXY(__NR_epoll_create1, sys_epoll_create1),           /* 20 */
+   LINXY(__NR_epoll_pwait, sys_epoll_pwait),               /* 22 */
+   GENXY(__NR_dup, sys_dup),                               /* 23 */
+   LINXY(__NR_dup3, sys_dup3),                             /* 24 */
+   LINXY(__NR_fcntl, sys_fcntl),                           /* 25 */
+   LINXY(__NR_ioctl, sys_ioctl),                           /* 29 */
+   LINX_(__NR_mkdirat, sys_mkdirat),                       /* 34 */
+   LINX_(__NR_unlinkat, sys_unlinkat),                     /* 35 */
+   GENX_(__NR_ftruncate, sys_ftruncate),                   /* 46 */
+   LINX_(__NR_faccessat, sys_faccessat),                   /* 48 */
+   GENX_(__NR_chdir, sys_chdir),                           /* 49 */
+   LINX_(__NR_fchmodat, sys_fchmodat),                     /* 53 */
+   LINXY(__NR_openat, sys_openat),                         /* 56 */
+   GENXY(__NR_close, sys_close),                           /* 57 */
+   LINXY(__NR_pipe2, sys_pipe2),                           /* 59 */
+   LINX_(__NR_lseek, sys_lseek),                           /* 62 */
+   GENXY(__NR_read, sys_read),                             /* 63 */
+   GENX_(__NR_write, sys_write),                           /* 64 */
+   GENXY(__NR_readv, sys_readv),                           /* 65 */
+   GENX_(__NR_writev, sys_writev),                         /* 66 */
+   LINXY(__NR_preadv, sys_preadv),                         /* 69 */
+   LINX_(__NR_pwritev, sys_pwritev),                       /* 70 */
+   LINXY(__NR_pselect6, sys_pselect6),                     /* 72 */
+   LINXY(__NR_ppoll, sys_ppoll),                           /* 73 */
+   LINXY(__NR_signalfd4, sys_signalfd4),                   /* 74 */
+   LINX_(__NR_readlinkat, sys_readlinkat),                 /* 78 */
+   LINXY(__NR_newfstatat, sys_newfstatat),                 /* 79 */
+   LINXY(__NR_timerfd_create, sys_timerfd_create),         /* 85 */
+   LINXY(__NR_timerfd_settime, sys_timerfd_settime),       /* 86 */
+   LINX_(__NR_utimensat, sys_utimensat),                   /* 88 */
+   LINXY(__NR_capget, sys_capget),                         /* 90 */
+   GENX_(__NR_exit, sys_exit),                             /* 93 */
+   LINX_(__NR_exit_group, sys_exit_group),                 /* 94 */
+   LINX_(__NR_set_tid_address, sys_set_tid_address),       /* 96 */
+   LINXY(__NR_futex, sys_futex),                           /* 98 */
+   LINX_(__NR_set_robust_list, sys_set_robust_list),       /* 99 */
+   GENXY(__NR_setitimer, sys_setitimer),                   /* 103 */
+   LINXY(__NR_clock_gettime, sys_clock_gettime),           /* 113 */
+   LINXY(__NR_clock_nanosleep, sys_clock_nanosleep),       /* 115 */
+   LINXY(__NR_syslog, sys_syslog),                         /* 116 */
+   LINX_(__NR_sched_yield, sys_sched_yield),               /* 124 */
+   GENX_(__NR_kill, sys_kill),                             /* 129 */
+   LINX_(__NR_tgkill, sys_tgkill),                         /* 131 */
+   GENXY(__NR_sigaltstack, sys_sigaltstack),               /* 132 */
+   LINX_(__NR_rt_sigsuspend, sys_rt_sigsuspend),           /* 133 */
+   LINXY(__NR_rt_sigaction, sys_rt_sigaction),             /* 134 */
+   LINXY(__NR_rt_sigprocmask, sys_rt_sigprocmask),         /* 135 */
+   LINXY(__NR_rt_sigtimedwait, sys_rt_sigtimedwait),       /* 137 */
+   LINXY(__NR_rt_sigqueueinfo, sys_rt_sigqueueinfo),       /* 138 */
+   PLAX_(__NR_rt_sigreturn, sys_rt_sigreturn),             /* 139 */
+   GENX_(__NR_getpgid, sys_getpgid),                       /* 155 */
+   GENXY(__NR_uname, sys_newuname),                        /* 160 */
+   LINXY(__NR_prctl, sys_prctl),                           /* 167 */
+   GENX_(__NR_getpid, sys_getpid),                         /* 172 */
+   GENX_(__NR_getppid, sys_getppid),                       /* 173 */
+   GENX_(__NR_getuid, sys_getuid),                         /* 174 */
+   GENX_(__NR_geteuid, sys_geteuid),                       /* 175 */
+   GENX_(__NR_getgid, sys_getgid),                         /* 176 */
+   GENX_(__NR_getegid, sys_getegid),                       /* 177 */
+   LINX_(__NR_gettid, sys_gettid),                         /* 178 */
+   LINXY(__NR_sysinfo, sys_sysinfo),                       /* 179 */
+   LINXY(__NR_mq_open, sys_mq_open),                       /* 180 */
+   LINX_(__NR_mq_unlink, sys_mq_unlink),                   /* 181 */
+   LINX_(__NR_mq_timedsend, sys_mq_timedsend),             /* 182 */
+   LINXY(__NR_mq_timedreceive, sys_mq_timedreceive),       /* 183 */
+   LINX_(__NR_mq_notify, sys_mq_notify),                   /* 184 */
+   LINXY(__NR_mq_getsetattr, sys_mq_getsetattr),           /* 185 */
+   LINX_(__NR_semget, sys_semget),                         /* 190 */
+   LINXY(__NR_semctl, sys_semctl),                         /* 191 */
+   LINX_(__NR_semtimedop, sys_semtimedop),                 /* 192 */
+   LINX_(__NR_shmget, sys_shmget),                         /* 194 */
+   LINXY(__NR_shmat, sys_shmat),                           /* 196 */
+   LINXY(__NR_socket, sys_socket),                         /* 198 */
+   LINXY(__NR_socketpair, sys_socketpair),                 /* 199 */
+   LINX_(__NR_bind, sys_bind),                             /* 200 */
+   LINX_(__NR_listen, sys_listen),                         /* 201 */
+   LINXY(__NR_accept, sys_accept),                         /* 202 */
+   LINX_(__NR_connect, sys_connect),                       /* 203 */
+   LINXY(__NR_getsockname, sys_getsockname),               /* 204 */
+   LINX_(__NR_sendto, sys_sendto),                         /* 206 */
+   LINXY(__NR_recvfrom, sys_recvfrom),                     /* 207 */
+   LINX_(__NR_setsockopt, sys_setsockopt),                 /* 208 */
+   LINXY(__NR_getsockopt, sys_getsockopt),                 /* 209 */
+   LINX_(__NR_sendmsg, sys_sendmsg),                       /* 211 */
+   LINXY(__NR_recvmsg, sys_recvmsg),                       /* 212 */
+   GENX_(__NR_brk, sys_brk),                               /* 214 */
+   GENXY(__NR_munmap, sys_munmap),                         /* 215 */
+   GENX_(__NR_mremap, sys_mremap),                         /* 216 */
+   LINX_(__NR_clone, sys_clone),                           /* 220 */
+   GENX_(__NR_execve, sys_execve),                         /* 221 */
+   PLAX_(__NR_mmap, sys_mmap),                             /* 222 */
+   GENXY(__NR_mprotect, sys_mprotect),                     /* 226 */
+   GENX_(__NR_madvise, sys_madvise),                       /* 233 */
+   PLAX_(__NR_riscv_flush_icache, sys_riscv_flush_icache), /* 259 */
+   GENXY(__NR_wait4, sys_wait4),                           /* 260 */
+   LINXY(__NR_prlimit64, sys_prlimit64),                   /* 261 */
+   LINXY(__NR_process_vm_readv, sys_process_vm_readv),     /* 270 */
+   LINX_(__NR_process_vm_writev, sys_process_vm_writev),   /* 271 */
+   LINXY(__NR_getrandom, sys_getrandom),                   /* 278 */
+   LINXY(__NR_memfd_create, sys_memfd_create),             /* 279 */
+   LINX_(__NR_execveat, sys_execveat),                     /* 281 */
+   LINX_(__NR_membarrier, sys_membarrier),                 /* 283 */
+   LINX_(__NR_copy_file_range, sys_copy_file_range),       /* 285 */
+   LINXY(__NR_preadv2, sys_preadv2),                       /* 286 */
+   LINX_(__NR_pwritev2, sys_pwritev2),                     /* 287 */
+   LINXY(__NR_statx, sys_statx),                           /* 291 */
+};
+
+SyscallTableEntry* ML_(get_linux_syscall_entry)(UInt sysno)
+{
+   const UInt syscall_main_table_size =
+      sizeof(syscall_main_table) / sizeof(syscall_main_table[0]);
+
+   /* Is it in the contiguous initial section of the table? */
+   if (sysno < syscall_main_table_size) {
+      SyscallTableEntry* sys = &syscall_main_table[sysno];
+      if (sys->before == NULL)
+         return NULL; /* no entry */
+      else
+         return sys;
+   }
+
+   /* Can't find a wrapper. */
+   return NULL;
+}
+
+#endif // defined(VGP_riscv64_linux)
+
+/*--------------------------------------------------------------------*/
+/*--- end                                  syswrap-riscv64-linux.c ---*/
+/*--------------------------------------------------------------------*/
--- a/coregrind/m_trampoline.S
+++ b/coregrind/m_trampoline.S
@@ -1498,6 +1498,47 @@
 #	undef UD2_1024
 #	undef UD2_PAGE
 
+/*---------------- riscv64-linux ----------------*/
+#else
+#if defined(VGP_riscv64_linux)
+
+#	define UD2_4      .word 0
+#	define UD2_16     UD2_4    ; UD2_4    ; UD2_4    ; UD2_4
+#	define UD2_64     UD2_16   ; UD2_16   ; UD2_16   ; UD2_16
+#	define UD2_256    UD2_64   ; UD2_64   ; UD2_64   ; UD2_64
+#	define UD2_1024   UD2_256  ; UD2_256  ; UD2_256  ; UD2_256
+#	define UD2_PAGE   UD2_1024 ; UD2_1024 ; UD2_1024 ; UD2_1024
+
+	/* a leading page of unexecutable code */
+	UD2_PAGE
+
+.global VG_(trampoline_stuff_start)
+VG_(trampoline_stuff_start):
+
+.global VG_(riscv64_linux_SUBST_FOR_rt_sigreturn)
+.type   VG_(riscv64_linux_SUBST_FOR_rt_sigreturn), @function
+VG_(riscv64_linux_SUBST_FOR_rt_sigreturn):
+	.cfi_startproc
+	.cfi_signal_frame
+	li a7, __NR_rt_sigreturn
+	ecall
+	.cfi_endproc
+.size VG_(riscv64_linux_SUBST_FOR_rt_sigreturn), \
+	.-VG_(riscv64_linux_SUBST_FOR_rt_sigreturn)
+
+.global VG_(trampoline_stuff_end)
+VG_(trampoline_stuff_end):
+
+	/* and a trailing page of unexecutable code */
+	UD2_PAGE
+
+#	undef UD2_4
+#	undef UD2_16
+#	undef UD2_64
+#	undef UD2_256
+#	undef UD2_1024
+#	undef UD2_PAGE
+
 /*---------------- x86-solaris ----------------*/
 #else
 #if defined(VGP_x86_solaris)
@@ -1684,6 +1725,7 @@
 #endif
 #endif
 #endif
+#endif
 #endif
 #endif
 #endif
--- a/coregrind/m_translate.c
+++ b/coregrind/m_translate.c
@@ -1749,6 +1749,11 @@
            vex_archinfo.arm64_requires_fallback_LLSC;
 #  endif
 
+#  if defined(VGP_riscv64_linux)
+   /* TODO Make weaker. */
+   vex_abiinfo.guest__use_fallback_LLSC = True;
+#  endif
+
    /* Set up closure args. */
    closure.tid    = tid;
    closure.nraddr = nraddr;
--- a/coregrind/pub_core_basics.h
+++ b/coregrind/pub_core_basics.h
@@ -55,8 +55,8 @@
 
 typedef
    struct {
-      ULong r_pc; /* x86:EIP, amd64:RIP, ppc:CIA, arm:R15, mips:pc */
-      ULong r_sp; /* x86:ESP, amd64:RSP, ppc:R1,  arm:R13, mips:sp */
+      ULong r_pc; /* x86:EIP, amd64:RIP, ppc:CIA, arm:R15, mips:pc, riscv64: pc */
+      ULong r_sp; /* x86:ESP, amd64:RSP, ppc:R1,  arm:R13, mips:sp, riscv64: x2 */
       union {
          struct {
             UInt r_ebp;
@@ -102,6 +102,11 @@
             ULong r31;  /* Return address of the last subroutine call */
             ULong r28;
          } MIPS64;
+	 struct {
+            ULong r_fp; /* x8 */
+            ULong r_ra; /* x1 */
+	    /* TODO Add gp/x3? */
+         } RISCV64;
       } misc;
    }
    UnwindStartRegs;
--- a/coregrind/pub_core_debuginfo.h
+++ b/coregrind/pub_core_debuginfo.h
@@ -131,6 +131,10 @@
 typedef
    struct { Addr pc; Addr sp; Addr fp; Addr ra; }
    D3UnwindRegs;
+#elif defined(VGA_riscv64)
+typedef
+   struct { Addr pc; Addr sp; Addr fp; Addr ra; }
+   D3UnwindRegs;
 #else
 #  error "Unsupported arch"
 #endif
--- a/coregrind/pub_core_machine.h
+++ b/coregrind/pub_core_machine.h
@@ -126,6 +126,11 @@
 #  define VG_ELF_MACHINE      EM_NANOMIPS
 #  define VG_ELF_CLASS        ELFCLASS32
 #  undef  VG_PLAT_USES_PPCTOC
+#elif defined(VGP_riscv64_linux)
+#  define VG_ELF_DATA2XXX     ELFDATA2LSB
+#  define VG_ELF_MACHINE      EM_RISCV
+#  define VG_ELF_CLASS        ELFCLASS64
+#  undef  VG_PLAT_USES_PPCTOC
 #else
 #  error Unknown platform
 #endif
@@ -163,6 +168,10 @@
 #  define VG_INSTR_PTR        guest_PC
 #  define VG_STACK_PTR        guest_r29
 #  define VG_FRAME_PTR        guest_r30
+#elif defined(VGA_riscv64)
+#  define VG_INSTR_PTR        guest_pc
+#  define VG_STACK_PTR        guest_x2
+#  define VG_FRAME_PTR        guest_x8
 #else
 #  error Unknown arch
 #endif
--- a/coregrind/pub_core_mallocfree.h
+++ b/coregrind/pub_core_mallocfree.h
@@ -83,6 +83,7 @@
       defined(VGP_x86_darwin)     || \
       defined(VGP_amd64_darwin)   || \
       defined(VGP_arm64_linux)    || \
+      defined(VGP_riscv64_linux)  || \
       defined(VGP_amd64_solaris)
 #  define VG_MIN_MALLOC_SZB       16
 #else
--- a/coregrind/pub_core_syscall.h
+++ b/coregrind/pub_core_syscall.h
@@ -105,6 +105,7 @@
 extern SysRes VG_(mk_SysRes_mips64_linux)( ULong v0, ULong v1,
                                            ULong a3 );
 extern SysRes VG_(mk_SysRes_nanomips_linux)( UWord a0);
+extern SysRes VG_(mk_SysRes_riscv64_linux) ( Long a0 );
 extern SysRes VG_(mk_SysRes_x86_solaris) ( Bool isErr, UInt val, UInt val2 );
 extern SysRes VG_(mk_SysRes_amd64_solaris) ( Bool isErr, ULong val, ULong val2 );
 extern SysRes VG_(mk_SysRes_Error)       ( UWord val );
--- a/coregrind/pub_core_trampoline.h
+++ b/coregrind/pub_core_trampoline.h
@@ -171,6 +171,10 @@
 extern UInt  VG_(nanomips_linux_REDIR_FOR_strlen)( void* );
 #endif
 
+#if defined(VGP_riscv64_linux)
+extern Addr VG_(riscv64_linux_SUBST_FOR_rt_sigreturn);
+#endif
+
 #if defined(VGP_x86_solaris)
 extern SizeT VG_(x86_solaris_REDIR_FOR_strcmp)(const HChar *, const HChar *);
 extern SizeT VG_(x86_solaris_REDIR_FOR_strlen)(const HChar *);
--- a/coregrind/pub_core_transtab.h
+++ b/coregrind/pub_core_transtab.h
@@ -72,7 +72,7 @@
    return merged & VG_TT_FAST_MASK;
 }
 
-#elif defined(VGA_s390x) || defined(VGA_arm) || defined(VGA_nanomips)
+#elif defined(VGA_s390x) || defined(VGA_arm) || defined(VGA_nanomips) || defined(VGA_riscv64)
 static inline UWord VG_TT_FAST_HASH ( Addr guest ) {
    // Instructions are 2-byte aligned.
    UWord merged = ((UWord)guest) >> 1;
--- a/coregrind/pub_core_transtab_asm.h
+++ b/coregrind/pub_core_transtab_asm.h
@@ -73,6 +73,7 @@
 
    On s390x the rightmost bit of an instruction address is zero, so the arm32
    scheme is used. */
+/* TODO Comment on riscv64. */
 
 #define VG_TT_FAST_BITS 13
 #define VG_TT_FAST_SETS (1 << VG_TT_FAST_BITS)
@@ -83,7 +84,7 @@
 #if defined(VGA_amd64) || defined(VGA_arm64) \
     || defined(VGA_ppc64be) || defined(VGA_ppc64le) \
     || (defined(VGA_mips64) && defined(VGABI_64)) \
-    || defined(VGA_s390x)
+    || defined(VGA_s390x) || defined(VGA_riscv64)
   // And all other 64-bit hosts
 # define VG_FAST_CACHE_SET_BITS 6
   // These FCS_{g,h}{0,1,2,3} are the values of
--- a/coregrind/vgdb-invoker-ptrace.c
+++ b/coregrind/vgdb-invoker-ptrace.c
@@ -40,6 +40,18 @@
 #include <sys/user.h>
 #include <sys/wait.h>
 
+#if defined(VGA_riscv64)
+/* TODO */
+/* Glibc on riscv64 does not provide a definition of user or user_regs_struct
+   in sys/user.h. Instead the definition of user_regs_struct is provided by the
+   kernel in asm/ptrace.h. Pull it and then define the expected user
+   structure. */
+#include <asm/ptrace.h>
+struct user {
+   struct user_regs_struct regs;
+};
+#endif
+
 #ifdef PTRACE_GETREGSET
 // TBD: better have a configure test instead ?
 #define HAVE_PTRACE_GETREGSET
@@ -874,6 +886,8 @@
    sp = p[29];
 #elif defined(VGA_mips64)
    sp = user_mod.regs[29];
+#elif defined(VGA_riscv64)
+   assert(0);
 #else
    I_die_here : (sp) architecture missing in vgdb-invoker-ptrace.c
 #endif
@@ -961,6 +975,10 @@
 
 #elif defined(VGA_mips64)
       assert(0); // cannot vgdb a 32 bits executable with a 64 bits exe
+
+#elif defined(VGA_riscv64)
+      assert(0);
+
 #else
       I_die_here : architecture missing in vgdb-invoker-ptrace.c
 #endif
@@ -1068,6 +1086,8 @@
       user_mod.regs[31] = bad_return;
       user_mod.regs[34] = shared64->invoke_gdbserver;
       user_mod.regs[25] = shared64->invoke_gdbserver;
+#elif defined(VGA_riscv64)
+      assert(0);
 #else
       I_die_here: architecture missing in vgdb-invoker-ptrace.c
 #endif
--- a/drd/drd_bitmap.h
+++ b/drd/drd_bitmap.h
@@ -140,7 +140,7 @@
 #define BITS_PER_BITS_PER_UWORD 5
 #elif defined(VGA_amd64) || defined(VGA_ppc64be) || defined(VGA_ppc64le) \
       || defined(VGA_s390x) || (defined(VGA_mips64) && !defined(VGABI_N32)) \
-      || defined(VGA_arm64) || defined(VGA_tilegx)
+      || defined(VGA_arm64) || defined(VGA_riscv64)
 #define BITS_PER_BITS_PER_UWORD 6
 #else
 #error Unknown platform.
--- a/drd/drd_load_store.c
+++ b/drd/drd_load_store.c
@@ -53,6 +53,8 @@
 #define STACK_POINTER_OFFSET OFFSET_mips32_r29
 #elif defined(VGA_mips64)
 #define STACK_POINTER_OFFSET OFFSET_mips64_r29
+#elif defined(VGA_riscv64)
+#define STACK_POINTER_OFFSET OFFSET_riscv64_x2
 #else
 #error Unknown architecture.
 #endif
--- a/helgrind/tests/annotate_hbefore.c
+++ b/helgrind/tests/annotate_hbefore.c
@@ -314,6 +314,17 @@
    return success;
 }
 
+#elif defined(VGA_riscv64)
+
+// riscv64
+/* return 1 if success, 0 if failure */
+UWord do_acasW ( UWord* addr, UWord expected, UWord nyu )
+{
+   /* TODO Implement. */
+   assert(0);
+   return 0;
+}
+
 #endif
 
 void atomic_incW ( UWord* w )
--- a/helgrind/tests/tc07_hbl1.c
+++ b/helgrind/tests/tc07_hbl1.c
@@ -18,6 +18,7 @@
 #undef PLAT_arm64_linux
 #undef PLAT_s390x_linux
 #undef PLAT_mips32_linux
+#undef PLAT_riscv64_linux
 #undef PLAT_x86_solaris
 #undef PLAT_amd64_solaris
 
@@ -47,6 +48,8 @@
 #  define PLAT_mips32_linux 1
 #elif defined(__linux__) && defined(__nanomips__)
 #  define PLAT_nanomips_linux 1
+#elif defined(__linux__) && defined(__riscv) && (__riscv_xlen == 64)
+#  define PLAT_riscv64_linux 1
 #elif defined(__sun__) && defined(__i386__)
 #  define PLAT_x86_solaris 1
 #elif defined(__sun__) && defined(__x86_64__)
@@ -131,6 +134,11 @@
       : /*out*/ : /*in*/ "r"(&(_lval))              \
       : /*trash*/ "$t0", "$t1", "memory"            \
    )
+#elif defined(PLAT_riscv64_linux)
+   /* TODO Implement. */
+#include <assert.h>
+#  define INC(_lval,_lqual)                         \
+     assert(0);
 #else
 #  error "Fix Me for this platform"
 #endif
--- a/helgrind/tests/tc08_hbl2.c
+++ b/helgrind/tests/tc08_hbl2.c
@@ -35,6 +35,7 @@
 #undef PLAT_s390x_linux
 #undef PLAT_mips32_linux
 #undef PLAT_mips64_linux
+#undef PLAT_riscv64_linux
 #undef PLAT_x86_solaris
 #undef PLAT_amd64_solaris
 
@@ -68,6 +69,8 @@
 #endif
 #elif defined(__linux__) && defined(__nanomips__)
 #  define PLAT_nanomips_linux 1
+#elif defined(__linux__) && defined(__riscv) && (__riscv_xlen == 64)
+#  define PLAT_riscv64_linux 1
 #elif defined(__sun__) && defined(__i386__)
 #  define PLAT_x86_solaris 1
 #elif defined(__sun__) && defined(__x86_64__)
@@ -151,6 +154,12 @@
       : /*out*/ : /*in*/ "r"(&(_lval))              \
       : /*trash*/ "$t0", "$t1", "memory"            \
    )
+#elif defined(PLAT_riscv64_linux)
+   /* TODO Implement. */
+#include <assert.h>
+#  define INC(_lval,_lqual)                         \
+     assert(0);
+
 #else
 #  error "Fix Me for this platform"
 #endif
--- a/helgrind/tests/tc11_XCHG.c
+++ b/helgrind/tests/tc11_XCHG.c
@@ -20,6 +20,7 @@
 #undef PLAT_arm_linux
 #undef PLAT_s390x_linux
 #undef PLAT_mips32_linux
+#undef PLAT_riscv64_linux
 #undef PLAT_x86_solaris
 #undef PLAT_amd64_solaris
 
@@ -49,6 +50,8 @@
 #  define PLAT_mips32_linux 1
 #elif defined(__linux__) && defined(__nanomips__)
 #  define PLAT_nanomips_linux 1
+#elif defined(__linux__) && defined(__riscv) && (__riscv_xlen == 64)
+#  define PLAT_riscv64_linux 1
 #elif defined(__sun__) && defined(__i386__)
 #  define PLAT_x86_solaris 1
 #elif defined(__sun__) && defined(__x86_64__)
@@ -124,7 +127,8 @@
       XCHG_M_R(_addr,_lval)
 
 #elif defined(PLAT_ppc32_linux) || defined(PLAT_ppc64_linux) \
-      || defined(PLAT_arm_linux) || defined(PLAT_arm64_linux)
+      || defined(PLAT_arm_linux) || defined(PLAT_arm64_linux) \
+      || defined(PLAT_riscv64_linux)
 #  if defined(HAVE_BUILTIN_ATOMIC)
 #    define XCHG_M_R(_addr,_lval)                                           \
         do {                                                                \
--- a/include/Makefile.am
+++ b/include/Makefile.am
@@ -63,6 +63,7 @@
 	vki/vki-posixtypes-mips32-linux.h \
 	vki/vki-posixtypes-mips64-linux.h \
 	vki/vki-posixtypes-nanomips-linux.h \
+        vki/vki-posixtypes-riscv64-linux.h \
 	vki/vki-amd64-linux.h		\
 	vki/vki-arm64-linux.h		\
 	vki/vki-ppc32-linux.h		\
@@ -75,6 +76,7 @@
 	vki/vki-mips32-linux.h		\
 	vki/vki-mips64-linux.h		\
 	vki/vki-nanomips-linux.h	\
+        vki/vki-riscv64-linux.h         \
 	vki/vki-scnums-amd64-linux.h	\
 	vki/vki-scnums-arm64-linux.h	\
 	vki/vki-scnums-ppc32-linux.h	\
@@ -86,6 +88,7 @@
 	vki/vki-scnums-mips32-linux.h	\
 	vki/vki-scnums-mips64-linux.h	\
 	vki/vki-scnums-nanomips-linux.h	\
+        vki/vki-scnums-riscv64-linux.h  \
 	vki/vki-scnums-darwin.h         \
 	vki/vki-scnums-solaris.h	\
 	vki/vki-scnums-shared-linux.h	\
--- a/include/pub_tool_basics.h
+++ b/include/pub_tool_basics.h
@@ -442,7 +442,8 @@
 
 #if defined(VGA_x86) || defined(VGA_amd64) || defined (VGA_arm) \
     || ((defined(VGA_mips32) || defined(VGA_mips64) || defined(VGA_nanomips)) \
-    && defined (_MIPSEL)) || defined(VGA_arm64)  || defined(VGA_ppc64le)
+    && defined (_MIPSEL)) || defined(VGA_arm64)  || defined(VGA_ppc64le) \
+    || defined(VGA_riscv64)
 #  define VG_LITTLEENDIAN 1
 #elif defined(VGA_ppc32) || defined(VGA_ppc64be) || defined(VGA_s390x) \
       || ((defined(VGA_mips32) || defined(VGA_mips64) || defined(VGA_nanomips)) \
@@ -490,7 +491,8 @@
       || defined(VGA_ppc64be) || defined(VGA_ppc64le) \
       || defined(VGA_arm) || defined(VGA_s390x) \
       || defined(VGA_mips32) || defined(VGA_mips64) \
-      || defined(VGA_arm64) || defined(VGA_nanomips)
+      || defined(VGA_arm64) || defined(VGA_nanomips) \
+      || defined(VGA_riscv64)
 #  define VG_REGPARM(n)            /* */
 #else
 #  error Unknown arch
--- a/include/pub_tool_guest.h
+++ b/include/pub_tool_guest.h
@@ -62,6 +62,9 @@
 #elif defined(VGA_mips64)
 #  include "libvex_guest_mips64.h"
    typedef VexGuestMIPS64State VexGuestArchState;
+#elif defined(VGA_riscv64)
+#  include "libvex_guest_riscv64.h"
+   typedef VexGuestRISCV64State VexGuestArchState;
 #else
 #  error Unknown arch
 #endif
--- a/include/pub_tool_machine.h
+++ b/include/pub_tool_machine.h
@@ -108,6 +108,12 @@
 #  define VG_CLREQ_SZB             20
 #  define VG_STACK_REDZONE_SZB      0
 
+#elif defined(VGP_riscv64_linux)
+#  define VG_MIN_INSTR_SZB          2
+#  define VG_MAX_INSTR_SZB          4
+#  define VG_CLREQ_SZB             20
+#  define VG_STACK_REDZONE_SZB      0
+
 #else
 #  error Unknown platform
 #endif
--- a/include/pub_tool_vkiscnums_asm.h
+++ b/include/pub_tool_vkiscnums_asm.h
@@ -80,6 +80,10 @@
 #elif defined(VGP_x86_darwin) || defined(VGP_amd64_darwin)
 #  include "vki/vki-scnums-darwin.h"
 
+#elif defined(VGP_riscv64_linux)
+#  include "vki/vki-scnums-shared-linux.h"
+#  include "vki/vki-scnums-riscv64-linux.h"
+
 #elif defined(VGP_x86_solaris) || (VGP_amd64_solaris)
 #  include "vki/vki-scnums-solaris.h"
 
--- a/include/valgrind.h
+++ b/include/valgrind.h
@@ -125,6 +125,7 @@
 #undef PLAT_mips32_linux
 #undef PLAT_mips64_linux
 #undef PLAT_nanomips_linux
+#undef PLAT_riscv64_linux
 #undef PLAT_x86_solaris
 #undef PLAT_amd64_solaris
 
@@ -169,6 +170,8 @@
 #  define PLAT_mips32_linux 1
 #elif defined(__linux__) && defined(__nanomips__)
 #  define PLAT_nanomips_linux 1
+#elif defined(__linux__) && defined(__riscv) && (__riscv_xlen == 64)
+#  define PLAT_riscv64_linux 1
 #elif defined(__sun) && defined(__i386__)
 #  define PLAT_x86_solaris 1
 #elif defined(__sun) && defined(__x86_64__)
@@ -1126,6 +1129,86 @@
  } while (0)
 
 #endif
+
+/* ----------------------- riscv64-linux ------------------------ */
+
+#if defined(PLAT_riscv64_linux)
+
+typedef
+   struct {
+      unsigned long int nraddr; /* where's the code? */
+   }
+   OrigFn;
+
+#define __SPECIAL_INSTRUCTION_PREAMBLE                            \
+            ".option push\n\t"                                    \
+            ".option norvc\n\t"                                   \
+            "srli a2, a2, 3\n\t"                                  \
+            "srli a2, a2, 13\n\t"                                 \
+            "srli a2, a2, 51\n\t"                                 \
+            "srli a2, a2, 61\n\t"
+
+#define __SPECIAL_INSTRUCTION_POSTAMBLE                           \
+            ".option pop\n\t"                                     \
+
+#define VALGRIND_DO_CLIENT_REQUEST_EXPR(                          \
+        _zzq_default, _zzq_request,                               \
+        _zzq_arg1, _zzq_arg2, _zzq_arg3, _zzq_arg4, _zzq_arg5)    \
+                                                                  \
+  __extension__                                                   \
+  ({volatile unsigned long int  _zzq_args[6];                     \
+    volatile unsigned long int  _zzq_result;                      \
+    _zzq_args[0] = (unsigned long int)(_zzq_request);             \
+    _zzq_args[1] = (unsigned long int)(_zzq_arg1);                \
+    _zzq_args[2] = (unsigned long int)(_zzq_arg2);                \
+    _zzq_args[3] = (unsigned long int)(_zzq_arg3);                \
+    _zzq_args[4] = (unsigned long int)(_zzq_arg4);                \
+    _zzq_args[5] = (unsigned long int)(_zzq_arg5);                \
+   __asm__ volatile("mv a3, %1\n\t" /*default*/                  \
+                     "mv a4, %2\n\t" /*ptr*/                      \
+                     __SPECIAL_INSTRUCTION_PREAMBLE               \
+                     /* a3 = client_request ( a4 ) */             \
+                     "or a0, a0, a0\n\t"                          \
+                     __SPECIAL_INSTRUCTION_POSTAMBLE              \
+                     "mv %0, a3"     /*result*/                   \
+                     : "=r" (_zzq_result)                         \
+                     : "r" ((unsigned long int)(_zzq_default)),   \
+                       "r" (&_zzq_args[0])                        \
+                     : "memory", "a3", "a4");                     \
+    _zzq_result;                                                  \
+  })
+
+#define VALGRIND_GET_NR_CONTEXT(_zzq_rlval)                       \
+  { volatile OrigFn* _zzq_orig = &(_zzq_rlval);                   \
+    unsigned long int __addr;                                     \
+    __asm__ volatile(__SPECIAL_INSTRUCTION_PREAMBLE               \
+                     /* a3 = guest_NRADDR */                      \
+                     "or a1, a1, a1\n\t"                          \
+                     __SPECIAL_INSTRUCTION_POSTAMBLE              \
+                     "mv %0, a3"                                  \
+                     : "=r" (__addr)                              \
+                     :                                            \
+                     : "memory", "a3"                             \
+                    );                                            \
+   _zzq_orig->nraddr = __addr;                                   \
+  }
+
+#define VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                    \
+                     __SPECIAL_INSTRUCTION_PREAMBLE               \
+                     /* branch-and-link-to-noredir t0 */          \
+                     "or a2, a2, a2\n\t"                          \
+                     __SPECIAL_INSTRUCTION_POSTAMBLE
+
+#define VALGRIND_VEX_INJECT_IR()                                 \
+ do {                                                            \
+    __asm__ volatile(__SPECIAL_INSTRUCTION_PREAMBLE              \
+                     "or a3, a3, a3\n\t"                         \
+                     __SPECIAL_INSTRUCTION_POSTAMBLE             \
+                     : : : "memory"                              \
+                    );                                           \
+ } while (0)
+
+#endif /* PLAT_riscv64_linux */
 /* Insert assembly code for other platforms here... */
 
 #endif /* NVALGRIND */
@@ -6603,6 +6686,454 @@
 
 #endif /* PLAT_mips64_linux */
 
+/* ----------------------- riscv64-linux ----------------------- */
+
+#if defined(PLAT_riscv64_linux)
+
+/* These regs are trashed by the hidden call. */
+#define __CALLER_SAVED_REGS \
+     "ra",                                                        \
+     "t0", "t1", "t2", "t3", "t4", "t5", "t6",                    \
+     "a0", "a1", "a2", "a3", "a4", "a5", "a6", "a7",              \
+     "ft0", "ft1", "ft2", "ft3", "ft4", "ft5", "ft6", "ft7",      \
+     "ft8", "ft9", "ft10", "ft11",                                \
+     "fa0", "fa1", "fa2", "fa3", "fa4", "fa5", "fa6", "fa7"
+
+/* s11 is callee-saved, so we can use it to save and restore sp around
+   the hidden call. */
+#define VALGRIND_ALIGN_STACK               \
+      "mv s11, sp\n\t"                     \
+      "andi sp, sp, 0xfffffffffffffff0\n\t"
+#define VALGRIND_RESTORE_STACK             \
+      "mv sp, s11\n\t"
+
+/* These CALL_FN_ macros assume that on riscv64-linux,
+   sizeof(unsigned long) == 8. */
+
+#define CALL_FN_W_v(lval, orig)                                   \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[1];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0\n"                                            \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_W(lval, orig, arg1)                             \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[2];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0\n"                                            \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_WW(lval, orig, arg1,arg2)                       \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[3];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0\n"                                            \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_WWW(lval, orig, arg1,arg2,arg3)                 \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[4];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0\n"                                            \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_WWWW(lval, orig, arg1,arg2,arg3,arg4)           \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[5];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_5W(lval, orig, arg1,arg2,arg3,arg4,arg5)        \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[6];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_6W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6)   \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[7];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+#define CALL_FN_W_7W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6,   \
+                                 arg7)                            \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[8];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld a6, 56(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_8W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6,   \
+                                 arg7,arg8)                       \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[9];                          \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld a6, 56(%1) \n\t"                                     \
+         "ld a7, 64(%1) \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_9W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6,   \
+                                 arg7,arg8,arg9)                  \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[10];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi sp, sp, -16 \n\t"                                  \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld a6, 56(%1) \n\t"                                     \
+         "ld a7, 64(%1) \n\t"                                     \
+         "ld t0, 72(%1) \n\t"                                     \
+         "sd t0, 0(sp)  \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_10W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6,  \
+                                  arg7,arg8,arg9,arg10)           \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[11];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      _argvec[10] = (unsigned long)(arg10);                       \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi sp, sp, -16 \n\t"                                  \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld a6, 56(%1) \n\t"                                     \
+         "ld a7, 64(%1) \n\t"                                     \
+         "ld t0, 72(%1) \n\t"                                     \
+         "sd t0, 0(sp)  \n\t"                                     \
+         "ld t0, 80(%1) \n\t"                                     \
+         "sd t0, 8(sp)  \n\t"                                     \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_11W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6,  \
+                                  arg7,arg8,arg9,arg10,arg11)     \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[12];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      _argvec[10] = (unsigned long)(arg10);                       \
+      _argvec[11] = (unsigned long)(arg11);                       \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi sp, sp, -32 \n\t"                                  \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld a6, 56(%1) \n\t"                                     \
+         "ld a7, 64(%1) \n\t"                                     \
+         "ld t0, 72(%1) \n\t"                                     \
+         "sd t0, 0(sp)  \n\t"                                     \
+         "ld t0, 80(%1) \n\t"                                     \
+         "sd t0, 8(sp)  \n\t"                                     \
+         "ld t0, 88(%1) \n\t"                                     \
+         "sd t0, 16(sp)  \n\t"                                    \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#define CALL_FN_W_12W(lval, orig, arg1,arg2,arg3,arg4,arg5,arg6,  \
+                                  arg7,arg8,arg9,arg10,arg11,     \
+                                  arg12)                          \
+   do {                                                           \
+      volatile OrigFn        _orig = (orig);                      \
+      volatile unsigned long _argvec[13];                         \
+      volatile unsigned long _res;                                \
+      _argvec[0] = (unsigned long)_orig.nraddr;                   \
+      _argvec[1] = (unsigned long)(arg1);                         \
+      _argvec[2] = (unsigned long)(arg2);                         \
+      _argvec[3] = (unsigned long)(arg3);                         \
+      _argvec[4] = (unsigned long)(arg4);                         \
+      _argvec[5] = (unsigned long)(arg5);                         \
+      _argvec[6] = (unsigned long)(arg6);                         \
+      _argvec[7] = (unsigned long)(arg7);                         \
+      _argvec[8] = (unsigned long)(arg8);                         \
+      _argvec[9] = (unsigned long)(arg9);                         \
+      _argvec[10] = (unsigned long)(arg10);                       \
+      _argvec[11] = (unsigned long)(arg11);                       \
+      _argvec[12] = (unsigned long)(arg12);                       \
+      __asm__ volatile(                                           \
+         VALGRIND_ALIGN_STACK                                     \
+         "addi sp, sp, -32 \n\t"                                  \
+         "ld a0, 8(%1) \n\t"                                      \
+         "ld a1, 16(%1) \n\t"                                     \
+         "ld a2, 24(%1) \n\t"                                     \
+         "ld a3, 32(%1) \n\t"                                     \
+         "ld a4, 40(%1) \n\t"                                     \
+         "ld a5, 48(%1) \n\t"                                     \
+         "ld a6, 56(%1) \n\t"                                     \
+         "ld a7, 64(%1) \n\t"                                     \
+         "ld t0, 72(%1) \n\t"                                     \
+         "sd t0, 0(sp)  \n\t"                                     \
+         "ld t0, 80(%1) \n\t"                                     \
+         "sd t0, 8(sp)  \n\t"                                     \
+         "ld t0, 88(%1) \n\t"                                     \
+         "sd t0, 16(sp)  \n\t"                                    \
+         "ld t0, 96(%1) \n\t"                                     \
+         "sd t0, 24(sp)  \n\t"                                    \
+         "ld t0, 0(%1) \n\t"  /* target->t0 */                    \
+         VALGRIND_BRANCH_AND_LINK_TO_NOREDIR_T0                   \
+         VALGRIND_RESTORE_STACK                                   \
+         "mv %0, a0"                                              \
+         : /*out*/   "=r" (_res)                                  \
+         : /*in*/    "0" (&_argvec[0])                            \
+         : /*trash*/ "memory", __CALLER_SAVED_REGS, "s11"         \
+      );                                                          \
+      lval = (__typeof__(lval)) _res;                             \
+   } while (0)
+
+#endif /* PLAT_riscv64_linux */
 /* ------------------------------------------------------------------ */
 /* ARCHITECTURE INDEPENDENT MACROS for CLIENT REQUESTS.               */
 /*                                                                    */
@@ -7159,6 +7690,7 @@
 #undef PLAT_mips32_linux
 #undef PLAT_mips64_linux
 #undef PLAT_nanomips_linux
+#undef PLAT_riscv64_linux
 #undef PLAT_x86_solaris
 #undef PLAT_amd64_solaris
 
--- a/include/vki/vki-linux.h
+++ b/include/vki/vki-linux.h
@@ -97,6 +97,8 @@
 #  include "vki-posixtypes-mips64-linux.h"
 #elif defined(VGA_nanomips)
 #  include "vki-posixtypes-nanomips-linux.h"
+#elif defined(VGA_riscv64)
+#  include "vki-posixtypes-riscv64-linux.h"
 #else
 #  error Unknown platform
 #endif
@@ -225,6 +227,8 @@
 #  include "vki-mips64-linux.h"
 #elif defined(VGA_nanomips)
 #  include "vki-nanomips-linux.h"
+#elif defined(VGA_riscv64)
+#  include "vki-riscv64-linux.h"
 #else
 #  error Unknown platform
 #endif
--- /dev/null
+++ b/include/vki/vki-posixtypes-riscv64-linux.h
@@ -0,0 +1,66 @@
+
+/*--------------------------------------------------------------------*/
+/*--- riscv64/Linux-specific kernel interface: posix types.        ---*/
+/*---                               vki-posixtypes-riscv64-linux.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef __VKI_POSIXTYPES_RISCV64_LINUX_H
+#define __VKI_POSIXTYPES_RISCV64_LINUX_H
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/posix_types.h
+//----------------------------------------------------------------------
+
+typedef unsigned int	__vki_kernel_mode_t;
+typedef long		__vki_kernel_off_t;
+typedef int		__vki_kernel_pid_t;
+typedef int		__vki_kernel_ipc_pid_t;
+typedef unsigned int	__vki_kernel_uid_t;
+typedef unsigned int	__vki_kernel_gid_t;
+typedef unsigned long	__vki_kernel_size_t;
+typedef long		__vki_kernel_time_t;
+typedef long		__vki_kernel_suseconds_t;
+typedef long		__vki_kernel_clock_t;
+typedef int		__vki_kernel_timer_t;
+typedef int		__vki_kernel_clockid_t;
+typedef char *		__vki_kernel_caddr_t;
+typedef unsigned int	__vki_kernel_uid32_t;
+typedef unsigned int	__vki_kernel_gid32_t;
+
+typedef unsigned int	__vki_kernel_old_uid_t;
+typedef unsigned int	__vki_kernel_old_gid_t;
+
+typedef long long	__vki_kernel_loff_t;
+
+typedef struct {
+	int	val[2];
+} __vki_kernel_fsid_t;
+
+#endif // __VKI_POSIXTYPES_RISCV64_LINUX_H
+
+/*--------------------------------------------------------------------*/
+/*--- end                           vki-posixtypes-riscv64-linux.h ---*/
+/*--------------------------------------------------------------------*/
--- /dev/null
+++ b/include/vki/vki-scnums-riscv64-linux.h
@@ -0,0 +1,169 @@
+
+/*--------------------------------------------------------------------*/
+/*--- System call numbers for riscv64-linux.                       ---*/
+/*---                                   vki-scnums-riscv64-linux.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef __VKI_SCNUMS_RISCV64_LINUX_H
+#define __VKI_SCNUMS_RISCV64_LINUX_H
+
+// From linux-5.10.4/arch/riscv/include/uapi/asm/unistd.h
+// is a #include of
+//      linux-5.10.4/include/uapi/asm-generic/unistd.h
+
+#define __NR_getxattr 8
+#define __NR_getcwd 17
+#define __NR_eventfd2 19
+#define __NR_epoll_create1 20
+#define __NR_epoll_pwait 22
+#define __NR_dup 23
+#define __NR_dup3 24
+#define __NR3264_fcntl 25
+#define __NR_ioctl 29
+#define __NR_mknodat 33
+#define __NR_mkdirat 34
+#define __NR_unlinkat 35
+#define __NR_ftruncate 46
+#define __NR_faccessat 48
+#define __NR_chdir 49
+#define __NR_fchmodat 53
+#define __NR_openat 56
+#define __NR_close 57
+#define __NR_pipe2 59
+#define __NR_getdents64 61
+#define __NR3264_lseek 62
+#define __NR_read 63
+#define __NR_write 64
+#define __NR_readv 65
+#define __NR_writev 66
+#define __NR_pread64 67
+#define __NR_preadv 69
+#define __NR_pwritev 70
+#define __NR_pselect6 72
+#define __NR_ppoll 73
+#define __NR_signalfd4 74
+#define __NR_readlinkat 78
+#define __NR3264_fstatat 79
+#define __NR_timerfd_create 85
+#define __NR_timerfd_settime 86
+#define __NR_utimensat 88
+#define __NR_capget 90
+#define __NR_exit 93
+#define __NR_exit_group 94
+#define __NR_set_tid_address 96
+#define __NR_futex 98
+#define __NR_set_robust_list 99
+#define __NR_setitimer 103
+#define __NR_clock_gettime 113
+#define __NR_clock_nanosleep 115
+#define __NR_syslog 116
+#define __NR_ptrace 117
+#define __NR_sched_yield 124
+#define __NR_kill 129
+#define __NR_tkill 130
+#define __NR_tgkill 131
+#define __NR_sigaltstack 132
+#define __NR_rt_sigsuspend 133
+#define __NR_rt_sigaction 134
+#define __NR_rt_sigprocmask 135
+#define __NR_rt_sigtimedwait 137
+#define __NR_rt_sigqueueinfo 138
+#define __NR_rt_sigreturn 139
+#define __NR_getpgid 155
+#define __NR_getgroups 158
+#define __NR_uname 160
+#define __NR_getrusage 165
+#define __NR_prctl 167
+#define __NR_gettimeofday 169
+#define __NR_getpid 172
+#define __NR_getppid 173
+#define __NR_getuid 174
+#define __NR_geteuid 175
+#define __NR_getgid 176
+#define __NR_getegid 177
+#define __NR_gettid 178
+#define __NR_sysinfo 179
+#define __NR_mq_open 180
+#define __NR_mq_unlink 181
+#define __NR_mq_timedsend 182
+#define __NR_mq_timedreceive 183
+#define __NR_mq_notify 184
+#define __NR_mq_getsetattr 185
+#define __NR_semget 190
+#define __NR_semctl 191
+#define __NR_semtimedop 192
+#define __NR_shmget 194
+#define __NR_shmctl 195
+#define __NR_shmat 196
+#define __NR_socket 198
+#define __NR_socketpair 199
+#define __NR_bind 200
+#define __NR_listen 201
+#define __NR_accept 202
+#define __NR_connect 203
+#define __NR_getsockname 204
+#define __NR_getpeername 205
+#define __NR_sendto 206
+#define __NR_recvfrom 207
+#define __NR_setsockopt 208
+#define __NR_getsockopt 209
+#define __NR_sendmsg 211
+#define __NR_recvmsg 212
+#define __NR_brk 214
+#define __NR_munmap 215
+#define __NR_mremap 216
+#define __NR_clone 220
+#define __NR_execve 221
+#define __NR3264_mmap 222
+#define __NR_mprotect 226
+#define __NR_madvise 233
+#define __NR_arch_specific_syscall 244
+#define __NR_wait4 260
+#define __NR_prlimit64 261
+#define __NR_process_vm_readv 270
+#define __NR_process_vm_writev 271
+#define __NR_renameat2 276
+#define __NR_getrandom 278
+#define __NR_memfd_create 279
+#define __NR_execveat 281
+#define __NR_membarrier 283
+#define __NR_copy_file_range 285
+#define __NR_preadv2 286
+#define __NR_pwritev2 287
+#define __NR_statx 291
+
+#define __NR_fcntl __NR3264_fcntl
+#define __NR_lseek __NR3264_lseek
+#define __NR_newfstatat __NR3264_fstatat
+#define __NR_mmap __NR3264_mmap
+
+#define __NR_riscv_flush_icache (__NR_arch_specific_syscall + 15)
+
+#endif /* __VKI_SCNUMS_RISCV64_LINUX_H */
+
+/*--------------------------------------------------------------------*/
+/*--- end                               vki-scnums-riscv64-linux.h ---*/
+/*--------------------------------------------------------------------*/
--- a/memcheck/mc_machine.c
+++ b/memcheck/mc_machine.c
@@ -1395,6 +1395,72 @@
 #  undef GOF
 #  undef SZB
 
+   /* ------------------- riscv64 ------------------- */
+
+#  elif defined(VGA_riscv64)
+
+#  define GOF(_fieldname) \
+      (offsetof(VexGuestRISCV64State,guest_##_fieldname))
+#  define SZB(_fieldname) \
+      (sizeof(((VexGuestRISCV64State*)0)->guest_##_fieldname))
+
+   Int  o    = offset;
+   Int  sz   = szB;
+   Bool is48 = sz == 8 || sz == 4;
+
+   tl_assert(sz > 0);
+   tl_assert(host_is_little_endian());
+
+   if (o == GOF(x0)  && is48) return -1;
+   if (o == GOF(x1)  && is48) return o;
+   if (o == GOF(x2)  && is48) return o;
+   if (o == GOF(x3)  && is48) return o;
+   if (o == GOF(x4)  && is48) return o;
+   if (o == GOF(x5)  && is48) return o;
+   if (o == GOF(x6)  && is48) return o;
+   if (o == GOF(x7)  && is48) return o;
+   if (o == GOF(x8)  && is48) return o;
+   if (o == GOF(x9)  && is48) return o;
+   if (o == GOF(x10) && is48) return o;
+   if (o == GOF(x11) && is48) return o;
+   if (o == GOF(x12) && is48) return o;
+   if (o == GOF(x13) && is48) return o;
+   if (o == GOF(x14) && is48) return o;
+   if (o == GOF(x15) && is48) return o;
+   if (o == GOF(x16) && is48) return o;
+   if (o == GOF(x17) && is48) return o;
+   if (o == GOF(x18) && is48) return o;
+   if (o == GOF(x19) && is48) return o;
+   if (o == GOF(x20) && is48) return o;
+   if (o == GOF(x21) && is48) return o;
+   if (o == GOF(x22) && is48) return o;
+   if (o == GOF(x23) && is48) return o;
+   if (o == GOF(x24) && is48) return o;
+   if (o == GOF(x25) && is48) return o;
+   if (o == GOF(x26) && is48) return o;
+   if (o == GOF(x27) && is48) return o;
+   if (o == GOF(x28) && is48) return o;
+   if (o == GOF(x29) && is48) return o;
+   if (o == GOF(x30) && is48) return o;
+   if (o == GOF(x31) && is48) return o;
+   if (o == GOF(pc)  && is48) return -1;
+
+   if (o == GOF(EMNOTE)  && sz == 4) return -1;
+   if (o == GOF(CMSTART) && sz == 8) return -1;
+   if (o == GOF(CMLEN)   && sz == 8) return -1;
+   if (o == GOF(NRADDR)  && sz == 4) return -1;
+   if (o == GOF(IP_AT_SYSCALL) && sz == 8) return -1;
+
+   if (o == GOF(LLSC_SIZE) && sz == 8) return -1;
+   if (o == GOF(LLSC_ADDR) && sz == 8) return o;
+   if (o == GOF(LLSC_DATA) && sz == 8) return o;
+
+   VG_(printf)("MC_(get_otrack_shadow_offset)(riscv64)(off=%d,sz=%d)\n",
+               offset,szB);
+   tl_assert(0);
+#  undef GOF
+#  undef SZB
+
 #  else
 #    error "FIXME: not implemented for this architecture"
 #  endif
@@ -1515,6 +1581,13 @@
    ppIRRegArray(arr);
    VG_(printf)("\n");
    tl_assert(0);
+
+  /* ------------------- riscv64 ------------------- */
+#  elif defined(VGA_riscv64)
+   VG_(printf)("get_reg_array_equiv_int_type(riscv64): unhandled: ");
+   ppIRRegArray(arr);
+   VG_(printf)("\n");
+   tl_assert(0);
 
 #  else
 #    error "FIXME: not implemented for this architecture"
--- a/memcheck/tests/Makefile.am
+++ b/memcheck/tests/Makefile.am
@@ -50,6 +50,9 @@
 if VGCONF_PLATFORMS_INCLUDE_ARM64_LINUX
 SUBDIRS += arm64-linux
 endif
+if VGCONF_PLATFORMS_INCLUDE_RISCV64_LINUX
+SUBDIRS += riscv64-linux
+endif
 if VGCONF_PLATFORMS_INCLUDE_X86_SOLARIS
 SUBDIRS += x86-solaris
 endif
@@ -64,7 +67,7 @@
 endif
 
 DIST_SUBDIRS = x86 amd64 ppc32 ppc64 s390x linux \
-		darwin solaris x86-linux amd64-linux arm64-linux \
+		darwin solaris x86-linux amd64-linux arm64-linux riscv64-linux \
 		x86-solaris amd64-solaris mips32 mips64 \
 		freebsd amd64-freebsd x86-freebsd \
 		common .
--- a/memcheck/tests/atomic_incs.c
+++ b/memcheck/tests/atomic_incs.c
@@ -234,6 +234,9 @@
       );
    } while (block[2] != 1);
 #endif
+#elif defined(VGA_riscv64)
+   /* TODO Implement. */
+   assert(0);
 #else
 # error "Unsupported arch"
 #endif
@@ -450,6 +453,9 @@
       );
    } while (block[2] != 1);
 #endif
+#elif defined(VGA_riscv64)
+   /* TODO Implement. */
+   assert(0);
 #else
 # error "Unsupported arch"
 #endif
@@ -605,6 +611,9 @@
          : /*trash*/ "memory", "t0", "t1", "t2", "t3"
       );
    } while (block[2] != 1);
+#elif defined(VGA_riscv64)
+   /* TODO Implement. */
+   assert(0);
 #else
 # error "Unsupported arch"
 #endif
@@ -707,6 +716,9 @@
          : /*trash*/ "memory", "t0", "t1", "t2", "t3"
       );
    } while (block[2] != 1);
+#elif defined(VGA_riscv64)
+   /* TODO Implement. */
+   assert(0);
 #else
 # error "Unsupported arch"
 #endif
--- a/memcheck/tests/leak-segv-jmp.c
+++ b/memcheck/tests/leak-segv-jmp.c
@@ -182,6 +182,23 @@
    return out;
 }
 
+#elif defined(VGP_riscv64_linux)
+extern UWord do_syscall_WRK (
+          UWord a1, UWord a2, UWord a3,
+          UWord a4, UWord a5, UWord a6,
+          UWord syscall_no
+       );
+asm(
+".text\n"
+".globl do_syscall_WRK\n"
+"do_syscall_WRK:\n"
+"        mv a7, a6\n"
+"        li a6, 0\n"
+"        ecall\n"
+"        ret\n"
+".previous\n"
+);
+
 #elif defined(VGP_x86_solaris)
 extern ULong
 do_syscall_WRK(UWord a1, UWord a2, UWord a3,
@@ -338,7 +355,7 @@
                                     &err);
    if (err)
       mprotect_result = -1;
-#elif defined(VGP_arm64_linux)
+#elif defined(VGP_arm64_linux) || defined(VGP_riscv64_linux)
    mprotect_result = do_syscall_WRK((UWord) addr, len, PROT_NONE,
                                     0, 0, 0,
                                     __NR_mprotect);
--- a/memcheck/tests/leak.h
+++ b/memcheck/tests/leak.h
@@ -143,6 +143,11 @@
                                   "$8", "$9", "$10", "$11", "$12", "$13",    \
                                   "$14", "$15", "$24", "$25", "$31");        \
    } while (0)
+#elif defined(__riscv)
+#define CLEAR_CALLER_SAVED_REGS \
+  do { \
+    __asm__ __volatile__( "li a0, 0" : : :/*trash*/"a0" ); \
+  } while (0)
 #else
 #define CLEAR_CALLER_SAVED_REGS  /*nothing*/
 #endif
--- /dev/null
+++ b/memcheck/tests/riscv64-linux/Makefile.am
@@ -0,0 +1,17 @@
+
+include $(top_srcdir)/Makefile.tool-tests.am
+
+dist_noinst_SCRIPTS = \
+	filter_stderr
+
+noinst_HEADERS = scalar.h
+
+EXTRA_DIST = \
+	scalar.stderr.exp scalar.vgtest
+
+check_PROGRAMS = \
+	scalar
+
+AM_CFLAGS    += @FLAG_M64@
+AM_CXXFLAGS  += @FLAG_M64@
+AM_CCASFLAGS += @FLAG_M64@
--- /dev/null
+++ b/memcheck/tests/riscv64-linux/filter_stderr
@@ -0,0 +1,3 @@
+#! /bin/sh
+
+../filter_stderr "$@"
--- /dev/null
+++ b/memcheck/tests/riscv64-linux/scalar.c
@@ -0,0 +1,3 @@
+/* TODO Implement. */
+#include "scalar.h"
+int main(void) { return 0; }
--- /dev/null
+++ b/memcheck/tests/riscv64-linux/scalar.h
@@ -0,0 +1 @@
+/* TODO Implement. */
--- /dev/null
+++ b/memcheck/tests/riscv64-linux/scalar.vgtest
@@ -0,0 +1,3 @@
+prog: scalar
+vgopts: -q --error-limit=no
+args: < scalar.c
--- a/none/tests/Makefile.am
+++ b/none/tests/Makefile.am
@@ -35,7 +35,9 @@
 if VGCONF_ARCHS_INCLUDE_NANOMIPS
 SUBDIRS += nanomips
 endif
-
+if VGCONF_ARCHS_INCLUDE_RISCV64
+SUBDIRS += riscv64
+endif
 
 # OS-specific tests
 if VGCONF_OS_IS_LINUX
@@ -72,7 +74,7 @@
 endif
 
 DIST_SUBDIRS = x86 amd64 ppc32 ppc64 arm arm64 s390x mips32 mips64 nanomips \
-               linux darwin solaris freebsd amd64-linux x86-linux amd64-darwin \
+               linux darwin solaris freebsd amd64-linux x86-linux amd64-darwin riscv64 \
                x86-darwin amd64-solaris x86-solaris scripts .
 
 dist_noinst_SCRIPTS = \
@@ -230,7 +232,7 @@
 	floored fork fucomip \
 	ioctl_moans \
 	libvex_test \
-	libvexmultiarch_test \
+	#libvexmultiarch_test \
 	manythreads \
 	mmap_fcntl_bug \
 	munmap_exe map_unaligned map_unmap mq \
--- a/none/tests/allexec_prepare_prereq
+++ b/none/tests/allexec_prepare_prereq
@@ -34,5 +34,6 @@
 pair arm                        arm64
 pair mips32                     mips64
 pair nanomips                   nanoMIPS_unexisting_in_64bits
+pair riscv_unexisting_in_32bits riscv64
 
 exit 0
--- a/none/tests/faultstatus.c
+++ b/none/tests/faultstatus.c
@@ -19,7 +19,7 @@
    Hence we get a SIGFPE but the SI_CODE is different from that on
    x86/amd64-linux.
  */
-#if defined(__powerpc__) || defined(__aarch64__)
+#if defined(__powerpc__) || defined(__aarch64__) || defined(__riscv)
 #  define DIVISION_BY_ZERO_TRIGGERS_FPE 0
 #  define DIVISION_BY_ZERO_SI_CODE      SI_TKILL
 #elif defined(__arm__)
--- a/none/tests/libvex_test.c
+++ b/none/tests/libvex_test.c
@@ -76,6 +76,8 @@
    *ga = VexArchMIPS64;
 #elif defined(VGA_nanomips)
    *ga = VexArchNANOMIPS;
+#elif defined(VGA_riscv64)
+   *ga = VexArchRISCV64;
 #else
    missing arch;
 #endif
@@ -113,6 +115,7 @@
          else
             return VexEndnessBE;
       }
+   case VexArchRISCV64: return VexEndnessLE;
    default: failure_exit();
    }
 }
@@ -139,6 +142,7 @@
    case VexArchMIPS64: return VEX_PRID_COMP_MIPS | VEX_MIPS_HOST_FR;
 #endif
    case VexArchNANOMIPS: return 0;
+   case VexArchRISCV64: return 0;
    default: failure_exit();
    }
 }
@@ -156,6 +160,7 @@
    case VexArchMIPS32: return False;
    case VexArchMIPS64: return True;
    case VexArchNANOMIPS: return False;
+   case VexArchRISCV64: return True;
    default: failure_exit();
    }
 }
@@ -275,7 +280,7 @@
    // explicitly via command line arguments.
    if (multiarch) {
       VexArch va;
-      for (va = VexArchX86; va <= VexArchNANOMIPS; va++) {
+      for (va = VexArchX86; va <= VexArchRISCV64; va++) {
          vta.arch_host = va;
          vta.archinfo_host.endness = arch_endness (vta.arch_host);
          vta.archinfo_host.hwcaps = arch_hwcaps (vta.arch_host);
--- /dev/null
+++ b/none/tests/riscv64/Makefile.am
@@ -0,0 +1,29 @@
+
+include $(top_srcdir)/Makefile.tool-tests.am
+
+dist_noinst_SCRIPTS = filter_stderr
+
+EXTRA_DIST = \
+	atomic.stdout.exp atomic.stderr.exp atomic.vgtest \
+	compressed.stdout.exp compressed.stderr.exp compressed.vgtest \
+	csr.stdout.exp csr.stderr.exp csr.vgtest \
+	float32.stdout.exp float32.stderr.exp float32.vgtest \
+	float64.stdout.exp float64.stderr.exp float64.vgtest \
+	integer.stdout.exp integer.stderr.exp integer.vgtest \
+	muldiv.stdout.exp muldiv.stderr.exp muldiv.vgtest
+
+check_PROGRAMS = \
+	allexec \
+	atomic \
+	compressed \
+	csr \
+	float32 \
+	float64 \
+	integer \
+	muldiv
+
+AM_CFLAGS    += @FLAG_M64@
+AM_CXXFLAGS  += @FLAG_M64@
+AM_CCASFLAGS += @FLAG_M64@
+
+allexec_CFLAGS = $(AM_CFLAGS) @FLAG_W_NO_NONNULL@
--- /dev/null
+++ b/none/tests/riscv64/allexec.c
@@ -0,0 +1,56 @@
+#include <assert.h>
+#include <stdio.h>
+#include <string.h>
+#include <sys/types.h>
+#include <sys/wait.h>
+#include <unistd.h>
+
+extern char **environ;
+
+#define S(...) (fprintf(stdout, __VA_ARGS__),fflush(stdout))
+#define FORKEXECWAIT(exec_call) do { \
+      int status;\
+      pid_t child = fork(); \
+      if (child == 0) {exec_call; perror ("exec failed");} \
+      else if (child == -1) perror ("cannot fork\n"); \
+      else if (child != wait (&status)) perror ("error waiting child"); \
+      else S("child exited\n"); \
+   } while (0)
+
+void test_allexec (char *exec)
+{
+   FORKEXECWAIT (execlp(exec, exec, (char *) NULL));
+   FORKEXECWAIT (execlp(exec, exec, "constant_arg1", "constant_arg2",
+                        (char *) NULL));
+   {
+      /* Solaris requires that the argv parameter to execve() isn't NULL, so
+         set it.  Note that this isn't necessary on Linux. */
+      char *const argv[] = {exec, NULL};
+      FORKEXECWAIT (execve(exec, argv, environ));
+   }
+}
+
+
+/* If a single argument "exec" is given, will execute itself
+   (in bi-arch, a 32 bit and 64 bit variant) via various exec system calls.
+   Note that this test can only be run after the prerequisite have been
+   prepared by allexec_prepare_prereq, which will a.o. make links
+   for the allexec32 and allexec64 executables. On single arch build,
+   these links points to the same executable to ensure this test works
+   everywhere the same.
+   No arguments or more arguments means just print its args. */
+int main(int argc, char **argv, char **envp)
+{
+   if ( (argc == 2) && (strcmp (argv[1], "exec") == 0)) {
+      S("%s will exec ./allexec32\n", argv[0]);
+      test_allexec ("./allexec32");
+      S("%s will exec ./allexec64\n", argv[0]);
+      test_allexec ("./allexec64");
+   } else {
+      int i;
+      S("program exec-ed:");
+      for (i = 0; i < argc; i++) S(" %s", argv[i]);
+      S("\n");
+   }
+   return 0;
+}
--- /dev/null
+++ b/none/tests/riscv64/atomic.c
@@ -0,0 +1,280 @@
+/* Tests for the RV64A standard atomic instruction-set extension. */
+
+#include "testinst.h"
+
+static void test_atomic_shared(void)
+{
+   printf("RV64A atomic instruction set, shared operations\n");
+
+   /* ------------------- lr.w rd, (rs1) -------------------- */
+   /* ----------------- sc.w rd, rs2, (rs1) ----------------- */
+   TESTINST_2_1_LRSC(4, "lr.w a0, (a2)", "sc.w a1, a0, (a2)", a0, a1, a2);
+
+   TESTINST_2_1_LRSC(4, "lr.w t4, (t6)", "sc.w t5, t4, (t6)", t4, t5, t6);
+   TESTINST_2_1_LRSC(4, "lr.w zero, (a1)", "sc.w a0, zero, (a1)", zero, a0, a1);
+   TESTINST_2_1_LRSC(4, "lr.w a0, (a1)", "sc.w zero, a0, (a1)", a0, zero, a1);
+
+   /* -------------- amoswap.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoswap.w a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoswap.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoswap.w zero, a0, (a1)", 0xabcdef0123456789, zero,
+                     a0, a1);
+   TESTINST_1_2_AMOX(4, "amoswap.w a0, zero, (a1)", 0xabcdef0123456789, a0,
+                     zero, a1);
+
+   /* --------------- amoadd.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoadd.w a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoadd.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoadd.w zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoadd.w a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amoxor.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoxor.w a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoxor.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoxor.w zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoxor.w a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amoand.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoand.w a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoand.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoand.w zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoand.w a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amoor.w rd, rs2, (rs1) ---------------- */
+   TESTINST_1_2_AMOX(4, "amoor.w a0, a1, (a2)", 0xabcdef0123456789, a0, a1, a2);
+
+   TESTINST_1_2_AMOX(4, "amoor.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5, t6);
+   TESTINST_1_2_AMOX(4, "amoor.w zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoor.w a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amomin.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amomin.w a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomin.w a0, a1, (a2)", 0x000000007fffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomin.w a0, a1, (a2)", 0x0000000080000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomin.w a0, a1, (a2)", 0x00000000ffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amomin.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amomin.w zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amomin.w a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amomax.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amomax.w a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomax.w a0, a1, (a2)", 0x000000007fffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomax.w a0, a1, (a2)", 0x0000000080000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomax.w a0, a1, (a2)", 0x00000000ffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amomax.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amomax.w zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amomax.w a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* -------------- amominu.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amominu.w a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amominu.w a0, a1, (a2)", 0x000000007fffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amominu.w a0, a1, (a2)", 0x0000000080000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amominu.w a0, a1, (a2)", 0x00000000ffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amominu.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amominu.w zero, a0, (a1)", 0xabcdef0123456789, zero,
+                     a0, a1);
+   TESTINST_1_2_AMOX(4, "amominu.w a0, zero, (a1)", 0xabcdef0123456789, a0,
+                     zero, a1);
+
+   /* -------------- amomaxu.w rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amomaxu.w a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomaxu.w a0, a1, (a2)", 0x000000007fffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomaxu.w a0, a1, (a2)", 0x0000000080000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomaxu.w a0, a1, (a2)", 0x00000000ffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amomaxu.w t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amomaxu.w zero, a0, (a1)", 0xabcdef0123456789, zero,
+                     a0, a1);
+   TESTINST_1_2_AMOX(4, "amomaxu.w a0, zero, (a1)", 0xabcdef0123456789, a0,
+                     zero, a1);
+
+   printf("\n");
+}
+
+static void test_atomic_additions(void)
+{
+   printf("RV64A atomic instruction set, additions\n");
+
+   /* ------------------- lr.d rd, (rs1) -------------------- */
+   /* ----------------- sc.d rd, rs2, (rs1) ----------------- */
+   TESTINST_2_1_LRSC(4, "lr.d a0, (a2)", "sc.d a1, a0, (a2)", a0, a1, a2);
+
+   TESTINST_2_1_LRSC(4, "lr.d t4, (t6)", "sc.d t5, t4, (t6)", t4, t5, t6);
+   TESTINST_2_1_LRSC(4, "lr.d zero, (a1)", "sc.d a0, zero, (a1)", zero, a0, a1);
+   TESTINST_2_1_LRSC(4, "lr.d a0, (a1)", "sc.d zero, a0, (a1)", a0, zero, a1);
+
+   /* -------------- amoswap.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoswap.d a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoswap.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoswap.d zero, a0, (a1)", 0xabcdef0123456789, zero,
+                     a0, a1);
+   TESTINST_1_2_AMOX(4, "amoswap.d a0, zero, (a1)", 0xabcdef0123456789, a0,
+                     zero, a1);
+
+   /* --------------- amoadd.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoadd.d a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoadd.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoadd.d zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoadd.d a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amoxor.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoxor.d a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoxor.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoxor.d zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoxor.d a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amoand.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amoand.d a0, a1, (a2)", 0xabcdef0123456789, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amoand.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amoand.d zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoand.d a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amoor.d rd, rs2, (rs1) ---------------- */
+   TESTINST_1_2_AMOX(4, "amoor.d a0, a1, (a2)", 0xabcdef0123456789, a0, a1, a2);
+
+   TESTINST_1_2_AMOX(4, "amoor.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5, t6);
+   TESTINST_1_2_AMOX(4, "amoor.d zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amoor.d a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amomin.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amomin.d a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomin.d a0, a1, (a2)", 0x7fffffffffffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomin.d a0, a1, (a2)", 0x8000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomin.d a0, a1, (a2)", 0xffffffffffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amomin.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amomin.d zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amomin.d a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* --------------- amomax.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amomax.d a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomax.d a0, a1, (a2)", 0x7fffffffffffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomax.d a0, a1, (a2)", 0x8000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomax.d a0, a1, (a2)", 0xffffffffffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amomax.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amomax.d zero, a0, (a1)", 0xabcdef0123456789, zero, a0,
+                     a1);
+   TESTINST_1_2_AMOX(4, "amomax.d a0, zero, (a1)", 0xabcdef0123456789, a0, zero,
+                     a1);
+
+   /* -------------- amominu.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amominu.d a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amominu.d a0, a1, (a2)", 0x7fffffffffffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amominu.d a0, a1, (a2)", 0x8000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amominu.d a0, a1, (a2)", 0xffffffffffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amominu.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amominu.d zero, a0, (a1)", 0xabcdef0123456789, zero,
+                     a0, a1);
+   TESTINST_1_2_AMOX(4, "amominu.d a0, zero, (a1)", 0xabcdef0123456789, a0,
+                     zero, a1);
+
+   /* -------------- amomaxu.d rd, rs2, (rs1) --------------- */
+   TESTINST_1_2_AMOX(4, "amomaxu.d a0, a1, (a2)", 0x0000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomaxu.d a0, a1, (a2)", 0x7fffffffffffffff, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomaxu.d a0, a1, (a2)", 0x8000000000000000, a0, a1,
+                     a2);
+   TESTINST_1_2_AMOX(4, "amomaxu.d a0, a1, (a2)", 0xffffffffffffffff, a0, a1,
+                     a2);
+
+   TESTINST_1_2_AMOX(4, "amomaxu.d t4, t5, (t6)", 0xabcdef0123456789, t4, t5,
+                     t6);
+   TESTINST_1_2_AMOX(4, "amomaxu.d zero, a0, (a1)", 0xabcdef0123456789, zero,
+                     a0, a1);
+   TESTINST_1_2_AMOX(4, "amomaxu.d a0, zero, (a1)", 0xabcdef0123456789, a0,
+                     zero, a1);
+}
+
+int main(void)
+{
+   test_atomic_shared();
+   test_atomic_additions();
+   return 0;
+}
--- /dev/null
+++ b/none/tests/riscv64/atomic.stdout.exp
@@ -0,0 +1,467 @@
+RV64A atomic instruction set, shared operations
+lr.w a0, (a2) ::
+  inputs: a2=&area_mid
+  output: a0=0xffffffffaf27d13b
+sc.w a1, a0, (a2) ::
+  inputs: a2=&area_mid, a0=0x0000000050d82ec4
+  output: a1=0x0000000000000000
+  [+000]  c4 2e d8 50 .. .. .. .. .. .. .. .. .. .. .. ..
+sc.w a1, a0, (a2) ::
+  inputs: a2=&area_mid, a0=0xffffffffaf27d13b
+  output: a1=0x0000000000000001
+lr.w t4, (t6) ::
+  inputs: t6=&area_mid
+  output: t4=0x0000000056a044b2
+sc.w t5, t4, (t6) ::
+  inputs: t6=&area_mid, t4=0xffffffffa95fbb4d
+  output: t5=0x0000000000000000
+  [+000]  4d bb 5f a9 .. .. .. .. .. .. .. .. .. .. .. ..
+sc.w t5, t4, (t6) ::
+  inputs: t6=&area_mid, t4=0x0000000056a044b2
+  output: t5=0x0000000000000001
+lr.w zero, (a1) ::
+  inputs: a1=&area_mid
+  output: zero=0x0000000000000000
+sc.w a0, zero, (a1) ::
+  inputs: a1=&area_mid, zero=0x0000000000000000
+  output: a0=0x0000000000000000
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+sc.w a0, zero, (a1) ::
+  inputs: a1=&area_mid, zero=0x0000000000000000
+  output: a0=0x0000000000000001
+lr.w a0, (a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffb7839b97
+sc.w zero, a0, (a1) ::
+  inputs: a1=&area_mid, a0=0x00000000487c6468
+  output: zero=0x0000000000000000
+  [+000]  68 64 7c 48 .. .. .. .. .. .. .. .. .. .. .. ..
+sc.w zero, a0, (a1) ::
+  inputs: a1=&area_mid, a0=0xffffffffb7839b97
+  output: zero=0x0000000000000000
+amoswap.w a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0xffffffffa705f65d
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amoswap.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x000000003df76c96
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amoswap.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amoswap.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xffffffffc95c9810
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+amoadd.w a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0xfffffffff7e8c6a9
+  [+000]  32 2e 2e 1b .. .. .. .. .. .. .. .. .. .. .. ..
+amoadd.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x000000003b13ff64
+  [+000]  ed 66 59 5e .. .. .. .. .. .. .. .. .. .. .. ..
+amoadd.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  f7 65 31 d4 .. .. .. .. .. .. .. .. .. .. .. ..
+amoadd.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x00000000727c80f3
+  no memory changes
+amoxor.w a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0xffffffff9ed0411e
+  [+000]  97 26 95 bd .. .. .. .. .. .. .. .. .. .. .. ..
+amoxor.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x0000000050f6fd1c
+  [+000]  95 9a b3 73 .. .. .. .. .. .. .. .. .. .. .. ..
+amoxor.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  91 17 b2 80 .. .. .. .. .. .. .. .. .. .. .. ..
+amoxor.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xffffffffb3e1553f
+  no memory changes
+amoand.w a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0xffffffff9dbf68bc
+  [+000]  88 60 05 01 .. .. .. .. .. .. .. .. .. .. .. ..
+amoand.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x000000007d9d67bc
+  [+000]  88 .. 05 21 .. .. .. .. .. .. .. .. .. .. .. ..
+amoand.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  09 04 00 22 .. .. .. .. .. .. .. .. .. .. .. ..
+amoand.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xffffffff8c8b14f4
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+amoor.w a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0xfffffffff4b23b84
+  [+000]  8d 7f f7 f7 .. .. .. .. .. .. .. .. .. .. .. ..
+amoor.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xffffffffc10a3c47
+  [+000]  cf 7f 4f e3 .. .. .. .. .. .. .. .. .. .. .. ..
+amoor.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  e9 f7 df 33 .. .. .. .. .. .. .. .. .. .. .. ..
+amoor.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xfffffffffc7bc013
+  no memory changes
+amomin.w a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0xffffffffa2acb976
+  no memory changes
+amomin.w a0, a1, (a2) ::
+  inputs: a1=0x000000007fffffff, a2=&area_mid
+  output: a0=0x000000001d3d7dbb
+  no memory changes
+amomin.w a0, a1, (a2) ::
+  inputs: a1=0x0000000080000000, a2=&area_mid
+  output: a0=0xffffffff8a3ac80e
+  [+000]  00 00 00 80 .. .. .. .. .. .. .. .. .. .. .. ..
+amomin.w a0, a1, (a2) ::
+  inputs: a1=0x00000000ffffffff, a2=&area_mid
+  output: a0=0x0000000004b0569c
+  [+000]  ff ff ff ff .. .. .. .. .. .. .. .. .. .. .. ..
+amomin.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xffffffffa7aae391
+  no memory changes
+amomin.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+amomin.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xffffffffdb5ce85e
+  no memory changes
+amomax.w a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0xffffffffa32bd88e
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+amomax.w a0, a1, (a2) ::
+  inputs: a1=0x000000007fffffff, a2=&area_mid
+  output: a0=0x0000000004afb8d5
+  [+000]  ff ff ff 7f .. .. .. .. .. .. .. .. .. .. .. ..
+amomax.w a0, a1, (a2) ::
+  inputs: a1=0x0000000080000000, a2=&area_mid
+  output: a0=0x000000001bf2425f
+  no memory changes
+amomax.w a0, a1, (a2) ::
+  inputs: a1=0x00000000ffffffff, a2=&area_mid
+  output: a0=0x0000000004033357
+  no memory changes
+amomax.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xffffffffd9eb46ea
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amomax.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amomax.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xffffffffbd76c58f
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+amominu.w a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0x00000000042fa9fa
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+amominu.w a0, a1, (a2) ::
+  inputs: a1=0x000000007fffffff, a2=&area_mid
+  output: a0=0xffffffffa7f19faf
+  [+000]  ff ff ff 7f .. .. .. .. .. .. .. .. .. .. .. ..
+amominu.w a0, a1, (a2) ::
+  inputs: a1=0x0000000080000000, a2=&area_mid
+  output: a0=0xffffffffc5c764db
+  [+000]  00 00 00 80 .. .. .. .. .. .. .. .. .. .. .. ..
+amominu.w a0, a1, (a2) ::
+  inputs: a1=0x00000000ffffffff, a2=&area_mid
+  output: a0=0x0000000077beb4a9
+  no memory changes
+amominu.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xffffffffdbe14b46
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amominu.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+amominu.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x0000000028dc3c9c
+  [+000]  00 00 00 00 .. .. .. .. .. .. .. .. .. .. .. ..
+amomaxu.w a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0x0000000049cc0fac
+  no memory changes
+amomaxu.w a0, a1, (a2) ::
+  inputs: a1=0x000000007fffffff, a2=&area_mid
+  output: a0=0xffffffff8b19183c
+  no memory changes
+amomaxu.w a0, a1, (a2) ::
+  inputs: a1=0x0000000080000000, a2=&area_mid
+  output: a0=0x000000000acd1475
+  [+000]  00 00 00 80 .. .. .. .. .. .. .. .. .. .. .. ..
+amomaxu.w a0, a1, (a2) ::
+  inputs: a1=0x00000000ffffffff, a2=&area_mid
+  output: a0=0xffffffffe3f6bf86
+  [+000]  ff ff ff ff .. .. .. .. .. .. .. .. .. .. .. ..
+amomaxu.w t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x0000000031a0d599
+  no memory changes
+amomaxu.w zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+amomaxu.w a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xffffffff9fa43077
+  no memory changes
+
+RV64A atomic instruction set, additions
+lr.d a0, (a2) ::
+  inputs: a2=&area_mid
+  output: a0=0x05d75ec6f616ee9a
+sc.d a1, a0, (a2) ::
+  inputs: a2=&area_mid, a0=0xfa28a13909e91165
+  output: a1=0x0000000000000000
+  [+000]  65 11 e9 09 39 a1 28 fa .. .. .. .. .. .. .. ..
+sc.d a1, a0, (a2) ::
+  inputs: a2=&area_mid, a0=0x05d75ec6f616ee9a
+  output: a1=0x0000000000000001
+lr.d t4, (t6) ::
+  inputs: t6=&area_mid
+  output: t4=0x141625713239066f
+sc.d t5, t4, (t6) ::
+  inputs: t6=&area_mid, t4=0xebe9da8ecdc6f990
+  output: t5=0x0000000000000000
+  [+000]  90 f9 c6 cd 8e da e9 eb .. .. .. .. .. .. .. ..
+sc.d t5, t4, (t6) ::
+  inputs: t6=&area_mid, t4=0x141625713239066f
+  output: t5=0x0000000000000001
+lr.d zero, (a1) ::
+  inputs: a1=&area_mid
+  output: zero=0x0000000000000000
+sc.d a0, zero, (a1) ::
+  inputs: a1=&area_mid, zero=0x0000000000000000
+  output: a0=0x0000000000000000
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+sc.d a0, zero, (a1) ::
+  inputs: a1=&area_mid, zero=0x0000000000000000
+  output: a0=0x0000000000000001
+lr.d a0, (a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xf2db8f44cbbf37e2
+sc.d zero, a0, (a1) ::
+  inputs: a1=&area_mid, a0=0x0d2470bb3440c81d
+  output: zero=0x0000000000000000
+  [+000]  1d c8 40 34 bb 70 24 0d .. .. .. .. .. .. .. ..
+sc.d zero, a0, (a1) ::
+  inputs: a1=&area_mid, a0=0xf2db8f44cbbf37e2
+  output: zero=0x0000000000000000
+amoswap.d a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0x78fb29445f3bc8d7
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amoswap.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x34a901384a97a32f
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amoswap.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amoswap.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xb9dd5dab8e212ab7
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+amoadd.d a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0x3bfcd80321664d3e
+  [+000]  c7 b4 ab 44 04 c7 ca e7 .. .. .. .. .. .. .. ..
+amoadd.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x63d9810079bbabd9
+  [+000]  62 13 01 9d 01 70 a7 0f .. .. .. .. .. .. .. ..
+amoadd.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  3a 68 72 d5 8d c4 0e b7 .. .. .. .. .. .. .. ..
+amoadd.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x10fd4e94e9c808f5
+  no memory changes
+amoxor.d a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0x4edb6a053a967ecf
+  [+000]  46 19 d3 19 04 85 16 e5 .. .. .. .. .. .. .. ..
+amoxor.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xa1a7a4c9c0a51f6b
+  [+000]  e2 78 e0 e3 c8 4b 6a 0a .. .. .. .. .. .. .. ..
+amoxor.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  7e c0 45 b4 cf 96 e3 4e .. .. .. .. .. .. .. ..
+amoxor.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0xf63a63fedcb4d29c
+  no memory changes
+amoand.d a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0xb097e047aacc5b89
+  [+000]  .. 43 44 22 01 .. 85 a0 .. .. .. .. .. .. .. ..
+amoand.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xef136b941e54ffe8
+  [+000]  88 67 44 02 00 .. 01 ab .. .. .. .. .. .. .. ..
+amoand.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  81 61 41 03 01 .. 49 8b .. .. .. .. .. .. .. ..
+amoand.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x6c949cea66e687ae
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+amoor.d a0, a1, (a2) ::
+  inputs: a1=0xabcdef0123456789, a2=&area_mid
+  output: a0=0x623139cb7207e36c
+  [+000]  ed e7 47 73 .. ff fd eb .. .. .. .. .. .. .. ..
+amoor.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x4c1cd56194c94a4e
+  [+000]  cf 6f cd b7 .. ff dd ef .. .. .. .. .. .. .. ..
+amoor.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  ff .. 77 .. 97 ef ed ab .. .. .. .. .. .. .. ..
+amoor.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x710cf757885d2728
+  no memory changes
+amomin.d a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0x63a8769192481679
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+amomin.d a0, a1, (a2) ::
+  inputs: a1=0x7fffffffffffffff, a2=&area_mid
+  output: a0=0xb9c3e32f2103009d
+  no memory changes
+amomin.d a0, a1, (a2) ::
+  inputs: a1=0x8000000000000000, a2=&area_mid
+  output: a0=0x5127ba1c529aa0bf
+  [+000]  00 00 00 00 00 00 00 80 .. .. .. .. .. .. .. ..
+amomin.d a0, a1, (a2) ::
+  inputs: a1=0xffffffffffffffff, a2=&area_mid
+  output: a0=0x06a17746411ab40c
+  [+000]  ff ff ff ff ff ff ff ff .. .. .. .. .. .. .. ..
+amomin.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xb3fd9698098ef5b0
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amomin.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amomin.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x698bec649583f5aa
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+amomax.d a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0x2a541ab7911c2b5a
+  no memory changes
+amomax.d a0, a1, (a2) ::
+  inputs: a1=0x7fffffffffffffff, a2=&area_mid
+  output: a0=0x532f9ae1d7da8010
+  [+000]  ff ff ff ff ff ff ff 7f .. .. .. .. .. .. .. ..
+amomax.d a0, a1, (a2) ::
+  inputs: a1=0x8000000000000000, a2=&area_mid
+  output: a0=0xc2e9e9cf82c7aff8
+  no memory changes
+amomax.d a0, a1, (a2) ::
+  inputs: a1=0xffffffffffffffff, a2=&area_mid
+  output: a0=0x514c816eaff2763f
+  no memory changes
+amomax.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0xde24e0a879648e11
+  no memory changes
+amomax.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+amomax.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x5d68e1a25652a804
+  no memory changes
+amominu.d a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0x086a7a39a1e6217d
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+amominu.d a0, a1, (a2) ::
+  inputs: a1=0x7fffffffffffffff, a2=&area_mid
+  output: a0=0x2112ca1cf9f1dd31
+  no memory changes
+amominu.d a0, a1, (a2) ::
+  inputs: a1=0x8000000000000000, a2=&area_mid
+  output: a0=0x822c4c377b82984c
+  [+000]  00 00 00 00 00 00 00 80 .. .. .. .. .. .. .. ..
+amominu.d a0, a1, (a2) ::
+  inputs: a1=0xffffffffffffffff, a2=&area_mid
+  output: a0=0x08847c7642a20df9
+  no memory changes
+amominu.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x8fe6d7c56a5ff965
+  no memory changes
+amominu.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amominu.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x11f7fa4450de2529
+  [+000]  00 00 00 00 00 00 00 00 .. .. .. .. .. .. .. ..
+amomaxu.d a0, a1, (a2) ::
+  inputs: a1=0x0000000000000000, a2=&area_mid
+  output: a0=0xc33ebc4b44b8ddd8
+  no memory changes
+amomaxu.d a0, a1, (a2) ::
+  inputs: a1=0x7fffffffffffffff, a2=&area_mid
+  output: a0=0xe6c097130b5efcf6
+  no memory changes
+amomaxu.d a0, a1, (a2) ::
+  inputs: a1=0x8000000000000000, a2=&area_mid
+  output: a0=0x56470887bfdd3daf
+  [+000]  00 00 00 00 00 00 00 80 .. .. .. .. .. .. .. ..
+amomaxu.d a0, a1, (a2) ::
+  inputs: a1=0xffffffffffffffff, a2=&area_mid
+  output: a0=0xef9f8c927c405d2f
+  [+000]  ff ff ff ff ff ff ff ff .. .. .. .. .. .. .. ..
+amomaxu.d t4, t5, (t6) ::
+  inputs: t5=0xabcdef0123456789, t6=&area_mid
+  output: t4=0x8d969e225f9318a0
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amomaxu.d zero, a0, (a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  output: zero=0x0000000000000000
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+amomaxu.d a0, zero, (a1) ::
+  inputs: zero=0xabcdef0123456789, a1=&area_mid
+  output: a0=0x478d5d7e053a4e0c
+  no memory changes
--- /dev/null
+++ b/none/tests/riscv64/atomic.vgtest
@@ -0,0 +1,2 @@
+prog: atomic
+vgopts: -q
--- /dev/null
+++ b/none/tests/riscv64/compressed.c
@@ -0,0 +1,456 @@
+/* Tests for the RV64C standard compressed instruction-set extension. */
+
+#include "testinst.h"
+
+static void test_compressed_00(void)
+{
+   printf("RV64C compressed instruction set, quadrant 0\n");
+
+   /* ------------- c.addi4spn rd, nzuimm[9:2] -------------- */
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 4", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 8", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 16", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 32", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 64", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 128", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 256", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 512", 0x0000000000001000, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 1020", 0x0000000000001000, a0, sp);
+
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 4", 0x000000007ffffffc, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 4", 0x00000000fffffffb, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a0, sp, 4", 0x00000000fffffffc, a0, sp);
+   TESTINST_1_1(2, "c.addi4spn a5, sp, 4", 0x0000000000001000, a0, sp);
+
+   /* -------------- c.fld rd, uimm[7:3](rs1) --------------- */
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 0(a1)", fa0, a1);
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 8(a1)", fa0, a1);
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 16(a1)", fa0, a1);
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 32(a1)", fa0, a1);
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 64(a1)", fa0, a1);
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 128(a1)", fa0, a1);
+   TESTINST_1_1_FLOAD(2, "c.fld fa0, 248(a1)", fa0, a1);
+
+   TESTINST_1_1_FLOAD(2, "c.fld fa4, 0(a5)", fa4, a5);
+
+   /* --------------- c.lw rd, uimm[6:2](rs1) --------------- */
+   TESTINST_1_1_LOAD(2, "c.lw a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.lw a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.lw a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.lw a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.lw a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.lw a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.lw a0, 124(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(2, "c.lw a4, 0(a5)", a4, a5);
+
+   /* --------------- c.ld rd, uimm[7:3](rs1) --------------- */
+   TESTINST_1_1_LOAD(2, "c.ld a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.ld a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.ld a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.ld a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.ld a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.ld a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(2, "c.ld a0, 248(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(2, "c.ld a4, 0(a5)", a4, a5);
+
+   /* -------------- c.fsd rs2, uimm[7:3](rs1) -------------- */
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 0(a1)", 0xabcdef0123456789, fa0, a1);
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 8(a1)", 0xabcdef0123456789, fa0, a1);
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 16(a1)", 0xabcdef0123456789, fa0, a1);
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 32(a1)", 0xabcdef0123456789, fa0, a1);
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 64(a1)", 0xabcdef0123456789, fa0, a1);
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 128(a1)", 0xabcdef0123456789, fa0, a1);
+   TESTINST_0_2_FSTORE(2, "c.fsd fa0, 248(a1)", 0xabcdef0123456789, fa0, a1);
+
+   TESTINST_0_2_FSTORE(2, "c.fsd fa4, 0(a5)", 0xabcdef0123456789, fa4, a5);
+
+   /* -------------- c.sw rs2, uimm[6:2](rs1) --------------- */
+   TESTINST_0_2_STORE(2, "c.sw a0, 0(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sw a0, 4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sw a0, 8(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sw a0, 16(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sw a0, 32(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sw a0, 64(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sw a0, 124(a1)", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_0_2_STORE(2, "c.sw a4, 0(a5)", 0xabcdef0123456789, a4, a5);
+
+   /* -------------- c.sd rs2, uimm[7:3](rs1) --------------- */
+   TESTINST_0_2_STORE(2, "c.sd a0, 0(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sd a0, 8(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sd a0, 16(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sd a0, 32(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sd a0, 64(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sd a0, 128(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(2, "c.sd a0, 248(a1)", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_0_2_STORE(2, "c.sd a4, 0(a5)", 0xabcdef0123456789, a4, a5);
+
+   printf("\n");
+}
+
+static void test_compressed_01(void)
+{
+   printf("RV64C compressed instruction set, quadrant 1\n");
+
+   /* ------------------------ c.nop ------------------------ */
+   TESTINST_0_0(2, "c.nop");
+
+   /* -------------- c.addi rd_rs1, nzimm[5:0] -------------- */
+   TESTINST_1_1(2, "c.addi a0, 1", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 2", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 4", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 8", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 16", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 31", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, -1", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, -32", 0x0000000000001000, a0, a0);
+
+   TESTINST_1_1(2, "c.addi a0, 1", 0x000000007fffffff, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 1", 0x00000000fffffffe, a0, a0);
+   TESTINST_1_1(2, "c.addi a0, 1", 0x00000000ffffffff, a0, a0);
+   TESTINST_1_1(2, "c.addi t6, 1", 0x0000000000001000, t6, t6);
+
+   /* -------------- c.addiw rd_rs1, imm[5:0] --------------- */
+   TESTINST_1_1(2, "c.addiw a0, 0", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 1", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 2", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 4", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 8", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 16", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 31", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, -1", 0x0000000000001000, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, -32", 0x0000000000001000, a0, a0);
+
+   TESTINST_1_1(2, "c.addiw a0, 1", 0x000000007fffffff, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 1", 0x00000000fffffffe, a0, a0);
+   TESTINST_1_1(2, "c.addiw a0, 1", 0x00000000ffffffff, a0, a0);
+   TESTINST_1_1(2, "c.addiw t6, 0", 0x0000000000001000, t6, t6);
+
+   /* ------------------ c.li rd, imm[5:0] ------------------ */
+   TESTINST_1_0(2, "c.li a0, 0", a0);
+   TESTINST_1_0(2, "c.li a0, 1", a0);
+   TESTINST_1_0(2, "c.li a0, 2", a0);
+   TESTINST_1_0(2, "c.li a0, 4", a0);
+   TESTINST_1_0(2, "c.li a0, 8", a0);
+   TESTINST_1_0(2, "c.li a0, 15", a0);
+   TESTINST_1_0(2, "c.li a0, -1", a0);
+   TESTINST_1_0(2, "c.li a0, -16", a0);
+
+   TESTINST_1_0(2, "c.li t6, 1", t6);
+
+   /* ---------------- c.addi16sp nzimm[9:4] ---------------- */
+   TESTINST_1_1(2, "c.addi16sp sp, 16", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 32", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 64", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 128", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 256", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 496", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, -16", 0x0000000000001000, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, -512", 0x0000000000001000, sp, sp);
+
+   TESTINST_1_1(2, "c.addi16sp sp, 16", 0x000000007ffffff0, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 16", 0x00000000ffffffef, sp, sp);
+   TESTINST_1_1(2, "c.addi16sp sp, 16", 0x00000000fffffff0, sp, sp);
+
+   /* --------------- c.lui rd, nzimm[17:12] ---------------- */
+   TESTINST_1_0(2, "c.lui a0, 1", a0);
+   TESTINST_1_0(2, "c.lui a0, 2", a0);
+   TESTINST_1_0(2, "c.lui a0, 4", a0);
+   TESTINST_1_0(2, "c.lui a0, 8", a0);
+   TESTINST_1_0(2, "c.lui a0, 16", a0);
+   TESTINST_1_0(2, "c.lui a0, 31", a0);
+   TESTINST_1_0(2, "c.lui a0, 0xfffff" /* -1 */, a0);
+   TESTINST_1_0(2, "c.lui a0, 0xfffe0" /* -32 */, a0);
+
+   TESTINST_1_0(2, "c.lui t6, 1", t6);
+
+   /* ------------- c.srli rd_rs1, nzuimm[5:0] -------------- */
+   TESTINST_1_1(2, "c.srli a0, 1", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srli a0, 2", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srli a0, 4", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srli a0, 8", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srli a0, 16", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srli a0, 32", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srli a0, 63", 0xabcdef0123456789, a0, a0);
+
+   TESTINST_1_1(2, "c.srli a5, 1", 0xabcdef0123456789, a5, a5);
+
+   /* ------------- c.srai rd_rs1, nzuimm[5:0] -------------- */
+   TESTINST_1_1(2, "c.srai a0, 1", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srai a0, 2", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srai a0, 4", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srai a0, 8", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srai a0, 16", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srai a0, 32", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.srai a0, 63", 0xabcdef0123456789, a0, a0);
+
+   TESTINST_1_1(2, "c.srai a5, 1", 0xabcdef0123456789, a5, a5);
+
+   /* --------------- c.andi rd_rs1, imm[5:0] --------------- */
+   TESTINST_1_1(2, "c.andi a0, 0", 0xffffffffffffffff, a0, a0);
+   TESTINST_1_1(2, "c.andi a0, 1", 0xffffffffffffffff, a0, a0);
+   TESTINST_1_1(2, "c.andi a0, 2", 0xffffffffffffffff, a0, a0);
+   TESTINST_1_1(2, "c.andi a0, 4", 0xffffffffffffffff, a0, a0);
+   TESTINST_1_1(2, "c.andi a0, 8", 0xffffffffffffffff, a0, a0);
+   TESTINST_1_1(2, "c.andi a0, 16", 0xffffffffffffffff, a0, a0);
+   TESTINST_1_1(2, "c.andi a0, 31", 0xffffffffffffffff, a0, a0);
+
+   TESTINST_1_1(2, "c.andi a5, 0", 0xffffffffffffffff, a5, a5);
+
+   /* ------------------ c.sub rd_rs1, rs2 ------------------ */
+   TESTINST_1_2(2, "c.sub a0, a1", 0x0000000000001000, 0x0000000000000fff, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.sub a0, a1", 0x0000000000001000, 0x0000000000001000, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.sub a0, a1", 0x0000000000001000, 0x0000000000001001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.sub a0, a1", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.sub a0, a1", 0x0000000100000000, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.sub a4, a5", 0x0000000000001000, 0x0000000000000fff, a4,
+                a4, a5);
+
+   /* ------------------ c.xor rd_rs1, rs2 ------------------ */
+   TESTINST_1_2(2, "c.xor a0, a1", 0x0000ffff0000ffff, 0x00000000ffffffff, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.xor a4, a5", 0x0000ffff0000ffff, 0x00000000ffffffff, a4,
+                a4, a5);
+
+   /* ------------------ c.or rd_rs1, rs2 ------------------- */
+   TESTINST_1_2(2, "c.or a0, a1", 0x0000ffff0000ffff, 0x00000000ffffffff, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.or a4, a5", 0x0000ffff0000ffff, 0x00000000ffffffff, a4,
+                a4, a5);
+
+   /* ------------------ c.and rd_rs1, rs2 ------------------ */
+   TESTINST_1_2(2, "c.and a0, a1", 0x0000ffff0000ffff, 0x00000000ffffffff, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.and a4, a5", 0x0000ffff0000ffff, 0x00000000ffffffff, a4,
+                a4, a5);
+
+   /* ----------------- c.subw rd_rs1, rs2 ------------------ */
+   TESTINST_1_2(2, "c.subw a0, a1", 0x0000000000001000, 0x0000000000000fff, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.subw a0, a1", 0x0000000000001000, 0x0000000000001000, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.subw a0, a1", 0x0000000000001000, 0x0000000000001001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.subw a0, a1", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.subw a0, a1", 0x0000000100000000, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.subw a4, a5", 0x0000000000001000, 0x0000000000000fff, a4,
+                a4, a5);
+
+   /* ----------------- c.addw rd_rs1, rs2 ------------------ */
+   TESTINST_1_2(2, "c.addw a0, a1", 0x0000000000001000, 0x0000000000002000, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.addw a0, a1", 0x000000007fffffff, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.addw a0, a1", 0x00000000fffffffe, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.addw a0, a1", 0x00000000ffffffff, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.addw a0, a1", 0xfffffffffffffffe, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.addw a0, a1", 0xffffffffffffffff, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.addw a4, a5", 0x0000000000001000, 0x0000000000002000, a4,
+                a4, a5);
+
+   /* -------------------- c.j imm[11:1] -------------------- */
+   TESTINST_0_0_J_RANGE(2, "c.j .+4", 4);
+   TESTINST_0_0_J_RANGE(2, "c.j .+6", 6);
+   TESTINST_0_0_J_RANGE(2, "c.j .+8", 8);
+   TESTINST_0_0_J_RANGE(2, "c.j .+16", 16);
+   TESTINST_0_0_J_RANGE(2, "c.j .+32", 32);
+   TESTINST_0_0_J_RANGE(2, "c.j .+64", 64);
+   TESTINST_0_0_J_RANGE(2, "c.j .+128", 128);
+   TESTINST_0_0_J_RANGE(2, "c.j .+256", 256);
+   TESTINST_0_0_J_RANGE(2, "c.j .+512", 512);
+   TESTINST_0_0_J_RANGE(2, "c.j .+1024", 1024);
+   TESTINST_0_0_J_RANGE(2, "c.j .+2044", 2044);
+   TESTINST_0_0_J_RANGE(2, "c.j .-4", -4);
+   TESTINST_0_0_J_RANGE(2, "c.j .-6", -6);
+   TESTINST_0_0_J_RANGE(2, "c.j .-2048", -2048);
+
+   /* ---------------- c.beqz rs1, imm[8:1] ----------------- */
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+4", 0, 4, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+6", 0, 6, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+8", 0, 8, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+16", 0, 16, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+32", 0, 32, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+64", 0, 64, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+128", 0, 128, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .+252", 0, 252, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .-4", 0, -4, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .-6", 0, -6, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a0, .-256", 0, -256, a0);
+
+   TESTINST_0_1_BxxZ_RANGE(2, "c.beqz a5, .+4", 0, 4, a5);
+   TESTINST_0_1_BxxZ_COND(2, "c.beqz a0, 1f", 0, a0);
+   TESTINST_0_1_BxxZ_COND(2, "c.beqz a0, 1f", 1, a0);
+
+   /* ---------------- c.bnez rs1, imm[8:1] ----------------- */
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+4", 1, 4, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+6", 1, 6, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+8", 1, 8, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+16", 1, 16, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+32", 1, 32, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+64", 1, 64, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+128", 1, 128, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .+252", 1, 252, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .-4", 1, -4, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .-6", 1, -6, a0);
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a0, .-256", 1, -256, a0);
+
+   TESTINST_0_1_BxxZ_RANGE(2, "c.bnez a5, .+4", 1, 4, a5);
+   TESTINST_0_1_BxxZ_COND(2, "c.bnez a0, 1f", 0, a0);
+   TESTINST_0_1_BxxZ_COND(2, "c.bnez a0, 1f", 1, a0);
+
+   printf("\n");
+}
+
+static void test_compressed_10(void)
+{
+   printf("RV64C compressed instruction set, quadrant 2\n");
+
+   /* ------------- c.slli rd_rs1, nzuimm[5:0] -------------- */
+   TESTINST_1_1(2, "c.slli a0, 1", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.slli a0, 2", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.slli a0, 4", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.slli a0, 8", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.slli a0, 16", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.slli a0, 32", 0xabcdef0123456789, a0, a0);
+   TESTINST_1_1(2, "c.slli a0, 63", 0xabcdef0123456789, a0, a0);
+
+   TESTINST_1_1(2, "c.slli a5, 1", 0xabcdef0123456789, a5, a5);
+
+   /* -------------- c.fldsp rd, uimm[8:3](x2) -------------- */
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 0(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 8(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 16(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 32(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 64(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 128(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 256(sp)", fa0, sp);
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa0, 504(sp)", fa0, sp);
+
+   TESTINST_1_1_FLOAD(2, "c.fldsp fa5, 0(sp)", fa5, sp);
+
+   /* -------------- c.lwsp rd, uimm[7:2](x2) --------------- */
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 0(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 4(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 8(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 16(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 32(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 64(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 128(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.lwsp a0, 252(sp)", a0, sp);
+
+   TESTINST_1_1_LOAD(2, "c.lwsp a5, 0(sp)", a5, sp);
+
+   /* -------------- c.ldsp rd, uimm[8:3](x2) --------------- */
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 0(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 8(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 16(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 32(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 64(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 128(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 256(sp)", a0, sp);
+   TESTINST_1_1_LOAD(2, "c.ldsp a0, 504(sp)", a0, sp);
+
+   TESTINST_1_1_LOAD(2, "c.ldsp a5, 0(sp)", a5, sp);
+
+   /* ---------------------- c.jr rs1 ----------------------- */
+   TESTINST_0_1_JR_RANGE(2, "c.jr t0", "1f+4", 4, t0);
+   TESTINST_0_1_JR_RANGE(2, "c.jr t0", "1f+6", 6, t0);
+   TESTINST_0_1_JR_RANGE(2, "c.jr t0", "1f+8", 8, t0);
+   TESTINST_0_1_JR_RANGE(2, "c.jr t0", "1f-4", -4, t0);
+   TESTINST_0_1_JR_RANGE(2, "c.jr t0", "1f-6", -6, t0);
+   TESTINST_0_1_JR_RANGE(2, "c.jr t0", "1f-8", -8, t0);
+
+   TESTINST_0_1_JR_RANGE(2, "c.jr t6", "1f+4", 4, t6);
+
+   /* -------------------- c.mv rd, rs2 --------------------- */
+   TESTINST_1_1(2, "c.mv t0, t6", 0xabcdef0123456789, t0, t6);
+   TESTINST_1_1(2, "c.mv t6, t0", 0xabcdef0123456789, t6, t0);
+   TESTINST_1_1(2, "c.mv s0, s11", 0xabcdef0123456789, s0, s11);
+   TESTINST_1_1(2, "c.mv s11, s0", 0xabcdef0123456789, s11, s0);
+   TESTINST_1_1(2, "c.mv a0, a7", 0xabcdef0123456789, a0, a7);
+   TESTINST_1_1(2, "c.mv a7, a0", 0xabcdef0123456789, a7, a0);
+
+   /* --------------------- c.jalr rs1 ---------------------- */
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t0", "1f+4", 4, ra, t0);
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t0", "1f+6", 6, ra, t0);
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t0", "1f+8", 8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t0", "1f-4", -4, ra, t0);
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t0", "1f-6", -6, ra, t0);
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t0", "1f-8", -8, ra, t0);
+
+   TESTINST_1_1_JALR_RANGE(2, "c.jalr t6", "1f+4", 4, ra, t6);
+
+   /* ------------------ c.add rd_rs1, rs2 ------------------ */
+   TESTINST_1_2(2, "c.add a0, a1", 0x0000000000001000, 0x0000000000002000, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.add a0, a1", 0x000000007fffffff, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.add a0, a1", 0x00000000fffffffe, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.add a0, a1", 0x00000000ffffffff, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.add a0, a1", 0xfffffffffffffffe, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.add a0, a1", 0xffffffffffffffff, 0x0000000000000001, a0,
+                a0, a1);
+   TESTINST_1_2(2, "c.add a4, a5", 0x0000000000001000, 0x0000000000002000, a4,
+                a4, a5);
+
+   /* ------------- c.fsdsp rs2, uimm[8:3](x2) -------------- */
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 0(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 8(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 16(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 32(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 64(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 128(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 256(sp)", 0xabcdef0123456789, fa0, sp);
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa0, 504(sp)", 0xabcdef0123456789, fa0, sp);
+
+   TESTINST_0_2_FSTORE(2, "c.fsdsp fa5, 0(sp)", 0xabcdef0123456789, fa5, sp);
+
+   /* -------------- c.swsp rs2, uimm[7:2](x2) -------------- */
+   TESTINST_0_2_STORE(2, "c.swsp a0, 0(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 4(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 8(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 16(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 32(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 64(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 128(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.swsp a0, 252(sp)", 0xabcdef0123456789, a0, sp);
+
+   TESTINST_0_2_STORE(2, "c.swsp a5, 0(sp)", 0xabcdef0123456789, a5, sp);
+
+   /* -------------- c.sdsp rs2, uimm[8:3](x2) -------------- */
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 0(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 8(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 16(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 32(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 64(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 128(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 256(sp)", 0xabcdef0123456789, a0, sp);
+   TESTINST_0_2_STORE(2, "c.sdsp a0, 504(sp)", 0xabcdef0123456789, a0, sp);
+
+   TESTINST_0_2_STORE(2, "c.sdsp a5, 0(sp)", 0xabcdef0123456789, a5, sp);
+}
+
+int main(void)
+{
+   test_compressed_00();
+   test_compressed_01();
+   test_compressed_10();
+   return 0;
+}
--- /dev/null
+++ b/none/tests/riscv64/compressed.stdout.exp
@@ -0,0 +1,917 @@
+RV64C compressed instruction set, quadrant 0
+c.addi4spn a0, sp, 4 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001004
+c.addi4spn a0, sp, 8 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001008
+c.addi4spn a0, sp, 16 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001010
+c.addi4spn a0, sp, 32 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001020
+c.addi4spn a0, sp, 64 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001040
+c.addi4spn a0, sp, 128 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001080
+c.addi4spn a0, sp, 256 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001100
+c.addi4spn a0, sp, 512 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000001200
+c.addi4spn a0, sp, 1020 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x00000000000013fc
+c.addi4spn a0, sp, 4 ::
+  inputs: sp=0x000000007ffffffc
+  output: a0=0x0000000080000000
+c.addi4spn a0, sp, 4 ::
+  inputs: sp=0x00000000fffffffb
+  output: a0=0x00000000ffffffff
+c.addi4spn a0, sp, 4 ::
+  inputs: sp=0x00000000fffffffc
+  output: a0=0x0000000100000000
+c.addi4spn a5, sp, 4 ::
+  inputs: sp=0x0000000000001000
+  output: a0=0x0000000000000020
+c.fld fa0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0xbeafe48541dc8da0
+  no memory changes
+c.fld fa0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0xba6d23fbddcfb6e4
+  no memory changes
+c.fld fa0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0xe23b6d7d6753321d
+  no memory changes
+c.fld fa0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0x849d6e092767dabd
+  no memory changes
+c.fld fa0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0xec83e542163f4e88
+  no memory changes
+c.fld fa0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0x21989d257082ce6f
+  no memory changes
+c.fld fa0, 248(a1) ::
+  inputs: a1=&area_mid
+  output: fa0=0x63082c1746f49884
+  no memory changes
+c.fld fa4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: fa4=0x5486cc410e1801e6
+  no memory changes
+c.lw a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000720eda7
+  no memory changes
+c.lw a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffec7a332e
+  no memory changes
+c.lw a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xfffffffff26fc107
+  no memory changes
+c.lw a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffb4a7dcfa
+  no memory changes
+c.lw a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffff81406130
+  no memory changes
+c.lw a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000da412a7
+  no memory changes
+c.lw a0, 124(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000006a330ec8
+  no memory changes
+c.lw a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0xffffffffd45c61ed
+  no memory changes
+c.ld a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x82501ceacc654dae
+  no memory changes
+c.ld a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x6dbeca915808e621
+  no memory changes
+c.ld a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x863d8543d33dd28a
+  no memory changes
+c.ld a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x07ff662e72b0598a
+  no memory changes
+c.ld a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x30a49d2822488e15
+  no memory changes
+c.ld a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xe539d48afb0b8e7d
+  no memory changes
+c.ld a0, 248(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x36f9f34ce2cde861
+  no memory changes
+c.ld a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0x172704a799a1c1f4
+  no memory changes
+c.fsd fa0, 0(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsd fa0, 8(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.fsd fa0, 16(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsd fa0, 32(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsd fa0, 64(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsd fa0, 128(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+128]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsd fa0, 248(a1) ::
+  inputs: fa0=0xabcdef0123456789, a1=&area_mid
+  [+240]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.fsd fa4, 0(a5) ::
+  inputs: fa4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sw a0, 0(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.sw a0, 4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. 89 67 45 23 .. .. .. .. .. .. .. ..
+c.sw a0, 8(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 .. .. .. ..
+c.sw a0, 16(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.sw a0, 32(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.sw a0, 64(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.sw a0, 124(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+112]  .. .. .. .. .. .. .. .. .. .. .. .. 89 67 45 23
+c.sw a4, 0(a5) ::
+  inputs: a4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.sd a0, 0(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 67 45 23 01 .. cd ab .. .. .. .. .. .. .. ..
+c.sd a0, 8(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.sd a0, 16(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sd a0, 32(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sd a0, 64(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sd a0, 128(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+128]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sd a0, 248(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+240]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.sd a4, 0(a5) ::
+  inputs: a4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+
+RV64C compressed instruction set, quadrant 1
+c.nop ::
+c.addi a0, 1 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001001
+c.addi a0, 2 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001002
+c.addi a0, 4 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001004
+c.addi a0, 8 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001008
+c.addi a0, 16 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001010
+c.addi a0, 31 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x000000000000101f
+c.addi a0, -1 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000000fff
+c.addi a0, -32 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000000fe0
+c.addi a0, 1 ::
+  inputs: a0=0x000000007fffffff
+  output: a0=0x0000000080000000
+c.addi a0, 1 ::
+  inputs: a0=0x00000000fffffffe
+  output: a0=0x00000000ffffffff
+c.addi a0, 1 ::
+  inputs: a0=0x00000000ffffffff
+  output: a0=0x0000000100000000
+c.addi t6, 1 ::
+  inputs: t6=0x0000000000001000
+  output: t6=0x0000000000001001
+c.addiw a0, 0 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001000
+c.addiw a0, 1 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001001
+c.addiw a0, 2 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001002
+c.addiw a0, 4 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001004
+c.addiw a0, 8 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001008
+c.addiw a0, 16 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000001010
+c.addiw a0, 31 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x000000000000101f
+c.addiw a0, -1 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000000fff
+c.addiw a0, -32 ::
+  inputs: a0=0x0000000000001000
+  output: a0=0x0000000000000fe0
+c.addiw a0, 1 ::
+  inputs: a0=0x000000007fffffff
+  output: a0=0xffffffff80000000
+c.addiw a0, 1 ::
+  inputs: a0=0x00000000fffffffe
+  output: a0=0xffffffffffffffff
+c.addiw a0, 1 ::
+  inputs: a0=0x00000000ffffffff
+  output: a0=0x0000000000000000
+c.addiw t6, 0 ::
+  inputs: t6=0x0000000000001000
+  output: t6=0x0000000000001000
+c.li a0, 0 ::
+  output: a0=0x0000000000000000
+c.li a0, 1 ::
+  output: a0=0x0000000000000001
+c.li a0, 2 ::
+  output: a0=0x0000000000000002
+c.li a0, 4 ::
+  output: a0=0x0000000000000004
+c.li a0, 8 ::
+  output: a0=0x0000000000000008
+c.li a0, 15 ::
+  output: a0=0x000000000000000f
+c.li a0, -1 ::
+  output: a0=0xffffffffffffffff
+c.li a0, -16 ::
+  output: a0=0xfffffffffffffff0
+c.li t6, 1 ::
+  output: t6=0x0000000000000001
+c.addi16sp sp, 16 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000001010
+c.addi16sp sp, 32 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000001020
+c.addi16sp sp, 64 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000001040
+c.addi16sp sp, 128 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000001080
+c.addi16sp sp, 256 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000001100
+c.addi16sp sp, 496 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x00000000000011f0
+c.addi16sp sp, -16 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000000ff0
+c.addi16sp sp, -512 ::
+  inputs: sp=0x0000000000001000
+  output: sp=0x0000000000000e00
+c.addi16sp sp, 16 ::
+  inputs: sp=0x000000007ffffff0
+  output: sp=0x0000000080000000
+c.addi16sp sp, 16 ::
+  inputs: sp=0x00000000ffffffef
+  output: sp=0x00000000ffffffff
+c.addi16sp sp, 16 ::
+  inputs: sp=0x00000000fffffff0
+  output: sp=0x0000000100000000
+c.lui a0, 1 ::
+  output: a0=0x0000000000001000
+c.lui a0, 2 ::
+  output: a0=0x0000000000002000
+c.lui a0, 4 ::
+  output: a0=0x0000000000004000
+c.lui a0, 8 ::
+  output: a0=0x0000000000008000
+c.lui a0, 16 ::
+  output: a0=0x0000000000010000
+c.lui a0, 31 ::
+  output: a0=0x000000000001f000
+c.lui a0, 0xfffff ::
+  output: a0=0xfffffffffffff000
+c.lui a0, 0xfffe0 ::
+  output: a0=0xfffffffffffe0000
+c.lui t6, 1 ::
+  output: t6=0x0000000000001000
+c.srli a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x55e6f78091a2b3c4
+c.srli a0, 2 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x2af37bc048d159e2
+c.srli a0, 4 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x0abcdef012345678
+c.srli a0, 8 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x00abcdef01234567
+c.srli a0, 16 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x0000abcdef012345
+c.srli a0, 32 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x00000000abcdef01
+c.srli a0, 63 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x0000000000000001
+c.srli a5, 1 ::
+  inputs: a5=0xabcdef0123456789
+  output: a5=0x55e6f78091a2b3c4
+c.srai a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xd5e6f78091a2b3c4
+c.srai a0, 2 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xeaf37bc048d159e2
+c.srai a0, 4 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xfabcdef012345678
+c.srai a0, 8 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xffabcdef01234567
+c.srai a0, 16 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xffffabcdef012345
+c.srai a0, 32 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xffffffffabcdef01
+c.srai a0, 63 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xffffffffffffffff
+c.srai a5, 1 ::
+  inputs: a5=0xabcdef0123456789
+  output: a5=0xd5e6f78091a2b3c4
+c.andi a0, 0 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x0000000000000000
+c.andi a0, 1 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x0000000000000001
+c.andi a0, 2 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x0000000000000002
+c.andi a0, 4 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x0000000000000004
+c.andi a0, 8 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x0000000000000008
+c.andi a0, 16 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x0000000000000010
+c.andi a0, 31 ::
+  inputs: a0=0xffffffffffffffff
+  output: a0=0x000000000000001f
+c.andi a5, 0 ::
+  inputs: a5=0xffffffffffffffff
+  output: a5=0x0000000000000000
+c.sub a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000000fff
+  output: a0=0x0000000000000001
+c.sub a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000001000
+  output: a0=0x0000000000000000
+c.sub a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000001001
+  output: a0=0xffffffffffffffff
+c.sub a0, a1 ::
+  inputs: a0=0xffffffffffffffff, a1=0x0000000000000000
+  output: a0=0xffffffffffffffff
+c.sub a0, a1 ::
+  inputs: a0=0x0000000100000000, a1=0x0000000000000001
+  output: a0=0x00000000ffffffff
+c.sub a4, a5 ::
+  inputs: a4=0x0000000000001000, a5=0x0000000000000fff
+  output: a4=0x0000000000000001
+c.xor a0, a1 ::
+  inputs: a0=0x0000ffff0000ffff, a1=0x00000000ffffffff
+  output: a0=0x0000ffffffff0000
+c.xor a4, a5 ::
+  inputs: a4=0x0000ffff0000ffff, a5=0x00000000ffffffff
+  output: a4=0x0000ffffffff0000
+c.or a0, a1 ::
+  inputs: a0=0x0000ffff0000ffff, a1=0x00000000ffffffff
+  output: a0=0x0000ffffffffffff
+c.or a4, a5 ::
+  inputs: a4=0x0000ffff0000ffff, a5=0x00000000ffffffff
+  output: a4=0x0000ffffffffffff
+c.and a0, a1 ::
+  inputs: a0=0x0000ffff0000ffff, a1=0x00000000ffffffff
+  output: a0=0x000000000000ffff
+c.and a4, a5 ::
+  inputs: a4=0x0000ffff0000ffff, a5=0x00000000ffffffff
+  output: a4=0x000000000000ffff
+c.subw a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000000fff
+  output: a0=0x0000000000000001
+c.subw a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000001000
+  output: a0=0x0000000000000000
+c.subw a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000001001
+  output: a0=0xffffffffffffffff
+c.subw a0, a1 ::
+  inputs: a0=0xffffffffffffffff, a1=0x0000000000000000
+  output: a0=0xffffffffffffffff
+c.subw a0, a1 ::
+  inputs: a0=0x0000000100000000, a1=0x0000000000000001
+  output: a0=0xffffffffffffffff
+c.subw a4, a5 ::
+  inputs: a4=0x0000000000001000, a5=0x0000000000000fff
+  output: a4=0x0000000000000001
+c.addw a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: a0=0x0000000000003000
+c.addw a0, a1 ::
+  inputs: a0=0x000000007fffffff, a1=0x0000000000000001
+  output: a0=0xffffffff80000000
+c.addw a0, a1 ::
+  inputs: a0=0x00000000fffffffe, a1=0x0000000000000001
+  output: a0=0xffffffffffffffff
+c.addw a0, a1 ::
+  inputs: a0=0x00000000ffffffff, a1=0x0000000000000001
+  output: a0=0x0000000000000000
+c.addw a0, a1 ::
+  inputs: a0=0xfffffffffffffffe, a1=0x0000000000000001
+  output: a0=0xffffffffffffffff
+c.addw a0, a1 ::
+  inputs: a0=0xffffffffffffffff, a1=0x0000000000000001
+  output: a0=0x0000000000000000
+c.addw a4, a5 ::
+  inputs: a4=0x0000000000001000, a5=0x0000000000002000
+  output: a4=0x0000000000003000
+c.j .+4 ::
+  target: reached
+c.j .+6 ::
+  target: reached
+c.j .+8 ::
+  target: reached
+c.j .+16 ::
+  target: reached
+c.j .+32 ::
+  target: reached
+c.j .+64 ::
+  target: reached
+c.j .+128 ::
+  target: reached
+c.j .+256 ::
+  target: reached
+c.j .+512 ::
+  target: reached
+c.j .+1024 ::
+  target: reached
+c.j .+2044 ::
+  target: reached
+c.j .-4 ::
+  target: reached
+c.j .-6 ::
+  target: reached
+c.j .-2048 ::
+  target: reached
+c.beqz a0, .+4 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+6 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+8 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+16 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+32 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+64 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+128 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .+252 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .-4 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .-6 ::
+  inputs: a0=0
+  target: reached
+c.beqz a0, .-256 ::
+  inputs: a0=0
+  target: reached
+c.beqz a5, .+4 ::
+  inputs: a5=0
+  target: reached
+c.beqz a0, 1f ::
+  inputs: a0=0
+  branch: taken
+c.beqz a0, 1f ::
+  inputs: a0=1
+  branch: not taken
+c.bnez a0, .+4 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+6 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+8 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+16 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+32 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+64 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+128 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .+252 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .-4 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .-6 ::
+  inputs: a0=1
+  target: reached
+c.bnez a0, .-256 ::
+  inputs: a0=1
+  target: reached
+c.bnez a5, .+4 ::
+  inputs: a5=1
+  target: reached
+c.bnez a0, 1f ::
+  inputs: a0=0
+  branch: not taken
+c.bnez a0, 1f ::
+  inputs: a0=1
+  branch: taken
+
+RV64C compressed instruction set, quadrant 2
+c.slli a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x579bde02468acf12
+c.slli a0, 2 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xaf37bc048d159e24
+c.slli a0, 4 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xbcdef01234567890
+c.slli a0, 8 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xcdef012345678900
+c.slli a0, 16 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0xef01234567890000
+c.slli a0, 32 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x2345678900000000
+c.slli a0, 63 ::
+  inputs: a0=0xabcdef0123456789
+  output: a0=0x8000000000000000
+c.slli a5, 1 ::
+  inputs: a5=0xabcdef0123456789
+  output: a5=0x579bde02468acf12
+c.fldsp fa0, 0(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0x09938bb5e378ccc9
+  no memory changes
+c.fldsp fa0, 8(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0xd46119bc4f7b459c
+  no memory changes
+c.fldsp fa0, 16(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0xcd40b4ceaa101165
+  no memory changes
+c.fldsp fa0, 32(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0x0ec2557909435825
+  no memory changes
+c.fldsp fa0, 64(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0xb7e70cf3395b0d30
+  no memory changes
+c.fldsp fa0, 128(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0x6c7c4355121e0d98
+  no memory changes
+c.fldsp fa0, 256(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0xee5b125eb6a74a8d
+  no memory changes
+c.fldsp fa0, 504(sp) ::
+  inputs: sp=&area_mid
+  output: fa0=0xbd4f1ec2a7cdc5d1
+  no memory changes
+c.fldsp fa5, 0(sp) ::
+  inputs: sp=&area_mid
+  output: fa5=0x6a642668a9bd2cd0
+  no memory changes
+c.lwsp a0, 0(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xffffffffa2c51890
+  no memory changes
+c.lwsp a0, 4(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x0000000003588d55
+  no memory changes
+c.lwsp a0, 8(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xffffffffbd9fc2c3
+  no memory changes
+c.lwsp a0, 16(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xffffffff8dc294e9
+  no memory changes
+c.lwsp a0, 32(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x00000000185128a6
+  no memory changes
+c.lwsp a0, 64(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xffffffffa02175a9
+  no memory changes
+c.lwsp a0, 128(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x000000002a54a500
+  no memory changes
+c.lwsp a0, 252(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x0000000042735f59
+  no memory changes
+c.lwsp a5, 0(sp) ::
+  inputs: sp=&area_mid
+  output: a5=0x00000000670a7797
+  no memory changes
+c.ldsp a0, 0(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x6429290760136358
+  no memory changes
+c.ldsp a0, 8(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xbefd16247abc5ae2
+  no memory changes
+c.ldsp a0, 16(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x05a2ce0c43b76420
+  no memory changes
+c.ldsp a0, 32(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x22efeba3bef6670c
+  no memory changes
+c.ldsp a0, 64(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x82ad9af526269470
+  no memory changes
+c.ldsp a0, 128(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xa872c2086f198487
+  no memory changes
+c.ldsp a0, 256(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0x0ab07070f302a1dc
+  no memory changes
+c.ldsp a0, 504(sp) ::
+  inputs: sp=&area_mid
+  output: a0=0xcb1f9e3eb6021eea
+  no memory changes
+c.ldsp a5, 0(sp) ::
+  inputs: sp=&area_mid
+  output: a5=0xc6fac5ba2658c35f
+  no memory changes
+c.jr t0 ::
+  inputs: t0=1f+4
+  target: reached
+c.jr t0 ::
+  inputs: t0=1f+6
+  target: reached
+c.jr t0 ::
+  inputs: t0=1f+8
+  target: reached
+c.jr t0 ::
+  inputs: t0=1f-4
+  target: reached
+c.jr t0 ::
+  inputs: t0=1f-6
+  target: reached
+c.jr t0 ::
+  inputs: t0=1f-8
+  target: reached
+c.jr t6 ::
+  inputs: t6=1f+4
+  target: reached
+c.mv t0, t6 ::
+  inputs: t6=0xabcdef0123456789
+  output: t0=0xabcdef0123456789
+c.mv t6, t0 ::
+  inputs: t0=0xabcdef0123456789
+  output: t6=0xabcdef0123456789
+c.mv s0, s11 ::
+  inputs: s11=0xabcdef0123456789
+  output: s0=0xabcdef0123456789
+c.mv s11, s0 ::
+  inputs: s0=0xabcdef0123456789
+  output: s11=0xabcdef0123456789
+c.mv a0, a7 ::
+  inputs: a7=0xabcdef0123456789
+  output: a0=0xabcdef0123456789
+c.mv a7, a0 ::
+  inputs: a0=0xabcdef0123456789
+  output: a7=0xabcdef0123456789
+c.jalr t0 ::
+  inputs: t0=1f+4
+  output: ra=1f+2
+  target: reached
+c.jalr t0 ::
+  inputs: t0=1f+6
+  output: ra=1f+2
+  target: reached
+c.jalr t0 ::
+  inputs: t0=1f+8
+  output: ra=1f+2
+  target: reached
+c.jalr t0 ::
+  inputs: t0=1f-4
+  output: ra=1f+2
+  target: reached
+c.jalr t0 ::
+  inputs: t0=1f-6
+  output: ra=1f+2
+  target: reached
+c.jalr t0 ::
+  inputs: t0=1f-8
+  output: ra=1f+2
+  target: reached
+c.jalr t6 ::
+  inputs: t6=1f+4
+  output: ra=1f+2
+  target: reached
+c.add a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: a0=0x0000000000003000
+c.add a0, a1 ::
+  inputs: a0=0x000000007fffffff, a1=0x0000000000000001
+  output: a0=0x0000000080000000
+c.add a0, a1 ::
+  inputs: a0=0x00000000fffffffe, a1=0x0000000000000001
+  output: a0=0x00000000ffffffff
+c.add a0, a1 ::
+  inputs: a0=0x00000000ffffffff, a1=0x0000000000000001
+  output: a0=0x0000000100000000
+c.add a0, a1 ::
+  inputs: a0=0xfffffffffffffffe, a1=0x0000000000000001
+  output: a0=0xffffffffffffffff
+c.add a0, a1 ::
+  inputs: a0=0xffffffffffffffff, a1=0x0000000000000001
+  output: a0=0x0000000000000000
+c.add a4, a5 ::
+  inputs: a4=0x0000000000001000, a5=0x0000000000002000
+  output: a4=0x0000000000003000
+c.fsdsp fa0, 0(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsdsp fa0, 8(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.fsdsp fa0, 16(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+016]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsdsp fa0, 32(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+032]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsdsp fa0, 64(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+064]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsdsp fa0, 128(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+128]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsdsp fa0, 256(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+256]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.fsdsp fa0, 504(sp) ::
+  inputs: fa0=0xabcdef0123456789, sp=&area_mid
+  [+496]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.fsdsp fa5, 0(sp) ::
+  inputs: fa5=0xabcdef0123456789, sp=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.swsp a0, 0(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.swsp a0, 4(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+000]  .. .. .. .. 89 67 45 23 .. .. .. .. .. .. .. ..
+c.swsp a0, 8(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 .. .. .. ..
+c.swsp a0, 16(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+016]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.swsp a0, 32(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+032]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.swsp a0, 64(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+064]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.swsp a0, 128(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+128]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.swsp a0, 252(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+240]  .. .. .. .. .. .. .. .. .. .. .. .. 89 67 45 23
+c.swsp a5, 0(sp) ::
+  inputs: a5=0xabcdef0123456789, sp=&area_mid
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+c.sdsp a0, 0(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sdsp a0, 8(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.sdsp a0, 16(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+016]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sdsp a0, 32(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+032]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sdsp a0, 64(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+064]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sdsp a0, 128(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+128]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sdsp a0, 256(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+256]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+c.sdsp a0, 504(sp) ::
+  inputs: a0=0xabcdef0123456789, sp=&area_mid
+  [+496]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+c.sdsp a5, 0(sp) ::
+  inputs: a5=0xabcdef0123456789, sp=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
--- /dev/null
+++ b/none/tests/riscv64/compressed.vgtest
@@ -0,0 +1,2 @@
+prog: compressed
+vgopts: -q
--- /dev/null
+++ b/none/tests/riscv64/filter_stderr
@@ -0,0 +1,3 @@
+#! /bin/sh
+
+../filter_stderr
--- /dev/null
+++ b/none/tests/riscv64/integer.c
@@ -0,0 +1,823 @@
+/* Tests for the RV64I base integer instruction set. */
+
+#include "testinst.h"
+
+static void test_integer_shared(void)
+{
+   printf("RV64I base instruction set, shared operations\n");
+
+   /* ----------------- lui rd, imm[31:12] ------------------ */
+   TESTINST_1_0(4, "lui a0, 0", a0);
+   TESTINST_1_0(4, "lui a0, 1", a0);
+   TESTINST_1_0(4, "lui a0, 2", a0);
+   TESTINST_1_0(4, "lui a0, 4", a0);
+   TESTINST_1_0(4, "lui a0, 8", a0);
+   TESTINST_1_0(4, "lui a0, 16", a0);
+   TESTINST_1_0(4, "lui a0, 32", a0);
+   TESTINST_1_0(4, "lui a0, 64", a0);
+   TESTINST_1_0(4, "lui a0, 128", a0);
+   TESTINST_1_0(4, "lui a0, 256", a0);
+   TESTINST_1_0(4, "lui a0, 512", a0);
+   TESTINST_1_0(4, "lui a0, 1024", a0);
+   TESTINST_1_0(4, "lui a0, 2048", a0);
+   TESTINST_1_0(4, "lui a0, 4096", a0);
+   TESTINST_1_0(4, "lui a0, 8192", a0);
+   TESTINST_1_0(4, "lui a0, 16384", a0);
+   TESTINST_1_0(4, "lui a0, 32768", a0);
+   TESTINST_1_0(4, "lui a0, 65536", a0);
+   TESTINST_1_0(4, "lui a0, 131072", a0);
+   TESTINST_1_0(4, "lui a0, 262144", a0);
+   TESTINST_1_0(4, "lui a0, 524288", a0);
+   TESTINST_1_0(4, "lui a0, 1048575", a0);
+
+   TESTINST_1_0(4, "lui t6, 1", t6);
+   TESTINST_1_0(4, "lui zero, 1", zero);
+
+   /* ---------------- auipc rd, imm[31:12] ----------------- */
+   TESTINST_1_0_AUIPC(4, "auipc a0, 0", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 1", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 2", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 4", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 8", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 16", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 32", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 64", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 128", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 256", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 512", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 1024", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 2048", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 4096", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 8192", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 16384", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 32768", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 65536", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 131072", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 262144", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 524288", a0);
+   TESTINST_1_0_AUIPC(4, "auipc a0, 1048575", a0);
+
+   TESTINST_1_0_AUIPC(4, "auipc t6, 1", t6);
+   TESTINST_1_0_AUIPC(4, "auipc zero, 1", zero);
+
+   /* ------------------ jal rd, imm[20:1] ------------------ */
+   /* Note: Only the imm[11:1] range is tested. */
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+4", 4, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+6", 6, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+8", 8, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+16", 16, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+32", 32, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+64", 64, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+128", 128, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+256", 256, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+512", 512, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+1024", 1024, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .+2048", 2048, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .-4", -4, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .-6", -6, t0);
+   TESTINST_1_0_JAL_RANGE(4, "jal t0, .-2048", -2048, t0);
+
+   TESTINST_1_0_JAL_RANGE(4, "jal t6, .+4", 4, t6);
+   TESTINST_1_0_JAL_RANGE(4, "jal zero, .+4", 4, zero);
+
+   /* --------------- jalr rd, imm[11:0](rs1) --------------- */
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f+4", 4, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f+6", 6, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f+8", 8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f-4", -4, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f-6", -6, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f-8", -8, ra, t0);
+
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t0)", "1f-8", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 1(t0)", "1f-9", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 2(t0)", "1f-10", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 4(t0)", "1f-12", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 8(t0)", "1f-16", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 16(t0)", "1f-24", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 32(t0)", "1f-40", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 64(t0)", "1f-72", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 128(t0)", "1f-136", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 256(t0)", "1f-264", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 512(t0)", "1f-520", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 1024(t0)", "1f-1032", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 2047(t0)", "1f-2055", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, -1(t0)", "1f-7", -8, ra, t0);
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, -2048(t0)", "1f+2040", -8, ra, t0);
+
+   TESTINST_1_1_JALR_RANGE(4, "jalr ra, 0(t6)", "1f+4", 4, ra, t6);
+   TESTINST_1_1_JALR_RANGE(4, "jalr zero, 0(a0)", "1f+4", 4, zero, a0);
+
+   /* --------------- beq rs1, rs2, imm[12:1] --------------- */
+   /* Note: Only the imm[11:1] range is tested. */
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+4", 0, 0, 4, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+6", 0, 0, 6, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+8", 0, 0, 8, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+16", 0, 0, 16, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+32", 0, 0, 32, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+64", 0, 0, 64, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+128", 0, 0, 128, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+256", 0, 0, 256, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+512", 0, 0, 512, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+1024", 0, 0, 1024, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .+2048", 0, 0, 2048, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .-4", 0, 0, -4, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .-6", 0, 0, -6, a0, a1);
+   TESTINST_0_2_Bxx_RANGE(4, "beq a0, a1, .-2048", 0, 0, -2048, a0, a1);
+
+   TESTINST_0_2_Bxx_RANGE(4, "beq t5, t6, .+4", 0, 0, 4, t5, t6);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", 0, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", 0, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", 1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", 1, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, zero, 1f", 0, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, zero, 1f", 1, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "beq zero, a0, 1f", 0, 0, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "beq zero, a0, 1f", 0, 1, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", 0, -1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", -1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "beq a0, a1, 1f", -1, -1, a0, a1);
+
+   /* --------------- bne rs1, rs2, imm[12:1] --------------- */
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", 0, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", 0, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", 1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", 1, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, zero, 1f", 0, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, zero, 1f", 1, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bne zero, a0, 1f", 0, 0, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bne zero, a0, 1f", 0, 1, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", 0, -1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", -1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bne a0, a1, 1f", -1, -1, a0, a1);
+
+   /* --------------- blt rs1, rs2, imm[12:1] --------------- */
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", 0, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", 0, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", 1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", 1, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, zero, 1f", 0, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, zero, 1f", 1, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "blt zero, a0, 1f", 0, 0, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "blt zero, a0, 1f", 0, 1, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", 0, -1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", -1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "blt a0, a1, 1f", -1, -1, a0, a1);
+
+   /* --------------- bge rs1, rs2, imm[12:1] --------------- */
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", 0, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", 0, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", 1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", 1, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, zero, 1f", 0, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, zero, 1f", 1, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bge zero, a0, 1f", 0, 0, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bge zero, a0, 1f", 0, 1, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", 0, -1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", -1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bge a0, a1, 1f", -1, -1, a0, a1);
+
+   /* -------------- bltu rs1, rs2, imm[12:1] --------------- */
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", 0, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", 0, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", 1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", 1, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, zero, 1f", 0, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, zero, 1f", 1, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bltu zero, a0, 1f", 0, 0, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bltu zero, a0, 1f", 0, 1, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", 0, -1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", -1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bltu a0, a1, 1f", -1, -1, a0, a1);
+
+   /* -------------- bgeu rs1, rs2, imm[12:1] --------------- */
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", 0, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", 0, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", 1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", 1, 1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, zero, 1f", 0, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, zero, 1f", 1, 0, a0, zero);
+   TESTINST_0_2_Bxx_COND(4, "bgeu zero, a0, 1f", 0, 0, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bgeu zero, a0, 1f", 0, 1, zero, a0);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", 0, -1, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", -1, 0, a0, a1);
+   TESTINST_0_2_Bxx_COND(4, "bgeu a0, a1, 1f", -1, -1, a0, a1);
+
+   /* ---------------- lb rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "lb a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 1(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 2(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, 2047(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, -1(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lb a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "lb a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "lb zero, 0(a0)", zero, a0);
+
+   /* ---------------- lh rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "lh a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 2(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, 2046(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, -2(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lh a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "lh a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "lh zero, 0(a0)", zero, a0);
+
+   /* ---------------- lw rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "lw a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, 2044(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, -4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lw a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "lw a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "lw zero, 0(a0)", zero, a0);
+
+   /* --------------- lbu rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "lbu a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 1(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 2(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, 2047(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, -1(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lbu a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "lbu a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "lbu zero, 0(a0)", zero, a0);
+
+   /* --------------- lhu rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "lhu a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 2(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, 2046(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, -2(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lhu a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "lhu a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "lhu zero, 0(a0)", zero, a0);
+
+   /* --------------- sb rs2, imm[11:0](rs1) ---------------- */
+   TESTINST_0_2_STORE(4, "sb a0, 0(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 1(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 2(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 8(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 16(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 32(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 64(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 128(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 256(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 512(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 1024(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, 2047(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, -1(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sb a0, -2048(a1)", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_0_2_STORE(4, "sb a4, 0(a5)", 0xabcdef0123456789, a4, a5);
+
+   /* --------------- sh rs2, imm[11:0](rs1) ---------------- */
+   TESTINST_0_2_STORE(4, "sh a0, 0(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 2(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 8(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 16(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 32(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 64(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 128(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 256(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 512(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 1024(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, 2046(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, -2(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sh a0, -2048(a1)", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_0_2_STORE(4, "sh a4, 0(a5)", 0xabcdef0123456789, a4, a5);
+
+   /* --------------- sw rs2, imm[11:0](rs1) ---------------- */
+   TESTINST_0_2_STORE(4, "sw a0, 0(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 8(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 16(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 32(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 64(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 128(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 256(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 512(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 1024(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, 2044(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, -4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sw a0, -2048(a1)", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_0_2_STORE(4, "sw a4, 0(a5)", 0xabcdef0123456789, a4, a5);
+
+   /* --------------- addi rd, rs1, imm[11:0] --------------- */
+   TESTINST_1_1(4, "addi a0, a1, 1", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 2", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 4", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 8", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 16", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 32", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 64", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 128", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 256", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 1024", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 2047", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, -1", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, -2048", 0x0000000000001000, a0, a1);
+
+   TESTINST_1_1(4, "addi a0, a1, 1", 0x000000007fffffff, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 1", 0x00000000fffffffe, a0, a1);
+   TESTINST_1_1(4, "addi a0, a1, 1", 0x00000000ffffffff, a0, a1);
+   TESTINST_1_1(4, "addi t5, t6, 1", 0x0000000000001000, t5, t6);
+   TESTINST_1_1(4, "addi zero, a0, 1", 0x0000000000001000, zero, a0);
+
+   /* --------------- slti rd, rs1, imm[11:0] --------------- */
+   TESTINST_1_1(4, "slti a0, a1, 0", 0x0000000000000000, a0, a1);
+   TESTINST_1_1(4, "slti a0, a1, 0", 0x0000000000000001, a0, a1);
+   TESTINST_1_1(4, "slti a0, a1, 0", 0xffffffffffffffff, a0, a1);
+   TESTINST_1_1(4, "slti a0, a1, 0x7ff", 0x00000000000007ff, a0, a1);
+   TESTINST_1_1(4, "slti a0, a1, 0x7ff", 0x0000000000000800, a0, a1);
+   TESTINST_1_1(4, "slti a0, a1, 0xffffffffffffffff", 0xffffffffffffffff, a0,
+                a1);
+   TESTINST_1_1(4, "slti a0, a1, 0xffffffffffffffff", 0x0000000000000000, a0,
+                a1);
+
+   TESTINST_1_1(4, "slti t5, t6, 0", 0x0000000000000000, t5, t6);
+   TESTINST_1_1(4, "slti t5, t6, 0", 0x0000000000000001, t5, t6);
+   TESTINST_1_1(4, "slti zero, a0, 1", 0x0000000000000000, zero, a0);
+
+   /* -------------- sltiu rd, rs1, imm[11:0] --------------- */
+   TESTINST_1_1(4, "sltiu a0, a1, 0", 0x0000000000000000, a0, a1);
+   TESTINST_1_1(4, "sltiu a0, a1, 0", 0x0000000000000001, a0, a1);
+   TESTINST_1_1(4, "sltiu a0, a1, 0", 0xffffffffffffffff, a0, a1);
+   TESTINST_1_1(4, "sltiu a0, a1, 0x7ff", 0x00000000000007ff, a0, a1);
+   TESTINST_1_1(4, "sltiu a0, a1, 0x7ff", 0x0000000000000800, a0, a1);
+   TESTINST_1_1(4, "sltiu a0, a1, 0xffffffffffffffff", 0xffffffffffffffff, a0,
+                a1);
+   TESTINST_1_1(4, "sltiu a0, a1, 0xffffffffffffffff", 0x0000000000000000, a0,
+                a1);
+
+   TESTINST_1_1(4, "sltiu t5, t6, 0", 0x0000000000000000, t5, t6);
+   TESTINST_1_1(4, "sltiu t5, t6, 0", 0x0000000000000001, t5, t6);
+   TESTINST_1_1(4, "sltiu zero, a0, 1", 0x0000000000000000, zero, a0);
+
+   /* --------------- xori rd, rs1, imm[11:0] --------------- */
+   TESTINST_1_1(4, "xori a0, a1, 0", 0x0000ffff0000ffff, a0, a1);
+   TESTINST_1_1(4, "xori a0, a1, 0", 0xffff0000ffff0000, a0, a1);
+   TESTINST_1_1(4, "xori a0, a1, 0x7ff", 0x0000ffff0000ffff, a0, a1);
+   TESTINST_1_1(4, "xori a0, a1, 0x7ff", 0xffff0000ffff0000, a0, a1);
+   TESTINST_1_1(4, "xori a0, a1, 0xffffffffffffffff", 0x0000ffff0000ffff, a0,
+                a1);
+   TESTINST_1_1(4, "xori a0, a1, 0xffffffffffffffff", 0xffff0000ffff0000, a0,
+                a1);
+
+   TESTINST_1_1(4, "xori t5, t6, 0", 0x0000ffff0000ffff, t5, t6);
+   TESTINST_1_1(4, "xori zero, a0, 0x7ff", 0x0000ffff0000ffff, zero, a0);
+
+   /* --------------- ori rd, rs1, imm[11:0] ---------------- */
+   TESTINST_1_1(4, "ori a0, a1, 0", 0x0000ffff0000ffff, a0, a1);
+   TESTINST_1_1(4, "ori a0, a1, 0", 0xffff0000ffff0000, a0, a1);
+   TESTINST_1_1(4, "ori a0, a1, 0x7ff", 0x0000ffff0000ffff, a0, a1);
+   TESTINST_1_1(4, "ori a0, a1, 0x7ff", 0xffff0000ffff0000, a0, a1);
+   TESTINST_1_1(4, "ori a0, a1, 0xffffffffffffffff", 0x0000ffff0000ffff, a0,
+                a1);
+   TESTINST_1_1(4, "ori a0, a1, 0xffffffffffffffff", 0xffff0000ffff0000, a0,
+                a1);
+
+   TESTINST_1_1(4, "ori t5, t6, 0", 0x0000ffff0000ffff, t5, t6);
+   TESTINST_1_1(4, "ori zero, a0, 0x7ff", 0x0000ffff0000ffff, zero, a0);
+
+   /* --------------- andi rd, rs1, imm[11:0] --------------- */
+   TESTINST_1_1(4, "andi a0, a1, 0", 0x0000ffff0000ffff, a0, a1);
+   TESTINST_1_1(4, "andi a0, a1, 0", 0xffff0000ffff0000, a0, a1);
+   TESTINST_1_1(4, "andi a0, a1, 0x7ff", 0x0000ffff0000ffff, a0, a1);
+   TESTINST_1_1(4, "andi a0, a1, 0x7ff", 0xffff0000ffff0000, a0, a1);
+   TESTINST_1_1(4, "andi a0, a1, 0xffffffffffffffff", 0x0000ffff0000ffff, a0,
+                a1);
+   TESTINST_1_1(4, "andi a0, a1, 0xffffffffffffffff", 0xffff0000ffff0000, a0,
+                a1);
+
+   TESTINST_1_1(4, "andi t5, t6, 0", 0x0000ffff0000ffff, t5, t6);
+   TESTINST_1_1(4, "andi zero, a0, 0x7ff", 0x0000ffff0000ffff, zero, a0);
+
+   /* --------------- slli rd, rs1, uimm[5:0] --------------- */
+   TESTINST_1_1(4, "slli a0, a1, 0", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 1", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 2", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 4", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 8", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 16", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 32", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slli a0, a1, 63", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_1_1(4, "slli t5, t6, 1", 0xabcdef0123456789, t5, t6);
+   TESTINST_1_1(4, "slli zero, a0, 1", 0xabcdef0123456789, zero, a0);
+
+   /* --------------- srli rd, rs1, uimm[5:0] --------------- */
+   TESTINST_1_1(4, "srli a0, a1, 0", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 1", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 2", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 4", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 8", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 16", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 32", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srli a0, a1, 63", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_1_1(4, "srli t5, t6, 1", 0xabcdef0123456789, t5, t6);
+   TESTINST_1_1(4, "srli zero, a0, 1", 0xabcdef0123456789, zero, a0);
+
+   /* --------------- srai rd, rs1, uimm[5:0] --------------- */
+   TESTINST_1_1(4, "srai a0, a1, 0", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 1", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 2", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 4", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 8", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 16", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 32", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srai a0, a1, 63", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_1_1(4, "srai t5, t6, 1", 0xabcdef0123456789, t5, t6);
+   TESTINST_1_1(4, "srai zero, a0, 1", 0xabcdef0123456789, zero, a0);
+
+   /* ------------------ add rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "add a0, a1, a2", 0x0000000000001000, 0x0000000000002000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "add a0, a1, a2", 0x000000007fffffff, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "add a0, a1, a2", 0x00000000fffffffe, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "add a0, a1, a2", 0x00000000ffffffff, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "add a0, a1, a2", 0xfffffffffffffffe, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "add a0, a1, a2", 0xffffffffffffffff, 0x0000000000000001, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "add t4, t5, t6", 0x0000000000001000, 0x0000000000002000, t4,
+                t5, t6);
+   TESTINST_1_2(4, "add zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ sub rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "sub a0, a1, a2", 0x0000000000001000, 0x0000000000000fff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "sub a0, a1, a2", 0x0000000000001000, 0x0000000000001000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "sub a0, a1, a2", 0x0000000000001000, 0x0000000000001001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "sub a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "sub a0, a1, a2", 0x0000000100000000, 0x0000000000000001, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "sub t4, t5, t6", 0x0000000000001000, 0x0000000000000fff, t4,
+                t5, t6);
+   TESTINST_1_2(4, "sub zero, a0, a1", 0x0000000000001000, 0x0000000000000fff,
+                zero, a0, a1);
+
+   /* ------------------ sll rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 0, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 1, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 2, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 4, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 8, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 16, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 32, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 63, a0, a1, a2);
+   TESTINST_1_2(4, "sll a0, a1, a2", 0xabcdef0123456789, 64, a0, a1, a2);
+
+   TESTINST_1_2(4, "sll t4, t5, t6", 0xabcdef0123456789, 1, t4, t5, t6);
+   TESTINST_1_2(4, "sll zero, a0, a1", 0xabcdef0123456789, 1, zero, a0, a1);
+
+   /* ------------------ slt rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "slt a0, a1, a2", 0x0000000000000000, 0x0000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "slt a0, a1, a2", 0x0000000000000000, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "slt a0, a1, a2", 0x0000000000000000, 0xffffffffffffffff, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "slt t4, t5, t6", 0x0000000000000000, 0x0000000000000000, t4,
+                t5, t6);
+   TESTINST_1_2(4, "slt t4, t5, t6", 0x0000000000000000, 0x0000000000000001, t4,
+                t5, t6);
+   TESTINST_1_2(4, "slt zero, a0, a1", 0x0000000000000000, 0x0000000000000001,
+                zero, a0, a1);
+
+   /* ------------------ sltu rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "sltu a0, a1, a2", 0x0000000000000000, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "sltu a0, a1, a2", 0x0000000000000000, 0x0000000000000001,
+                a0, a1, a2);
+   TESTINST_1_2(4, "sltu a0, a1, a2", 0x0000000000000000, 0xffffffffffffffff,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "sltu t4, t5, t6", 0x0000000000000000, 0x0000000000000000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "sltu t4, t5, t6", 0x0000000000000000, 0x0000000000000001,
+                t4, t5, t6);
+   TESTINST_1_2(4, "sltu zero, a0, a1", 0x0000000000000000, 0x0000000000000001,
+                zero, a0, a1);
+
+   /* ------------------ xor rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "xor a0, a1, a2", 0x0000ffff0000ffff, 0x00000000ffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "xor t4, t5, t6", 0x0000ffff0000ffff, 0x00000000ffffffff, t4,
+                t5, t6);
+   TESTINST_1_2(4, "xor zero, a0, a1", 0x0000ffff0000ffff, 0x00000000ffffffff,
+                zero, a0, a1);
+
+   /* ------------------ srl rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 0, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 1, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 2, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 4, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 8, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 16, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 32, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 63, a0, a1, a2);
+   TESTINST_1_2(4, "srl a0, a1, a2", 0xabcdef0123456789, 64, a0, a1, a2);
+
+   TESTINST_1_2(4, "srl t4, t5, t6", 0xabcdef0123456789, 1, t4, t5, t6);
+   TESTINST_1_2(4, "srl zero, a0, a1", 0xabcdef0123456789, 1, zero, a0, a1);
+
+   /* ------------------ sra rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 0, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 1, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 2, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 4, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 8, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 16, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 32, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 63, a0, a1, a2);
+   TESTINST_1_2(4, "sra a0, a1, a2", 0xabcdef0123456789, 64, a0, a1, a2);
+
+   TESTINST_1_2(4, "sra t4, t5, t6", 0xabcdef0123456789, 1, t4, t5, t6);
+   TESTINST_1_2(4, "sra zero, a0, a1", 0xabcdef0123456789, 1, zero, a0, a1);
+
+   /* ------------------- or rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "or a0, a1, a2", 0x0000ffff0000ffff, 0x00000000ffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "or t4, t5, t6", 0x0000ffff0000ffff, 0x00000000ffffffff, t4,
+                t5, t6);
+   TESTINST_1_2(4, "or zero, a0, a1", 0x0000ffff0000ffff, 0x00000000ffffffff,
+                zero, a0, a1);
+
+   /* ------------------ and rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "and a0, a1, a2", 0x0000ffff0000ffff, 0x00000000ffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "and t4, t5, t6", 0x0000ffff0000ffff, 0x00000000ffffffff, t4,
+                t5, t6);
+   TESTINST_1_2(4, "and zero, a0, a1", 0x0000ffff0000ffff, 0x00000000ffffffff,
+                zero, a0, a1);
+
+   /* ------------------------ fence ------------------------ */
+   TESTINST_0_0(4, "fence");
+
+   /* ------------------------ ecall ------------------------ */
+   /* Not tested here. */
+
+   /* ----------------------- ebreak ------------------------ */
+   /* Not tested here. */
+
+   printf("\n");
+}
+
+static void test_integer_additions(void)
+{
+   printf("RV64I base instruction set, additions\n");
+
+   /* --------------- lwu rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "lwu a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, 2044(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, -4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "lwu a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "lwu a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "lwu zero, 0(a0)", zero, a0);
+
+   /* ---------------- ld rd, imm[11:0](rs1) ---------------- */
+   TESTINST_1_1_LOAD(4, "ld a0, 0(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 8(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 16(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 32(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 64(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 128(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 256(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 512(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 1024(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, 2040(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, -4(a1)", a0, a1);
+   TESTINST_1_1_LOAD(4, "ld a0, -2048(a1)", a0, a1);
+
+   TESTINST_1_1_LOAD(4, "ld a4, 0(a5)", a4, a5);
+   TESTINST_1_1_LOAD(4, "ld zero, 0(a0)", zero, a0);
+
+   /* --------------- sd rs2, imm[11:0](rs1) ---------------- */
+   TESTINST_0_2_STORE(4, "sd a0, 0(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 8(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 16(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 32(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 64(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 128(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 256(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 512(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 1024(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, 2040(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, -4(a1)", 0xabcdef0123456789, a0, a1);
+   TESTINST_0_2_STORE(4, "sd a0, -2048(a1)", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_0_2_STORE(4, "sd a4, 0(a5)", 0xabcdef0123456789, a4, a5);
+
+   /* -------------- addiw rd, rs1, imm[11:0] --------------- */
+   TESTINST_1_1(4, "addiw a0, a1, 1", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 2", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 4", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 8", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 16", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 32", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 64", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 128", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 256", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 1024", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 2047", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, -1", 0x0000000000001000, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, -2048", 0x0000000000001000, a0, a1);
+
+   TESTINST_1_1(4, "addiw a0, a1, 1", 0x000000007fffffff, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 1", 0x00000000fffffffe, a0, a1);
+   TESTINST_1_1(4, "addiw a0, a1, 1", 0x00000000ffffffff, a0, a1);
+   TESTINST_1_1(4, "addiw t5, t6, 1", 0x0000000000001000, t5, t6);
+   TESTINST_1_1(4, "addiw zero, a0, 1", 0x0000000000001000, zero, a0);
+
+   /* -------------- slliw rd, rs1, uimm[4:0] --------------- */
+   TESTINST_1_1(4, "slliw a0, a1, 0", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slliw a0, a1, 1", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slliw a0, a1, 2", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slliw a0, a1, 4", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slliw a0, a1, 8", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slliw a0, a1, 16", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "slliw a0, a1, 31", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_1_1(4, "slliw t5, t6, 1", 0xabcdef0123456789, t5, t6);
+   TESTINST_1_1(4, "slliw zero, a0, 1", 0xabcdef0123456789, zero, a0);
+
+   /* -------------- srliw rd, rs1, uimm[4:0] --------------- */
+   TESTINST_1_1(4, "srliw a0, a1, 0", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srliw a0, a1, 1", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srliw a0, a1, 2", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srliw a0, a1, 4", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srliw a0, a1, 8", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srliw a0, a1, 16", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "srliw a0, a1, 31", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_1_1(4, "srliw t5, t6, 1", 0xabcdef0123456789, t5, t6);
+   TESTINST_1_1(4, "srliw zero, a0, 1", 0xabcdef0123456789, zero, a0);
+
+   /* -------------- sraiw rd, rs1, uimm[4:0] --------------- */
+   TESTINST_1_1(4, "sraiw a0, a1, 0", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "sraiw a0, a1, 1", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "sraiw a0, a1, 2", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "sraiw a0, a1, 4", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "sraiw a0, a1, 8", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "sraiw a0, a1, 16", 0xabcdef0123456789, a0, a1);
+   TESTINST_1_1(4, "sraiw a0, a1, 31", 0xabcdef0123456789, a0, a1);
+
+   TESTINST_1_1(4, "srai t5, t6, 1", 0xabcdef0123456789, t5, t6);
+   TESTINST_1_1(4, "srai zero, a0, 1", 0xabcdef0123456789, zero, a0);
+
+   /* ------------------ addw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "addw a0, a1, a2", 0x0000000000001000, 0x0000000000002000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "addw a0, a1, a2", 0x000000007fffffff, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "addw a0, a1, a2", 0x00000000fffffffe, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "addw a0, a1, a2", 0x00000000ffffffff, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "addw a0, a1, a2", 0xfffffffffffffffe, 0x0000000000000001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "addw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000001, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "addw t4, t5, t6", 0x0000000000001000, 0x0000000000002000, t4,
+                t5, t6);
+   TESTINST_1_2(4, "addw zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ subw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "subw a0, a1, a2", 0x0000000000001000, 0x0000000000000fff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "subw a0, a1, a2", 0x0000000000001000, 0x0000000000001000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "subw a0, a1, a2", 0x0000000000001000, 0x0000000000001001, a0,
+                a1, a2);
+   TESTINST_1_2(4, "subw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "subw a0, a1, a2", 0x0000000100000000, 0x0000000000000001, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "subw t4, t5, t6", 0x0000000000001000, 0x0000000000000fff, t4,
+                t5, t6);
+   TESTINST_1_2(4, "subw zero, a0, a1", 0x0000000000001000, 0x0000000000000fff,
+                zero, a0, a1);
+
+   /* ------------------ sllw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 0, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 1, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 2, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 4, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 8, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 16, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 31, a0, a1, a2);
+   TESTINST_1_2(4, "sllw a0, a1, a2", 0xabcdef0123456789, 32, a0, a1, a2);
+
+   TESTINST_1_2(4, "sllw t4, t5, t6", 0xabcdef0123456789, 1, t4, t5, t6);
+   TESTINST_1_2(4, "sllw zero, a0, a1", 0xabcdef0123456789, 1, zero, a0, a1);
+
+   /* ------------------ srlw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 0, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 1, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 2, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 4, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 8, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 16, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 31, a0, a1, a2);
+   TESTINST_1_2(4, "srlw a0, a1, a2", 0xabcdef0123456789, 32, a0, a1, a2);
+
+   TESTINST_1_2(4, "srlw t4, t5, t6", 0xabcdef0123456789, 1, t4, t5, t6);
+   TESTINST_1_2(4, "srlw zero, a0, a1", 0xabcdef0123456789, 1, zero, a0, a1);
+
+   /* ------------------ sraw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 0, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 1, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 2, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 4, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 8, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 16, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 31, a0, a1, a2);
+   TESTINST_1_2(4, "sraw a0, a1, a2", 0xabcdef0123456789, 32, a0, a1, a2);
+
+   TESTINST_1_2(4, "sraw t4, t5, t6", 0xabcdef0123456789, 1, t4, t5, t6);
+   TESTINST_1_2(4, "sraw zero, a0, a1", 0xabcdef0123456789, 1, zero, a0, a1);
+}
+
+int main(void)
+{
+   test_integer_shared();
+   test_integer_additions();
+   return 0;
+}
--- /dev/null
+++ b/none/tests/riscv64/integer.stdout.exp
@@ -0,0 +1,1858 @@
+RV64I base instruction set, shared operations
+lui a0, 0 ::
+  output: a0=0x0000000000000000
+lui a0, 1 ::
+  output: a0=0x0000000000001000
+lui a0, 2 ::
+  output: a0=0x0000000000002000
+lui a0, 4 ::
+  output: a0=0x0000000000004000
+lui a0, 8 ::
+  output: a0=0x0000000000008000
+lui a0, 16 ::
+  output: a0=0x0000000000010000
+lui a0, 32 ::
+  output: a0=0x0000000000020000
+lui a0, 64 ::
+  output: a0=0x0000000000040000
+lui a0, 128 ::
+  output: a0=0x0000000000080000
+lui a0, 256 ::
+  output: a0=0x0000000000100000
+lui a0, 512 ::
+  output: a0=0x0000000000200000
+lui a0, 1024 ::
+  output: a0=0x0000000000400000
+lui a0, 2048 ::
+  output: a0=0x0000000000800000
+lui a0, 4096 ::
+  output: a0=0x0000000001000000
+lui a0, 8192 ::
+  output: a0=0x0000000002000000
+lui a0, 16384 ::
+  output: a0=0x0000000004000000
+lui a0, 32768 ::
+  output: a0=0x0000000008000000
+lui a0, 65536 ::
+  output: a0=0x0000000010000000
+lui a0, 131072 ::
+  output: a0=0x0000000020000000
+lui a0, 262144 ::
+  output: a0=0x0000000040000000
+lui a0, 524288 ::
+  output: a0=0xffffffff80000000
+lui a0, 1048575 ::
+  output: a0=0xfffffffffffff000
+lui t6, 1 ::
+  output: t6=0x0000000000001000
+lui zero, 1 ::
+  output: zero=0x0000000000000000
+auipc a0, 0 ::
+  output: a0=1f+0
+auipc a0, 1 ::
+  output: a0=1f+4096
+auipc a0, 2 ::
+  output: a0=1f+8192
+auipc a0, 4 ::
+  output: a0=1f+16384
+auipc a0, 8 ::
+  output: a0=1f+32768
+auipc a0, 16 ::
+  output: a0=1f+65536
+auipc a0, 32 ::
+  output: a0=1f+131072
+auipc a0, 64 ::
+  output: a0=1f+262144
+auipc a0, 128 ::
+  output: a0=1f+524288
+auipc a0, 256 ::
+  output: a0=1f+1048576
+auipc a0, 512 ::
+  output: a0=1f+2097152
+auipc a0, 1024 ::
+  output: a0=1f+4194304
+auipc a0, 2048 ::
+  output: a0=1f+8388608
+auipc a0, 4096 ::
+  output: a0=1f+16777216
+auipc a0, 8192 ::
+  output: a0=1f+33554432
+auipc a0, 16384 ::
+  output: a0=1f+67108864
+auipc a0, 32768 ::
+  output: a0=1f+134217728
+auipc a0, 65536 ::
+  output: a0=1f+268435456
+auipc a0, 131072 ::
+  output: a0=1f+536870912
+auipc a0, 262144 ::
+  output: a0=1f+1073741824
+auipc a0, 524288 ::
+  output: a0=1f-2147483648
+auipc a0, 1048575 ::
+  output: a0=1f-4096
+auipc t6, 1 ::
+  output: t6=1f+4096
+auipc zero, 1 ::
+  output: zero=0x0000000000000000
+jal t0, .+4 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+6 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+8 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+16 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+32 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+64 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+128 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+256 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+512 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+1024 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .+2048 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .-4 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .-6 ::
+  output: t0=1f+4
+  target: reached
+jal t0, .-2048 ::
+  output: t0=1f+4
+  target: reached
+jal t6, .+4 ::
+  output: t6=1f+4
+  target: reached
+jal zero, .+4 ::
+  output: zero=0x0000000000000000
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f+4
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f+6
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f+8
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f-4
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f-6
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f-8
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t0) ::
+  inputs: t0=1f-8
+  output: ra=1f+4
+  target: reached
+jalr ra, 1(t0) ::
+  inputs: t0=1f-9
+  output: ra=1f+4
+  target: reached
+jalr ra, 2(t0) ::
+  inputs: t0=1f-10
+  output: ra=1f+4
+  target: reached
+jalr ra, 4(t0) ::
+  inputs: t0=1f-12
+  output: ra=1f+4
+  target: reached
+jalr ra, 8(t0) ::
+  inputs: t0=1f-16
+  output: ra=1f+4
+  target: reached
+jalr ra, 16(t0) ::
+  inputs: t0=1f-24
+  output: ra=1f+4
+  target: reached
+jalr ra, 32(t0) ::
+  inputs: t0=1f-40
+  output: ra=1f+4
+  target: reached
+jalr ra, 64(t0) ::
+  inputs: t0=1f-72
+  output: ra=1f+4
+  target: reached
+jalr ra, 128(t0) ::
+  inputs: t0=1f-136
+  output: ra=1f+4
+  target: reached
+jalr ra, 256(t0) ::
+  inputs: t0=1f-264
+  output: ra=1f+4
+  target: reached
+jalr ra, 512(t0) ::
+  inputs: t0=1f-520
+  output: ra=1f+4
+  target: reached
+jalr ra, 1024(t0) ::
+  inputs: t0=1f-1032
+  output: ra=1f+4
+  target: reached
+jalr ra, 2047(t0) ::
+  inputs: t0=1f-2055
+  output: ra=1f+4
+  target: reached
+jalr ra, -1(t0) ::
+  inputs: t0=1f-7
+  output: ra=1f+4
+  target: reached
+jalr ra, -2048(t0) ::
+  inputs: t0=1f+2040
+  output: ra=1f+4
+  target: reached
+jalr ra, 0(t6) ::
+  inputs: t6=1f+4
+  output: ra=1f+4
+  target: reached
+jalr zero, 0(a0) ::
+  inputs: a0=1f+4
+  output: zero=0x0000000000000000
+  target: reached
+beq a0, a1, .+4 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+6 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+8 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+16 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+32 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+64 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+128 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+256 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+512 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+1024 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .+2048 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .-4 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .-6 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq a0, a1, .-2048 ::
+  inputs: a0=0, a1=0
+  target: reached
+beq t5, t6, .+4 ::
+  inputs: t5=0, t6=0
+  target: reached
+beq a0, a1, 1f ::
+  inputs: a0=0, a1=0
+  branch: taken
+beq a0, a1, 1f ::
+  inputs: a0=0, a1=1
+  branch: not taken
+beq a0, a1, 1f ::
+  inputs: a0=1, a1=0
+  branch: not taken
+beq a0, a1, 1f ::
+  inputs: a0=1, a1=1
+  branch: taken
+beq a0, zero, 1f ::
+  inputs: a0=0, zero=0
+  branch: taken
+beq a0, zero, 1f ::
+  inputs: a0=1, zero=0
+  branch: not taken
+beq zero, a0, 1f ::
+  inputs: zero=0, a0=0
+  branch: taken
+beq zero, a0, 1f ::
+  inputs: zero=0, a0=1
+  branch: not taken
+beq a0, a1, 1f ::
+  inputs: a0=0, a1=-1
+  branch: not taken
+beq a0, a1, 1f ::
+  inputs: a0=-1, a1=0
+  branch: not taken
+beq a0, a1, 1f ::
+  inputs: a0=-1, a1=-1
+  branch: taken
+bne a0, a1, 1f ::
+  inputs: a0=0, a1=0
+  branch: not taken
+bne a0, a1, 1f ::
+  inputs: a0=0, a1=1
+  branch: taken
+bne a0, a1, 1f ::
+  inputs: a0=1, a1=0
+  branch: taken
+bne a0, a1, 1f ::
+  inputs: a0=1, a1=1
+  branch: not taken
+bne a0, zero, 1f ::
+  inputs: a0=0, zero=0
+  branch: not taken
+bne a0, zero, 1f ::
+  inputs: a0=1, zero=0
+  branch: taken
+bne zero, a0, 1f ::
+  inputs: zero=0, a0=0
+  branch: not taken
+bne zero, a0, 1f ::
+  inputs: zero=0, a0=1
+  branch: taken
+bne a0, a1, 1f ::
+  inputs: a0=0, a1=-1
+  branch: taken
+bne a0, a1, 1f ::
+  inputs: a0=-1, a1=0
+  branch: taken
+bne a0, a1, 1f ::
+  inputs: a0=-1, a1=-1
+  branch: not taken
+blt a0, a1, 1f ::
+  inputs: a0=0, a1=0
+  branch: not taken
+blt a0, a1, 1f ::
+  inputs: a0=0, a1=1
+  branch: taken
+blt a0, a1, 1f ::
+  inputs: a0=1, a1=0
+  branch: not taken
+blt a0, a1, 1f ::
+  inputs: a0=1, a1=1
+  branch: not taken
+blt a0, zero, 1f ::
+  inputs: a0=0, zero=0
+  branch: not taken
+blt a0, zero, 1f ::
+  inputs: a0=1, zero=0
+  branch: not taken
+blt zero, a0, 1f ::
+  inputs: zero=0, a0=0
+  branch: not taken
+blt zero, a0, 1f ::
+  inputs: zero=0, a0=1
+  branch: taken
+blt a0, a1, 1f ::
+  inputs: a0=0, a1=-1
+  branch: not taken
+blt a0, a1, 1f ::
+  inputs: a0=-1, a1=0
+  branch: taken
+blt a0, a1, 1f ::
+  inputs: a0=-1, a1=-1
+  branch: not taken
+bge a0, a1, 1f ::
+  inputs: a0=0, a1=0
+  branch: taken
+bge a0, a1, 1f ::
+  inputs: a0=0, a1=1
+  branch: not taken
+bge a0, a1, 1f ::
+  inputs: a0=1, a1=0
+  branch: taken
+bge a0, a1, 1f ::
+  inputs: a0=1, a1=1
+  branch: taken
+bge a0, zero, 1f ::
+  inputs: a0=0, zero=0
+  branch: taken
+bge a0, zero, 1f ::
+  inputs: a0=1, zero=0
+  branch: taken
+bge zero, a0, 1f ::
+  inputs: zero=0, a0=0
+  branch: taken
+bge zero, a0, 1f ::
+  inputs: zero=0, a0=1
+  branch: not taken
+bge a0, a1, 1f ::
+  inputs: a0=0, a1=-1
+  branch: taken
+bge a0, a1, 1f ::
+  inputs: a0=-1, a1=0
+  branch: not taken
+bge a0, a1, 1f ::
+  inputs: a0=-1, a1=-1
+  branch: taken
+bltu a0, a1, 1f ::
+  inputs: a0=0, a1=0
+  branch: not taken
+bltu a0, a1, 1f ::
+  inputs: a0=0, a1=1
+  branch: taken
+bltu a0, a1, 1f ::
+  inputs: a0=1, a1=0
+  branch: not taken
+bltu a0, a1, 1f ::
+  inputs: a0=1, a1=1
+  branch: not taken
+bltu a0, zero, 1f ::
+  inputs: a0=0, zero=0
+  branch: not taken
+bltu a0, zero, 1f ::
+  inputs: a0=1, zero=0
+  branch: not taken
+bltu zero, a0, 1f ::
+  inputs: zero=0, a0=0
+  branch: not taken
+bltu zero, a0, 1f ::
+  inputs: zero=0, a0=1
+  branch: taken
+bltu a0, a1, 1f ::
+  inputs: a0=0, a1=-1
+  branch: taken
+bltu a0, a1, 1f ::
+  inputs: a0=-1, a1=0
+  branch: not taken
+bltu a0, a1, 1f ::
+  inputs: a0=-1, a1=-1
+  branch: not taken
+bgeu a0, a1, 1f ::
+  inputs: a0=0, a1=0
+  branch: taken
+bgeu a0, a1, 1f ::
+  inputs: a0=0, a1=1
+  branch: not taken
+bgeu a0, a1, 1f ::
+  inputs: a0=1, a1=0
+  branch: taken
+bgeu a0, a1, 1f ::
+  inputs: a0=1, a1=1
+  branch: taken
+bgeu a0, zero, 1f ::
+  inputs: a0=0, zero=0
+  branch: taken
+bgeu a0, zero, 1f ::
+  inputs: a0=1, zero=0
+  branch: taken
+bgeu zero, a0, 1f ::
+  inputs: zero=0, a0=0
+  branch: taken
+bgeu zero, a0, 1f ::
+  inputs: zero=0, a0=1
+  branch: not taken
+bgeu a0, a1, 1f ::
+  inputs: a0=0, a1=-1
+  branch: not taken
+bgeu a0, a1, 1f ::
+  inputs: a0=-1, a1=0
+  branch: taken
+bgeu a0, a1, 1f ::
+  inputs: a0=-1, a1=-1
+  branch: taken
+lb a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffa0
+  no memory changes
+lb a0, 1(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000079
+  no memory changes
+lb a0, 2(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffed
+  no memory changes
+lb a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000068
+  no memory changes
+lb a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffef
+  no memory changes
+lb a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000011
+  no memory changes
+lb a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffe3
+  no memory changes
+lb a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffff92
+  no memory changes
+lb a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000042
+  no memory changes
+lb a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffc7
+  no memory changes
+lb a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffcd
+  no memory changes
+lb a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000004a
+  no memory changes
+lb a0, 2047(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffffbc
+  no memory changes
+lb a0, -1(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000002b
+  no memory changes
+lb a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffff8c
+  no memory changes
+lb a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0xffffffffffffffed
+  no memory changes
+lb zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+lh a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000396f
+  no memory changes
+lh a0, 2(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffbe76
+  no memory changes
+lh a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000036cd
+  no memory changes
+lh a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000003f2d
+  no memory changes
+lh a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000607f
+  no memory changes
+lh a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000031b1
+  no memory changes
+lh a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000007a20
+  no memory changes
+lh a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffa24f
+  no memory changes
+lh a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000002ed4
+  no memory changes
+lh a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffff9eda
+  no memory changes
+lh a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffdc57
+  no memory changes
+lh a0, 2046(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffff8f1e
+  no memory changes
+lh a0, -2(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xfffffffffffffef5
+  no memory changes
+lh a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffffffff99
+  no memory changes
+lh a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0x00000000000020fa
+  no memory changes
+lh zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+lw a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000050f7f87c
+  no memory changes
+lw a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffdde6ba3c
+  no memory changes
+lw a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffff8349fb67
+  no memory changes
+lw a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000006ce42545
+  no memory changes
+lw a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000027636871
+  no memory changes
+lw a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000f131555
+  no memory changes
+lw a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000005806056c
+  no memory changes
+lw a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xffffffffdcef22c1
+  no memory changes
+lw a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000003109b267
+  no memory changes
+lw a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000003db72f24
+  no memory changes
+lw a0, 2044(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000006e0e703
+  no memory changes
+lw a0, -4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000074b87535
+  no memory changes
+lw a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000003d9ad2e6
+  no memory changes
+lw a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0xfffffffff267f447
+  no memory changes
+lw zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+lbu a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000c9
+  no memory changes
+lbu a0, 1(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000b8
+  no memory changes
+lbu a0, 2(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000089
+  no memory changes
+lbu a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000098
+  no memory changes
+lbu a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000a8
+  no memory changes
+lbu a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000005a
+  no memory changes
+lbu a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000004c
+  no memory changes
+lbu a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000003b
+  no memory changes
+lbu a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000006a
+  no memory changes
+lbu a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000ef
+  no memory changes
+lbu a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000f5
+  no memory changes
+lbu a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000072
+  no memory changes
+lbu a0, 2047(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000036
+  no memory changes
+lbu a0, -1(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000a5
+  no memory changes
+lbu a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000000b4
+  no memory changes
+lbu a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0x0000000000000015
+  no memory changes
+lbu zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+lhu a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000007797
+  no memory changes
+lhu a0, 2(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000006013
+  no memory changes
+lhu a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000ddfe
+  no memory changes
+lhu a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000cde5
+  no memory changes
+lhu a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000003ec7
+  no memory changes
+lhu a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000af19
+  no memory changes
+lhu a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000038c8
+  no memory changes
+lhu a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000e078
+  no memory changes
+lhu a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000006dfd
+  no memory changes
+lhu a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000000dd03
+  no memory changes
+lhu a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000001a80
+  no memory changes
+lhu a0, 2046(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000000af2
+  no memory changes
+lhu a0, -2(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000000079ca
+  no memory changes
+lhu a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000000003dc2
+  no memory changes
+lhu a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0x0000000000005f23
+  no memory changes
+lhu zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+sb a0, 0(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 1(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. 89 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 2(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. 89 .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. 89 .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 8(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 .. .. .. .. .. .. ..
+sb a0, 16(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 32(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 64(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 128(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+128]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 256(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+256]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 512(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+512]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 1024(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+1024]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a0, 2047(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+2032]  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 89
+sb a0, -1(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-016]  .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 89
+sb a0, -2048(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-2048]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sb a4, 0(a5) ::
+  inputs: a4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 .. .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 0(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 2(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. 89 67 .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. 89 67 .. .. .. .. .. .. .. .. .. ..
+sh a0, 8(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 .. .. .. .. .. ..
+sh a0, 16(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 32(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 64(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 128(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+128]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 256(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+256]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 512(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+512]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 1024(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+1024]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a0, 2046(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+2032]  .. .. .. .. .. .. .. .. .. .. .. .. .. .. 89 67
+sh a0, -2(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-016]  .. .. .. .. .. .. .. .. .. .. .. .. .. .. 89 67
+sh a0, -2048(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-2048]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sh a4, 0(a5) ::
+  inputs: a4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 67 .. .. .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 0(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. 89 67 45 23 .. .. .. .. .. .. .. ..
+sw a0, 8(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 .. .. .. ..
+sw a0, 16(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 32(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 64(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 128(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+128]  89 .. 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 256(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+256]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 512(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+512]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 1024(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+1024]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a0, 2044(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+2032]  .. .. .. .. .. .. .. .. .. .. .. .. 89 67 45 23
+sw a0, -4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-016]  .. .. .. .. .. .. .. .. .. .. .. .. 89 67 45 23
+sw a0, -2048(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-2048]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+sw a4, 0(a5) ::
+  inputs: a4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 67 45 23 .. .. .. .. .. .. .. .. .. .. .. ..
+addi a0, a1, 1 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001001
+addi a0, a1, 2 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001002
+addi a0, a1, 4 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001004
+addi a0, a1, 8 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001008
+addi a0, a1, 16 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001010
+addi a0, a1, 32 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001020
+addi a0, a1, 64 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001040
+addi a0, a1, 128 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001080
+addi a0, a1, 256 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001100
+addi a0, a1, 1024 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001400
+addi a0, a1, 2047 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x00000000000017ff
+addi a0, a1, -1 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000000fff
+addi a0, a1, -2048 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000000800
+addi a0, a1, 1 ::
+  inputs: a1=0x000000007fffffff
+  output: a0=0x0000000080000000
+addi a0, a1, 1 ::
+  inputs: a1=0x00000000fffffffe
+  output: a0=0x00000000ffffffff
+addi a0, a1, 1 ::
+  inputs: a1=0x00000000ffffffff
+  output: a0=0x0000000100000000
+addi t5, t6, 1 ::
+  inputs: t6=0x0000000000001000
+  output: t5=0x0000000000001001
+addi zero, a0, 1 ::
+  inputs: a0=0x0000000000001000
+  output: zero=0x0000000000000000
+slti a0, a1, 0 ::
+  inputs: a1=0x0000000000000000
+  output: a0=0x0000000000000000
+slti a0, a1, 0 ::
+  inputs: a1=0x0000000000000001
+  output: a0=0x0000000000000000
+slti a0, a1, 0 ::
+  inputs: a1=0xffffffffffffffff
+  output: a0=0x0000000000000001
+slti a0, a1, 0x7ff ::
+  inputs: a1=0x00000000000007ff
+  output: a0=0x0000000000000000
+slti a0, a1, 0x7ff ::
+  inputs: a1=0x0000000000000800
+  output: a0=0x0000000000000000
+slti a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0xffffffffffffffff
+  output: a0=0x0000000000000000
+slti a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0x0000000000000000
+  output: a0=0x0000000000000000
+slti t5, t6, 0 ::
+  inputs: t6=0x0000000000000000
+  output: t5=0x0000000000000000
+slti t5, t6, 0 ::
+  inputs: t6=0x0000000000000001
+  output: t5=0x0000000000000000
+slti zero, a0, 1 ::
+  inputs: a0=0x0000000000000000
+  output: zero=0x0000000000000000
+sltiu a0, a1, 0 ::
+  inputs: a1=0x0000000000000000
+  output: a0=0x0000000000000000
+sltiu a0, a1, 0 ::
+  inputs: a1=0x0000000000000001
+  output: a0=0x0000000000000000
+sltiu a0, a1, 0 ::
+  inputs: a1=0xffffffffffffffff
+  output: a0=0x0000000000000000
+sltiu a0, a1, 0x7ff ::
+  inputs: a1=0x00000000000007ff
+  output: a0=0x0000000000000000
+sltiu a0, a1, 0x7ff ::
+  inputs: a1=0x0000000000000800
+  output: a0=0x0000000000000000
+sltiu a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0xffffffffffffffff
+  output: a0=0x0000000000000000
+sltiu a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0x0000000000000000
+  output: a0=0x0000000000000001
+sltiu t5, t6, 0 ::
+  inputs: t6=0x0000000000000000
+  output: t5=0x0000000000000000
+sltiu t5, t6, 0 ::
+  inputs: t6=0x0000000000000001
+  output: t5=0x0000000000000000
+sltiu zero, a0, 1 ::
+  inputs: a0=0x0000000000000000
+  output: zero=0x0000000000000000
+xori a0, a1, 0 ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x0000ffff0000ffff
+xori a0, a1, 0 ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0xffff0000ffff0000
+xori a0, a1, 0x7ff ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x0000ffff0000f800
+xori a0, a1, 0x7ff ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0xffff0000ffff07ff
+xori a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0xffff0000ffff0000
+xori a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0x0000ffff0000ffff
+xori t5, t6, 0 ::
+  inputs: t6=0x0000ffff0000ffff
+  output: t5=0x0000ffff0000ffff
+xori zero, a0, 0x7ff ::
+  inputs: a0=0x0000ffff0000ffff
+  output: zero=0x0000000000000000
+ori a0, a1, 0 ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x0000ffff0000ffff
+ori a0, a1, 0 ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0xffff0000ffff0000
+ori a0, a1, 0x7ff ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x0000ffff0000ffff
+ori a0, a1, 0x7ff ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0xffff0000ffff07ff
+ori a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0xffffffffffffffff
+ori a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0xffffffffffffffff
+ori t5, t6, 0 ::
+  inputs: t6=0x0000ffff0000ffff
+  output: t5=0x0000ffff0000ffff
+ori zero, a0, 0x7ff ::
+  inputs: a0=0x0000ffff0000ffff
+  output: zero=0x0000000000000000
+andi a0, a1, 0 ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x0000000000000000
+andi a0, a1, 0 ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0x0000000000000000
+andi a0, a1, 0x7ff ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x00000000000007ff
+andi a0, a1, 0x7ff ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0x0000000000000000
+andi a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0x0000ffff0000ffff
+  output: a0=0x0000ffff0000ffff
+andi a0, a1, 0xffffffffffffffff ::
+  inputs: a1=0xffff0000ffff0000
+  output: a0=0xffff0000ffff0000
+andi t5, t6, 0 ::
+  inputs: t6=0x0000ffff0000ffff
+  output: t5=0x0000000000000000
+andi zero, a0, 0x7ff ::
+  inputs: a0=0x0000ffff0000ffff
+  output: zero=0x0000000000000000
+slli a0, a1, 0 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xabcdef0123456789
+slli a0, a1, 1 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x579bde02468acf12
+slli a0, a1, 2 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xaf37bc048d159e24
+slli a0, a1, 4 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xbcdef01234567890
+slli a0, a1, 8 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xcdef012345678900
+slli a0, a1, 16 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xef01234567890000
+slli a0, a1, 32 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x2345678900000000
+slli a0, a1, 63 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x8000000000000000
+slli t5, t6, 1 ::
+  inputs: t6=0xabcdef0123456789
+  output: t5=0x579bde02468acf12
+slli zero, a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: zero=0x0000000000000000
+srli a0, a1, 0 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xabcdef0123456789
+srli a0, a1, 1 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x55e6f78091a2b3c4
+srli a0, a1, 2 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x2af37bc048d159e2
+srli a0, a1, 4 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0abcdef012345678
+srli a0, a1, 8 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x00abcdef01234567
+srli a0, a1, 16 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000abcdef012345
+srli a0, a1, 32 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x00000000abcdef01
+srli a0, a1, 63 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000000001
+srli t5, t6, 1 ::
+  inputs: t6=0xabcdef0123456789
+  output: t5=0x55e6f78091a2b3c4
+srli zero, a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: zero=0x0000000000000000
+srai a0, a1, 0 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xabcdef0123456789
+srai a0, a1, 1 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xd5e6f78091a2b3c4
+srai a0, a1, 2 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xeaf37bc048d159e2
+srai a0, a1, 4 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xfabcdef012345678
+srai a0, a1, 8 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xffabcdef01234567
+srai a0, a1, 16 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xffffabcdef012345
+srai a0, a1, 32 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xffffffffabcdef01
+srai a0, a1, 63 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xffffffffffffffff
+srai t5, t6, 1 ::
+  inputs: t6=0xabcdef0123456789
+  output: t5=0xd5e6f78091a2b3c4
+srai zero, a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: zero=0x0000000000000000
+add a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000002000
+  output: a0=0x0000000000003000
+add a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000001
+  output: a0=0x0000000080000000
+add a0, a1, a2 ::
+  inputs: a1=0x00000000fffffffe, a2=0x0000000000000001
+  output: a0=0x00000000ffffffff
+add a0, a1, a2 ::
+  inputs: a1=0x00000000ffffffff, a2=0x0000000000000001
+  output: a0=0x0000000100000000
+add a0, a1, a2 ::
+  inputs: a1=0xfffffffffffffffe, a2=0x0000000000000001
+  output: a0=0xffffffffffffffff
+add a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000001
+  output: a0=0x0000000000000000
+add t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000003000
+add zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+sub a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000000fff
+  output: a0=0x0000000000000001
+sub a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000001000
+  output: a0=0x0000000000000000
+sub a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000001001
+  output: a0=0xffffffffffffffff
+sub a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+sub a0, a1, a2 ::
+  inputs: a1=0x0000000100000000, a2=0x0000000000000001
+  output: a0=0x00000000ffffffff
+sub t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000000fff
+  output: t4=0x0000000000000001
+sub zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000000fff
+  output: zero=0x0000000000000000
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000000
+  output: a0=0xabcdef0123456789
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000001
+  output: a0=0x579bde02468acf12
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000002
+  output: a0=0xaf37bc048d159e24
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000004
+  output: a0=0xbcdef01234567890
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000008
+  output: a0=0xcdef012345678900
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000010
+  output: a0=0xef01234567890000
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000020
+  output: a0=0x2345678900000000
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x000000000000003f
+  output: a0=0x8000000000000000
+sll a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000040
+  output: a0=0xabcdef0123456789
+sll t4, t5, t6 ::
+  inputs: t5=0xabcdef0123456789, t6=0x0000000000000001
+  output: t4=0x579bde02468acf12
+sll zero, a0, a1 ::
+  inputs: a0=0xabcdef0123456789, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+slt a0, a1, a2 ::
+  inputs: a1=0x0000000000000000, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+slt a0, a1, a2 ::
+  inputs: a1=0x0000000000000000, a2=0x0000000000000001
+  output: a0=0x0000000000000001
+slt a0, a1, a2 ::
+  inputs: a1=0x0000000000000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+slt t4, t5, t6 ::
+  inputs: t5=0x0000000000000000, t6=0x0000000000000000
+  output: t4=0x0000000000000000
+slt t4, t5, t6 ::
+  inputs: t5=0x0000000000000000, t6=0x0000000000000001
+  output: t4=0x0000000000000001
+slt zero, a0, a1 ::
+  inputs: a0=0x0000000000000000, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+sltu a0, a1, a2 ::
+  inputs: a1=0x0000000000000000, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+sltu a0, a1, a2 ::
+  inputs: a1=0x0000000000000000, a2=0x0000000000000001
+  output: a0=0x0000000000000001
+sltu a0, a1, a2 ::
+  inputs: a1=0x0000000000000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+sltu t4, t5, t6 ::
+  inputs: t5=0x0000000000000000, t6=0x0000000000000000
+  output: t4=0x0000000000000000
+sltu t4, t5, t6 ::
+  inputs: t5=0x0000000000000000, t6=0x0000000000000001
+  output: t4=0x0000000000000001
+sltu zero, a0, a1 ::
+  inputs: a0=0x0000000000000000, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+xor a0, a1, a2 ::
+  inputs: a1=0x0000ffff0000ffff, a2=0x00000000ffffffff
+  output: a0=0x0000ffffffff0000
+xor t4, t5, t6 ::
+  inputs: t5=0x0000ffff0000ffff, t6=0x00000000ffffffff
+  output: t4=0x0000ffffffff0000
+xor zero, a0, a1 ::
+  inputs: a0=0x0000ffff0000ffff, a1=0x00000000ffffffff
+  output: zero=0x0000000000000000
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000000
+  output: a0=0xabcdef0123456789
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000001
+  output: a0=0x55e6f78091a2b3c4
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000002
+  output: a0=0x2af37bc048d159e2
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000004
+  output: a0=0x0abcdef012345678
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000008
+  output: a0=0x00abcdef01234567
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000010
+  output: a0=0x0000abcdef012345
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000020
+  output: a0=0x00000000abcdef01
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x000000000000003f
+  output: a0=0x0000000000000001
+srl a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000040
+  output: a0=0xabcdef0123456789
+srl t4, t5, t6 ::
+  inputs: t5=0xabcdef0123456789, t6=0x0000000000000001
+  output: t4=0x55e6f78091a2b3c4
+srl zero, a0, a1 ::
+  inputs: a0=0xabcdef0123456789, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000000
+  output: a0=0xabcdef0123456789
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000001
+  output: a0=0xd5e6f78091a2b3c4
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000002
+  output: a0=0xeaf37bc048d159e2
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000004
+  output: a0=0xfabcdef012345678
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000008
+  output: a0=0xffabcdef01234567
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000010
+  output: a0=0xffffabcdef012345
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000020
+  output: a0=0xffffffffabcdef01
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x000000000000003f
+  output: a0=0xffffffffffffffff
+sra a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000040
+  output: a0=0xabcdef0123456789
+sra t4, t5, t6 ::
+  inputs: t5=0xabcdef0123456789, t6=0x0000000000000001
+  output: t4=0xd5e6f78091a2b3c4
+sra zero, a0, a1 ::
+  inputs: a0=0xabcdef0123456789, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+or a0, a1, a2 ::
+  inputs: a1=0x0000ffff0000ffff, a2=0x00000000ffffffff
+  output: a0=0x0000ffffffffffff
+or t4, t5, t6 ::
+  inputs: t5=0x0000ffff0000ffff, t6=0x00000000ffffffff
+  output: t4=0x0000ffffffffffff
+or zero, a0, a1 ::
+  inputs: a0=0x0000ffff0000ffff, a1=0x00000000ffffffff
+  output: zero=0x0000000000000000
+and a0, a1, a2 ::
+  inputs: a1=0x0000ffff0000ffff, a2=0x00000000ffffffff
+  output: a0=0x000000000000ffff
+and t4, t5, t6 ::
+  inputs: t5=0x0000ffff0000ffff, t6=0x00000000ffffffff
+  output: t4=0x000000000000ffff
+and zero, a0, a1 ::
+  inputs: a0=0x0000ffff0000ffff, a1=0x00000000ffffffff
+  output: zero=0x0000000000000000
+fence ::
+
+RV64I base instruction set, additions
+lwu a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000ab16b28b
+  no memory changes
+lwu a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000000ec1edba
+  no memory changes
+lwu a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000006117bfcc
+  no memory changes
+lwu a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000002d4154e2
+  no memory changes
+lwu a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000ce7e8e1c
+  no memory changes
+lwu a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000022aa69b
+  no memory changes
+lwu a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000e3156eea
+  no memory changes
+lwu a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000097ee3bb0
+  no memory changes
+lwu a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x000000004ce82b35
+  no memory changes
+lwu a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000185668b2
+  no memory changes
+lwu a0, 2044(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x00000000e3fcbe9a
+  no memory changes
+lwu a0, -4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000052d34dcc
+  no memory changes
+lwu a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0000000098b98bf4
+  no memory changes
+lwu a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0x000000004d86ad56
+  no memory changes
+lwu zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+ld a0, 0(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x396ebd333e9785d7
+  no memory changes
+ld a0, 4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x2d4a090205687129
+  no memory changes
+ld a0, 8(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xd300b16a85cd7c06
+  no memory changes
+ld a0, 16(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x0de93ef601cc1aa8
+  no memory changes
+ld a0, 32(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x933e83556593c5dc
+  no memory changes
+ld a0, 64(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xc30c82379dd342d0
+  no memory changes
+ld a0, 128(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x88f1496a86e6d207
+  no memory changes
+ld a0, 256(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x2a6f38124a0f2f9c
+  no memory changes
+ld a0, 512(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xd04a4b711fa93ec2
+  no memory changes
+ld a0, 1024(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x4aa8a98a2b57bc7f
+  no memory changes
+ld a0, 2040(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0x59bee9ecfe744029
+  no memory changes
+ld a0, -4(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xeef6a921c896781e
+  no memory changes
+ld a0, -2048(a1) ::
+  inputs: a1=&area_mid
+  output: a0=0xa56a8d842a3a5f41
+  no memory changes
+ld a4, 0(a5) ::
+  inputs: a5=&area_mid
+  output: a4=0x9821dab6df0781a2
+  no memory changes
+ld zero, 0(a0) ::
+  inputs: a0=&area_mid
+  output: zero=0x0000000000000000
+  no memory changes
+sd a0, 0(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. 89 67 45 23 01 ef cd ab .. .. .. ..
+sd a0, 8(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+000]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+sd a0, 16(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+016]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 32(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+032]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 64(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+064]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 128(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+128]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 256(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+256]  .. 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 512(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+512]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 1024(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+1024]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a0, 2040(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [+2032]  .. .. .. .. .. .. .. .. 89 67 45 23 01 ef cd ab
+sd a0, -4(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-016]  .. .. .. .. .. .. .. .. .. .. .. .. 89 67 45 23
+  [+000]  01 ef cd ab .. .. .. .. .. .. .. .. .. .. .. ..
+sd a0, -2048(a1) ::
+  inputs: a0=0xabcdef0123456789, a1=&area_mid
+  [-2048]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+sd a4, 0(a5) ::
+  inputs: a4=0xabcdef0123456789, a5=&area_mid
+  [+000]  89 67 45 23 01 ef cd ab .. .. .. .. .. .. .. ..
+addiw a0, a1, 1 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001001
+addiw a0, a1, 2 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001002
+addiw a0, a1, 4 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001004
+addiw a0, a1, 8 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001008
+addiw a0, a1, 16 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001010
+addiw a0, a1, 32 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001020
+addiw a0, a1, 64 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001040
+addiw a0, a1, 128 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001080
+addiw a0, a1, 256 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001100
+addiw a0, a1, 1024 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000001400
+addiw a0, a1, 2047 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x00000000000017ff
+addiw a0, a1, -1 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000000fff
+addiw a0, a1, -2048 ::
+  inputs: a1=0x0000000000001000
+  output: a0=0x0000000000000800
+addiw a0, a1, 1 ::
+  inputs: a1=0x000000007fffffff
+  output: a0=0xffffffff80000000
+addiw a0, a1, 1 ::
+  inputs: a1=0x00000000fffffffe
+  output: a0=0xffffffffffffffff
+addiw a0, a1, 1 ::
+  inputs: a1=0x00000000ffffffff
+  output: a0=0x0000000000000000
+addiw t5, t6, 1 ::
+  inputs: t6=0x0000000000001000
+  output: t5=0x0000000000001001
+addiw zero, a0, 1 ::
+  inputs: a0=0x0000000000001000
+  output: zero=0x0000000000000000
+slliw a0, a1, 0 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000023456789
+slliw a0, a1, 1 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x00000000468acf12
+slliw a0, a1, 2 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xffffffff8d159e24
+slliw a0, a1, 4 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000034567890
+slliw a0, a1, 8 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000045678900
+slliw a0, a1, 16 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000067890000
+slliw a0, a1, 31 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0xffffffff80000000
+slliw t5, t6, 1 ::
+  inputs: t6=0xabcdef0123456789
+  output: t5=0x00000000468acf12
+slliw zero, a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: zero=0x0000000000000000
+srliw a0, a1, 0 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000023456789
+srliw a0, a1, 1 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000011a2b3c4
+srliw a0, a1, 2 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000008d159e2
+srliw a0, a1, 4 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000002345678
+srliw a0, a1, 8 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000234567
+srliw a0, a1, 16 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000002345
+srliw a0, a1, 31 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000000000
+srliw t5, t6, 1 ::
+  inputs: t6=0xabcdef0123456789
+  output: t5=0x0000000011a2b3c4
+srliw zero, a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: zero=0x0000000000000000
+sraiw a0, a1, 0 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000023456789
+sraiw a0, a1, 1 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000011a2b3c4
+sraiw a0, a1, 2 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000008d159e2
+sraiw a0, a1, 4 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000002345678
+sraiw a0, a1, 8 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000234567
+sraiw a0, a1, 16 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000002345
+sraiw a0, a1, 31 ::
+  inputs: a1=0xabcdef0123456789
+  output: a0=0x0000000000000000
+srai t5, t6, 1 ::
+  inputs: t6=0xabcdef0123456789
+  output: t5=0xd5e6f78091a2b3c4
+srai zero, a0, 1 ::
+  inputs: a0=0xabcdef0123456789
+  output: zero=0x0000000000000000
+addw a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000002000
+  output: a0=0x0000000000003000
+addw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000001
+  output: a0=0xffffffff80000000
+addw a0, a1, a2 ::
+  inputs: a1=0x00000000fffffffe, a2=0x0000000000000001
+  output: a0=0xffffffffffffffff
+addw a0, a1, a2 ::
+  inputs: a1=0x00000000ffffffff, a2=0x0000000000000001
+  output: a0=0x0000000000000000
+addw a0, a1, a2 ::
+  inputs: a1=0xfffffffffffffffe, a2=0x0000000000000001
+  output: a0=0xffffffffffffffff
+addw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000001
+  output: a0=0x0000000000000000
+addw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000003000
+addw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+subw a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000000fff
+  output: a0=0x0000000000000001
+subw a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000001000
+  output: a0=0x0000000000000000
+subw a0, a1, a2 ::
+  inputs: a1=0x0000000000001000, a2=0x0000000000001001
+  output: a0=0xffffffffffffffff
+subw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+subw a0, a1, a2 ::
+  inputs: a1=0x0000000100000000, a2=0x0000000000000001
+  output: a0=0xffffffffffffffff
+subw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000000fff
+  output: t4=0x0000000000000001
+subw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000000fff
+  output: zero=0x0000000000000000
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000000
+  output: a0=0x0000000023456789
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000001
+  output: a0=0x00000000468acf12
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000002
+  output: a0=0xffffffff8d159e24
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000004
+  output: a0=0x0000000034567890
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000008
+  output: a0=0x0000000045678900
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000010
+  output: a0=0x0000000067890000
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x000000000000001f
+  output: a0=0xffffffff80000000
+sllw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000020
+  output: a0=0x0000000023456789
+sllw t4, t5, t6 ::
+  inputs: t5=0xabcdef0123456789, t6=0x0000000000000001
+  output: t4=0x00000000468acf12
+sllw zero, a0, a1 ::
+  inputs: a0=0xabcdef0123456789, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000000
+  output: a0=0x0000000023456789
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000001
+  output: a0=0x0000000011a2b3c4
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000002
+  output: a0=0x0000000008d159e2
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000004
+  output: a0=0x0000000002345678
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000008
+  output: a0=0x0000000000234567
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000010
+  output: a0=0x0000000000002345
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x000000000000001f
+  output: a0=0x0000000000000000
+srlw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000020
+  output: a0=0x0000000023456789
+srlw t4, t5, t6 ::
+  inputs: t5=0xabcdef0123456789, t6=0x0000000000000001
+  output: t4=0x0000000011a2b3c4
+srlw zero, a0, a1 ::
+  inputs: a0=0xabcdef0123456789, a1=0x0000000000000001
+  output: zero=0x0000000000000000
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000000
+  output: a0=0x0000000023456789
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000001
+  output: a0=0x0000000011a2b3c4
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000002
+  output: a0=0x0000000008d159e2
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000004
+  output: a0=0x0000000002345678
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000008
+  output: a0=0x0000000000234567
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000010
+  output: a0=0x0000000000002345
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x000000000000001f
+  output: a0=0x0000000000000000
+sraw a0, a1, a2 ::
+  inputs: a1=0xabcdef0123456789, a2=0x0000000000000020
+  output: a0=0x0000000023456789
+sraw t4, t5, t6 ::
+  inputs: t5=0xabcdef0123456789, t6=0x0000000000000001
+  output: t4=0x0000000011a2b3c4
+sraw zero, a0, a1 ::
+  inputs: a0=0xabcdef0123456789, a1=0x0000000000000001
+  output: zero=0x0000000000000000
--- /dev/null
+++ b/none/tests/riscv64/integer.vgtest
@@ -0,0 +1,2 @@
+prog: integer
+vgopts: -q
--- /dev/null
+++ b/none/tests/riscv64/muldiv.c
@@ -0,0 +1,377 @@
+/* Tests for the RV64M standard multiplication and division instruction-set
+   extension. */
+
+#include "testinst.h"
+
+static void test_muldiv_shared(void)
+{
+   printf(
+      "RV64M multiplication and division instruction set, shared operations\n");
+
+   /* ------------------ mul rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x0000000000005000, 0x0000000000002000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x8000000000000000, 0x0000000000000002, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x8000000000000000, 0x8000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0x0000000000000001, 0x0000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "mul a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "mul t4, t5, t6", 0x0000000000001000, 0x0000000000002000, t4,
+                t5, t6);
+   TESTINST_1_2(4, "mul zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ mulh rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x8000000000000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x8000000000000000, 0x8000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulh a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "mulh t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "mulh zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ----------------- mulhsu rd, rs1, rs2 ----------------- */
+#if 0 /* TODO Enable. */
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x8000000000000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x8000000000000000, 0x8000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhsu a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "mulhsu t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "mulhsu zero, a0, a1", 0x0000000000001000,
+                0x0000000000002000, zero, a0, a1);
+#endif
+
+   /* ----------------- mulhu rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x8000000000000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x8000000000000000, 0x8000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulhu a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "mulhu t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "mulhu zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ div rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "div a0, a1, a2", 0x0000000000005000, 0x0000000000002000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x8000000000000000, 0x0000000000000002, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x8000000000000000, 0x8000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0x0000000000000001, 0x0000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "div a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "div t4, t5, t6", 0x0000000000005000, 0x0000000000002000, t4,
+                t5, t6);
+   TESTINST_1_2(4, "div zero, a0, a1", 0x0000000000005000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ divu rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x8000000000000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x8000000000000000, 0x8000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divu a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "divu t4, t5, t6", 0x0000000000005000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "divu zero, a0, a1", 0x0000000000005000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ rem rd, rs1, rs2 ------------------- */
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x0000000000005000, 0x0000000000002000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x8000000000000000, 0x0000000000000002, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x8000000000000000, 0x8000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0x0000000000000001, 0x0000000000000000, a0,
+                a1, a2);
+   TESTINST_1_2(4, "rem a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000, a0,
+                a1, a2);
+
+   TESTINST_1_2(4, "rem t4, t5, t6", 0x0000000000005000, 0x0000000000002000, t4,
+                t5, t6);
+   TESTINST_1_2(4, "rem zero, a0, a1", 0x0000000000005000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ remu rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x7fffffffffffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x7fffffffffffffff, 0x7fffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x7fffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x8000000000000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x8000000000000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x8000000000000000, 0x8000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remu a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "remu t4, t5, t6", 0x0000000000005000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "remu zero, a0, a1", 0x0000000000005000, 0x0000000000002000,
+                zero, a0, a1);
+
+   printf("\n");
+}
+
+static void test_muldiv_additions(void)
+{
+   printf("RV64M multiplication and division instruction set, additions\n");
+
+   /* ------------------ mulw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x000000007fffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x000000007fffffff, 0x000000007fffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x000000007fffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x0000000080000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x0000000080000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x0000000080000000, 0x0000000080000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "mulw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "mulw t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "mulw zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ divw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x000000007fffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x000000007fffffff, 0x000000007fffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x000000007fffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x0000000080000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x0000000080000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x0000000080000000, 0x0000000080000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "divw t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "divw zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ----------------- divuw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x000000007fffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x000000007fffffff, 0x000000007fffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x000000007fffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x0000000080000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x0000000080000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x0000000080000000, 0x0000000080000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "divuw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "divuw t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "divuw zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ------------------ remw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x000000007fffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x000000007fffffff, 0x000000007fffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x000000007fffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x0000000080000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x0000000080000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x0000000080000000, 0x0000000080000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "remw t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "remw zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+
+   /* ----------------- remuw rd, rs1, rs2 ------------------ */
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x0000000000005000, 0x0000000000002000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x000000007fffffff, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x000000007fffffff, 0x000000007fffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x000000007fffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0xffffffffffffffff, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x0000000080000000, 0x0000000000000002,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x0000000080000000, 0xffffffffffffffff,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x0000000080000000, 0x0000000080000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0x0000000000000001, 0x0000000000000000,
+                a0, a1, a2);
+   TESTINST_1_2(4, "remuw a0, a1, a2", 0xffffffffffffffff, 0x0000000000000000,
+                a0, a1, a2);
+
+   TESTINST_1_2(4, "remuw t4, t5, t6", 0x0000000000001000, 0x0000000000002000,
+                t4, t5, t6);
+   TESTINST_1_2(4, "remuw zero, a0, a1", 0x0000000000001000, 0x0000000000002000,
+                zero, a0, a1);
+}
+
+int main(void)
+{
+   test_muldiv_shared();
+   test_muldiv_additions();
+   return 0;
+}
--- /dev/null
+++ b/none/tests/riscv64/muldiv.stdout.exp
@@ -0,0 +1,435 @@
+RV64M multiplication and division instruction set, shared operations
+mul a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x000000000a000000
+mul a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0xfffffffffffffffe
+mul a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x0000000000000001
+mul a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x8000000000000001
+mul a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+mul a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+mul a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x8000000000000000
+mul a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x0000000000000000
+mul a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mul a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mul t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000002000000
+mul zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x3fffffffffffffff
+mulh a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0xffffffffffffffff
+mulh a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0xffffffffffffffff
+mulh a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x4000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mulh a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mulh t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000000000
+mulh zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+mulhu a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000000000
+mulhu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+mulhu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x3fffffffffffffff
+mulhu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x7ffffffffffffffe
+mulhu a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0xfffffffffffffffe
+mulhu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0x0000000000000001
+mulhu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x7fffffffffffffff
+mulhu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x4000000000000000
+mulhu a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mulhu a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mulhu t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000000000
+mulhu zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+div a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000000002
+div a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0x3fffffffffffffff
+div a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x0000000000000001
+div a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x8000000000000001
+div a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+div a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0xc000000000000000
+div a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x8000000000000000
+div a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x0000000000000001
+div a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+div a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+div t4, t5, t6 ::
+  inputs: t5=0x0000000000005000, t6=0x0000000000002000
+  output: t4=0x0000000000000002
+div zero, a0, a1 ::
+  inputs: a0=0x0000000000005000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+divu a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000000002
+divu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0x3fffffffffffffff
+divu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x0000000000000001
+divu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+divu a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+divu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0x4000000000000000
+divu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+divu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x0000000000000001
+divu a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+divu a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+divu t4, t5, t6 ::
+  inputs: t5=0x0000000000005000, t6=0x0000000000002000
+  output: t4=0x0000000000000002
+divu zero, a0, a1 ::
+  inputs: a0=0x0000000000005000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000001000
+rem a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0x0000000000000001
+rem a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x0000000000000000
+rem a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000001
+rem a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+rem t4, t5, t6 ::
+  inputs: t5=0x0000000000005000, t6=0x0000000000002000
+  output: t4=0x0000000000001000
+rem zero, a0, a1 ::
+  inputs: a0=0x0000000000005000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+remu a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000001000
+remu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x0000000000000002
+  output: a0=0x0000000000000001
+remu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0x7fffffffffffffff
+  output: a0=0x0000000000000000
+remu a0, a1, a2 ::
+  inputs: a1=0x7fffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x7fffffffffffffff
+remu a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+remu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+remu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0xffffffffffffffff
+  output: a0=0x8000000000000000
+remu a0, a1, a2 ::
+  inputs: a1=0x8000000000000000, a2=0x8000000000000000
+  output: a0=0x0000000000000000
+remu a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000001
+remu a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+remu t4, t5, t6 ::
+  inputs: t5=0x0000000000005000, t6=0x0000000000002000
+  output: t4=0x0000000000001000
+remu zero, a0, a1 ::
+  inputs: a0=0x0000000000005000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+
+RV64M multiplication and division instruction set, additions
+mulw a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x000000000a000000
+mulw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000002
+  output: a0=0xfffffffffffffffe
+mulw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x000000007fffffff
+  output: a0=0x0000000000000001
+mulw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0xffffffffffffffff
+  output: a0=0xffffffff80000001
+mulw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+mulw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+mulw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0xffffffffffffffff
+  output: a0=0xffffffff80000000
+mulw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000080000000
+  output: a0=0x0000000000000000
+mulw a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mulw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0x0000000000000000
+mulw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000002000000
+mulw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+divw a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000000002
+divw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000002
+  output: a0=0x000000003fffffff
+divw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x000000007fffffff
+  output: a0=0x0000000000000001
+divw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0xffffffffffffffff
+  output: a0=0xffffffff80000001
+divw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+divw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000000000002
+  output: a0=0xffffffffc0000000
+divw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0xffffffffffffffff
+  output: a0=0xffffffff80000000
+divw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000080000000
+  output: a0=0x0000000000000001
+divw a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+divw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+divw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000000000
+divw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+divuw a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000000002
+divuw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000002
+  output: a0=0x000000003fffffff
+divuw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x000000007fffffff
+  output: a0=0x0000000000000001
+divuw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+divuw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000001
+divuw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000000000002
+  output: a0=0x0000000040000000
+divuw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+divuw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000080000000
+  output: a0=0x0000000000000001
+divuw a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+divuw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+divuw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000000000
+divuw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000001000
+remw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000002
+  output: a0=0x0000000000000001
+remw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x000000007fffffff
+  output: a0=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000080000000
+  output: a0=0x0000000000000000
+remw a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000001
+remw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+remw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000001000
+remw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
+remuw a0, a1, a2 ::
+  inputs: a1=0x0000000000005000, a2=0x0000000000002000
+  output: a0=0x0000000000001000
+remuw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x0000000000000002
+  output: a0=0x0000000000000001
+remuw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0x000000007fffffff
+  output: a0=0x0000000000000000
+remuw a0, a1, a2 ::
+  inputs: a1=0x000000007fffffff, a2=0xffffffffffffffff
+  output: a0=0x000000007fffffff
+remuw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0xffffffffffffffff
+  output: a0=0x0000000000000000
+remuw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000000000002
+  output: a0=0x0000000000000000
+remuw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0xffffffffffffffff
+  output: a0=0xffffffff80000000
+remuw a0, a1, a2 ::
+  inputs: a1=0x0000000080000000, a2=0x0000000080000000
+  output: a0=0x0000000000000000
+remuw a0, a1, a2 ::
+  inputs: a1=0x0000000000000001, a2=0x0000000000000000
+  output: a0=0x0000000000000001
+remuw a0, a1, a2 ::
+  inputs: a1=0xffffffffffffffff, a2=0x0000000000000000
+  output: a0=0xffffffffffffffff
+remuw t4, t5, t6 ::
+  inputs: t5=0x0000000000001000, t6=0x0000000000002000
+  output: t4=0x0000000000001000
+remuw zero, a0, a1 ::
+  inputs: a0=0x0000000000001000, a1=0x0000000000002000
+  output: zero=0x0000000000000000
--- /dev/null
+++ b/none/tests/riscv64/muldiv.vgtest
@@ -0,0 +1,2 @@
+prog: muldiv
+vgopts: -q
--- /dev/null
+++ b/none/tests/riscv64/testinst.h
@@ -0,0 +1,758 @@
+#include "tests/malloc.h"
+#include <stdbool.h>
+#include <stdio.h>
+
+/* Helper functions. */
+
+static inline unsigned char rand_uchar(void)
+{
+   static unsigned int seed = 80021;
+
+   seed = 1103515245 * seed + 12345;
+   return (seed >> 17) & 0xFF;
+}
+
+static void show_block_diff(unsigned char* block1,
+                            unsigned char* block2,
+                            size_t         n,
+                            size_t         offset)
+{
+   bool block_changed = false;
+   for (size_t i = 0; i < n; i += 16) {
+      bool line_changed = false;
+      for (size_t j = i; j < n && j < i + 16; j++) {
+         if (block1[j] != block2[j]) {
+            line_changed = true;
+            break;
+         }
+      }
+      if (!line_changed)
+         continue;
+
+      if (i < offset)
+         printf("  [-%03zu] ", offset - i);
+      else
+         printf("  [+%03zu] ", i - offset);
+      for (size_t j = i; j < n && j < i + 16; j++) {
+         unsigned char diff = block1[j] - block2[j];
+         if (diff == 0)
+            printf(" ..");
+         else
+            printf(" %02x", block2[j]);
+      }
+      printf("\n");
+
+      block_changed = true;
+   }
+   if (!block_changed)
+      printf("  no memory changes\n");
+}
+
+/* Macros for testing individual instructions
+
+   Naming is in form TESTINST_<#outputs>_<#inputs>_<suffix-id>.
+
+   Environment to test each instruction is set up by a carefully crafted inline
+   assembly. The code implements own handling of input and output operands
+   which most importantly allows also use of the sp register as an instruction
+   operand. Register t1 is reserved for this purpose and must be avoided in
+   instruction tests.
+ */
+
+/* Disable clang-format for the test macros because it would mess up the inline
+   assembly. */
+/* clang-format off */
+
+#define ASMINST_2(instruction)                                                 \
+   ".option push;"                                                             \
+   ".option rvc;"                                                              \
+   instruction ";"                                                             \
+   ".option pop"
+
+#define ASMINST_4(instruction)                                                 \
+   ".option push;"                                                             \
+   ".option norvc;"                                                            \
+   instruction ";"                                                             \
+   ".option pop"
+
+#define TESTINST_0_0(length, instruction)                                      \
+   {                                                                           \
+      __asm__ __volatile__(ASMINST_##length(instruction));                     \
+      printf("%s ::\n", instruction);                                          \
+   }
+
+#define TESTINST_1_0(length, instruction, rd)                                  \
+   {                                                                           \
+      unsigned long w[1 /*out*/ + 1 /*spill*/] = {0, 0};                       \
+      /* w[0] = output rd value                                                \
+         w[1] = spill slot for rd                                              \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "sd " #rd ", 8(%[w]);"        /* Spill rd. */                         \
+         ASMINST_##length(instruction) ";"                                     \
+         "sd " #rd ", 0(%[w]);"        /* Save result of the operation. */     \
+         "ld " #rd ", 8(%[w]);"        /* Reload rd. */                        \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "memory");                                                          \
+      printf("%s ::\n", instruction);                                          \
+      printf("  output: %s=0x%016lx\n", #rd, w[0]);                            \
+   }
+
+#define TESTINST_1_1(length, instruction, rs1_val, rd, rs1)                    \
+   {                                                                           \
+      unsigned long w[1 /*out*/ + 1 /*in*/ + 2 /*spill*/] = {                  \
+         0, (unsigned long)rs1_val, 0, 0};                                     \
+      /* w[0] = output rd value                                                \
+         w[1] = input rs1 value                                                \
+         w[2] = spill slot for rd                                              \
+         w[3] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "sd " #rd ", 16(%[w]);"       /* Spill rd. */                         \
+         "sd " #rs1 ", 24(%[w]);"      /* Spill rs1. */                        \
+         "ld " #rs1 ", 8(%[w]);"       /* Load the first input. */             \
+         ASMINST_##length(instruction) ";"                                     \
+         "sd " #rd ", 0(%[w]);"        /* Save result of the operation. */     \
+         "ld " #rd ", 16(%[w]);"       /* Reload rd. */                        \
+         "ld " #rs1 ", 24(%[w]);"      /* Reload rs1. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "memory");                                                          \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx\n", #rs1, (unsigned long)rs1_val);         \
+      printf("  output: %s=0x%016lx\n", #rd, w[0]);                            \
+   }
+
+#define TESTINST_1_2(length, instruction, rs1_val, rs2_val, rd, rs1, rs2)      \
+   {                                                                           \
+      unsigned long w[1 /*out*/ + 2 /*in*/ + 3 /*spill*/] = {                  \
+         0, (unsigned long)rs1_val, (unsigned long)rs2_val, 0, 0, 0};          \
+      /* w[0] = output rd value                                                \
+         w[1] = input rs1 value                                                \
+         w[2] = input rs2 value                                                \
+         w[3] = spill slot for rd                                              \
+         w[4] = spill slot for rs1                                             \
+         w[5] = spill slot for rs2                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "sd " #rd ", 24(%[w]);"       /* Spill rd. */                         \
+         "sd " #rs1 ", 32(%[w]);"      /* Spill rs1. */                        \
+         "sd " #rs2 ", 40(%[w]);"      /* Spill rs2. */                        \
+         "ld " #rs1 ", 8(%[w]);"       /* Load the first input. */             \
+         "ld " #rs2 ", 16(%[w]);"      /* Load the second input. */            \
+         ASMINST_##length(instruction) ";"                                     \
+         "sd " #rd ", 0(%[w]);"        /* Save result of the operation. */     \
+         "ld " #rd ", 24(%[w]);"       /* Reload rd. */                        \
+         "ld " #rs1 ", 32(%[w]);"      /* Reload rs1. */                       \
+         "ld " #rs2 ", 40(%[w]);"      /* Reload rs2. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "memory");                                                          \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, %s=0x%016lx\n", #rs1,                     \
+             (unsigned long)rs1_val, #rs2, (unsigned long)rs2_val);            \
+      printf("  output: %s=0x%016lx\n", #rd, w[0]);                            \
+   }
+
+#define TYPED_LOAD(length, instruction, rd, rs1, ipre)                         \
+   {                                                                           \
+      const size_t   N     = 4096;                                             \
+      unsigned char* area  = memalign16(N);                                    \
+      unsigned char* area2 = memalign16(N);                                    \
+      for (size_t i = 0; i < N; i++)                                           \
+         area[i] = area2[i] = rand_uchar();                                    \
+      unsigned long w[1 /*out*/ + 1 /*in*/ + 2 /*spill*/] = {                  \
+         0, (unsigned long)(area2 + N / 2), 0, 0};                             \
+      /* w[0] = output rd value                                                \
+         w[1] = input rs1 value                                                \
+         w[2] = spill slot for rd                                              \
+         w[3] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         ipre "sd " #rd ", 16(%[w]);"  /* Spill rd. */                         \
+         "sd " #rs1 ", 24(%[w]);"      /* Spill rs1. */                        \
+         "ld " #rs1 ", 8(%[w]);"       /* Load the first input. */             \
+         ASMINST_##length(instruction) ";"                                     \
+         ipre "sd " #rd ", 0(%[w]);"   /* Save result of the operation. */     \
+         ipre "ld " #rd ", 16(%[w]);"  /* Reload rd. */                        \
+         "ld " #rs1 ", 24(%[w]);"      /* Reload rs1. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "memory");                                                          \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=&area_mid\n", #rs1);                                \
+      printf("  output: %s=0x%016lx\n", #rd, w[0]);                            \
+      show_block_diff(area, area2, N, N / 2);                                  \
+      free(area);                                                              \
+      free(area2);                                                             \
+   }
+
+#define TESTINST_1_1_LOAD(length, instruction, rd, rs1)                        \
+   TYPED_LOAD(length, instruction, rd, rs1, "")
+
+#define TESTINST_1_1_FLOAD(length, instruction, rd, rs1)                       \
+   TYPED_LOAD(length, instruction, rd, rs1, "f")
+
+#define TYPED_STORE(length, instruction, rs2_val, rs2, rs1, ipre)              \
+   {                                                                           \
+      const size_t   N     = 4096;                                             \
+      unsigned char* area  = memalign16(N);                                    \
+      unsigned char* area2 = memalign16(N);                                    \
+      for (size_t i = 0; i < N; i++)                                           \
+         area[i] = area2[i] = rand_uchar();                                    \
+      unsigned long w[2 /*in*/ + 2 /*spill*/] = {                              \
+         (unsigned long)rs2_val, (unsigned long)(area2 + N / 2), 0, 0};        \
+      /* w[0] = input rs2 value                                                \
+         w[1] = input rs1 value                                                \
+         w[2] = spill slot for rs2                                             \
+         w[3] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         ipre "sd " #rs2 ", 16(%[w]);" /* Spill rs2. */                        \
+         "sd " #rs1 ", 24(%[w]);"      /* Spill rs1. */                        \
+         ipre "ld " #rs2 ", 0(%[w]);"  /* Load the first input. */             \
+         "ld " #rs1 ", 8(%[w]);"       /* Load the second input. */            \
+         ASMINST_##length(instruction) ";"                                     \
+         ipre "ld " #rs2 ", 16(%[w]);" /* Reload rs2. */                       \
+         "ld " #rs1 ", 24(%[w]);"      /* Reload rs1. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "memory");                                                          \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, %s=&area_mid\n", #rs2,                    \
+             (unsigned long)rs2_val, #rs1);                                    \
+      show_block_diff(area, area2, N, N / 2);                                  \
+      free(area);                                                              \
+      free(area2);                                                             \
+   }
+
+#define TESTINST_0_2_STORE(length, instruction, rs2_val, rs2, rs1)             \
+   TYPED_STORE(length, instruction, rs2_val, rs2, rs1, "")
+
+#define TESTINST_0_2_FSTORE(length, instruction, rs2_val, rs2, rs1)            \
+   TYPED_STORE(length, instruction, rs2_val, rs2, rs1, "f")
+
+#define TESTINST_2_1_LRSC(length, lr_instruction, sc_instruction, lr_rd,       \
+                          sc_rd, rs1)                                          \
+   {                                                                           \
+      const size_t   N     = 32;                                               \
+      unsigned char* area  = memalign16(N);                                    \
+      unsigned char* area2 = memalign16(N);                                    \
+      for (size_t i = 0; i < N; i++)                                           \
+         area2[i] = rand_uchar();                                              \
+      unsigned long w[4 /*out*/ + 1 /*in*/ + 3 /*spill*/];                     \
+      /* w[0] = output lr_rd value                                             \
+         w[1] = modded lr_rd value                                             \
+         w[2] = output sc_rd value, first instruction                          \
+         w[3] = output sc_rd value, second instruction                         \
+         w[4] = address of the area midpoint                                   \
+         w[5] = spill slot for lr_rd                                           \
+         w[6] = spill slot for sc_rd                                           \
+         w[7] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      do {                                                                     \
+         w[0] = w[1] = w[2] = w[3] = w[5] = w[6] = w[7] = 0;                   \
+         w[4] = (unsigned long)(area2 + N / 2);                                \
+         for (size_t i = 0; i < N; i++)                                        \
+            area[i] = area2[i];                                                \
+         __asm__ __volatile__(                                                 \
+            "sd " #lr_rd ", 40(%[w]);" /* Spill lr_rd. */                      \
+            "sd " #sc_rd ", 48(%[w]);" /* Spill sc_rd. */                      \
+            "sd " #rs1 ", 56(%[w]);"   /* Spill rs1. */                        \
+            "ld " #rs1 ", 32(%[w]);"   /* Load the first input. */             \
+            /* Perform a load and create a reservation. */                     \
+            ASMINST_##length(lr_instruction) ";"                               \
+            "mv t2, " #lr_rd ";"       /* Record the loaded value. */          \
+            /* Store a negated value which should succeed. */                  \
+            "not " #lr_rd ", " #lr_rd ";" /* Modify the loaded value. */       \
+            ASMINST_##length(sc_instruction) ";"                               \
+            "sd t2, 0(%[w]);"          /* Save result of the lr operation. */  \
+            "sd " #lr_rd" , 8(%[w]);"  /* Save result of the not operation. */ \
+            "sd " #sc_rd ", 16(%[w]);" /* Save result of the sc operation. */  \
+            /* Store back the original value which should now fail. */         \
+            "mv " #lr_rd ", t2;"       /* Get the original value. */           \
+            ASMINST_##length(sc_instruction) ";"                               \
+            "sd " #sc_rd ", 24(%[w]);" /* Save result of the sc operation. */  \
+            "ld " #lr_rd ", 40(%[w]);" /* Reload lr_rd. */                     \
+            "ld " #sc_rd ", 48(%[w]);" /* Reload sc_rd. */                     \
+            "ld " #rs1 ", 56(%[w]);"   /* Reload rs1. */                       \
+            :                                                                  \
+            : [w] "r"(t1)                                                      \
+            : "t2", "memory");                                                 \
+         /* Re-run the test in case it happens that the first sc instruction   \
+            unexpectedly fails. */                                             \
+      } while (w[2] != 0);                                                     \
+      printf("%s ::\n", lr_instruction);                                       \
+      printf("  inputs: %s=&area_mid\n", #rs1);                                \
+      printf("  output: %s=0x%016lx\n", #lr_rd, w[0]);                         \
+      printf("%s ::\n", sc_instruction);                                       \
+      printf("  inputs: %s=&area_mid, %s=0x%016lx\n", #rs1, #lr_rd, w[1]);     \
+      printf("  output: %s=0x%016lx\n", #sc_rd, w[2]);                         \
+      show_block_diff(area, area2, N, N / 2);                                  \
+      printf("%s ::\n", sc_instruction);                                       \
+      printf("  inputs: %s=&area_mid, %s=0x%016lx\n", #rs1, #lr_rd, w[0]);     \
+      printf("  output: %s=0x%016lx\n", #sc_rd, w[3]);                         \
+      free(area);                                                              \
+      free(area2);                                                             \
+   }
+
+#define TESTINST_1_2_AMOX(length, instruction, rs2_val, rd, rs2, rs1)          \
+   {                                                                           \
+      const size_t   N     = 32;                                               \
+      unsigned char* area  = memalign16(N);                                    \
+      unsigned char* area2 = memalign16(N);                                    \
+      for (size_t i = 0; i < N; i++)                                           \
+         area[i] = area2[i] = rand_uchar();                                    \
+      unsigned long w[1 /*out*/ + 2 /*in*/ + 3 /*spill*/] = {                  \
+         0, (unsigned long)rs2_val, (unsigned long)(area2 + N / 2), 0, 0, 0};  \
+      /* w[0] = output rd value                                                \
+         w[1] = input rs2 value                                                \
+         w[2] = address of the area midpoint                                   \
+         w[3] = spill slot for rd                                              \
+         w[4] = spill slot for rs2                                             \
+         w[5] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "sd " #rd ", 24(%[w]);"       /* Spill rd. */                         \
+         "sd " #rs2 ", 32(%[w]);"      /* Spill rs2. */                        \
+         "sd " #rs1 ", 40(%[w]);"      /* Spill rs1. */                        \
+         "ld " #rs2 ", 8(%[w]);"       /* Load the first input. */             \
+         "ld " #rs1 ", 16(%[w]);"      /* Load the second input. */            \
+         ASMINST_##length(instruction) ";"                                     \
+         "sd " #rd ", 0(%[w]);"        /* Save result of the operation. */     \
+         "ld " #rd ", 24(%[w]);"       /* Reload rd. */                        \
+         "ld " #rs2 ", 32(%[w]);"      /* Reload rs2. */                       \
+         "ld " #rs1 ", 40(%[w]);"      /* Reload rs1. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "memory");                                                          \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, %s=&area_mid\n", #rs2,                    \
+             (unsigned long)rs2_val, #rs1);                                    \
+      printf("  output: %s=0x%016lx\n", #rd, w[0]);                            \
+      show_block_diff(area, area2, N, N / 2);                                  \
+      free(area);                                                              \
+      free(area2);                                                             \
+   }
+
+#define DEST_FMT_zero              "0x%016lx"
+#define DEST_DIFF_zero(dest, base) (dest)
+
+#define DEST_FMT_norm              "1f%+ld"
+#define DEST_DIFF_norm(dest, base) ((long)(dest - base))
+#define DEST_FMT_ra                DEST_FMT_norm
+#define DEST_DIFF_ra(dest, base)   DEST_DIFF_norm(dest, base)
+#define DEST_FMT_a0                DEST_FMT_norm
+#define DEST_DIFF_a0(dest, base)   DEST_DIFF_norm(dest, base)
+#define DEST_FMT_t0                DEST_FMT_norm
+#define DEST_DIFF_t0(dest, base)   DEST_DIFF_norm(dest, base)
+#define DEST_FMT_t6                DEST_FMT_norm
+#define DEST_DIFF_t6(dest, base)   DEST_DIFF_norm(dest, base)
+
+#define DEST_FMT_unused              "%s"
+#define DEST_DIFF_unused(dest, base) "unused"
+
+#define TESTINST_1_0_AUIPC(length, instruction, rd)                            \
+   {                                                                           \
+      unsigned long w[2 /*out*/ + 1 /*spill*/] = {0, 0, 0};                    \
+      /* w[0] = output rd value                                                \
+         w[1] = address of the test instruction                                \
+         w[2] = spill slot for rd                                              \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "sd " #rd ", 16(%[w]);"       /* Spill rd. */                         \
+         "1:;"                                                                 \
+         ASMINST_##length(instruction) ";"                                     \
+         "sd " #rd ", 0(%[w]);"        /* Save result of the operation. */     \
+         "la t2, 1b;"                                                          \
+         "sd t2, 8(%[w]);"             /* Store address of the test instr. */  \
+         "ld " #rd ", 16(%[w]);"       /* Reload rd. */                        \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      printf("  output: %s=" DEST_FMT_##rd "\n", #rd,                          \
+             DEST_DIFF_##rd(w[0], w[1]));                                      \
+   }
+
+#define JMP_RANGE(length, instruction, rs1_val, rs2_val, offset, rd, rs1, rs2) \
+   {                                                                           \
+      unsigned long w[5 /*out*/ + 3 /*spill*/] = {0, 0, 0, 0, 0, 0, 0, 0};     \
+      /* w[0] = output rd value                                                \
+         w[1] = address of the test instruction                                \
+         w[2] = flag that rd is valid                                          \
+         w[3] = flag that rs1 is valid                                         \
+         w[4] = flag that rs2 is valid                                         \
+         w[5] = spill slot for rd                                              \
+         w[6] = spill slot for rs1                                             \
+         w[7] = spill slot for rs2                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         ".if \"" #rd "\" != \"unused\";"                                      \
+         "sd " #rd ", 40(%[w]);"       /* Spill rd. */                         \
+         ".endif;"                                                             \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "sd " #rs1 ", 48(%[w]);"      /* Spill rs1. */                        \
+         "la " #rs1 ", " rs1_val ";"   /* Load the first input. */             \
+         ".endif;"                                                             \
+         ".if \"" #rs2 "\" != \"unused\";"                                     \
+         "sd " #rs2 ", 56(%[w]);"      /* Spill rs2. */                        \
+         "la " #rs2 ", " rs2_val ";"   /* Load the second input. */            \
+         ".endif;"                                                             \
+         "j 1f;"                                                               \
+         ".option push;"                                                       \
+         ".option norvc;"                                                      \
+         /* Generate a target area for negative offset. */                     \
+         ".if " #offset " < 0;"                                                \
+         ".if 4096 + " #offset " > 0; .space 4096 + " #offset "; .endif;"      \
+         "j 2f;"                                                               \
+         ".if -" #offset " - 4 > 0; .space -" #offset " - 4; .endif;"          \
+         ".else;"                                                              \
+         ".space 4096;"                                                        \
+         ".endif;"                                                             \
+         "1:;"                                                                 \
+         ASMINST_##length(instruction) ";"                                     \
+         /* Generate a target area for positive offset. */                     \
+         ".if " #length " == 2; .space 2; .endif;"                             \
+         ".if " #offset " > 0;"                                                \
+         ".if " #offset " - 4 > 0; .space " #offset " - 4; .endif;"            \
+         "j 2f;"                                                               \
+         ".if 4094 - " #offset " > 0; .space 4094 - " #offset "; .endif;"      \
+         ".else;"                                                              \
+         ".space 4094;"                                                        \
+         ".endif;"                                                             \
+         "2:;"                                                                 \
+         ".option pop;"                                                        \
+         ".if \"" #rd "\" != \"unused\";"                                      \
+         "sd " #rd ", 0(%[w]);"        /* Store the output return address. */  \
+         "la t2, 1b;"                                                          \
+         "sd t2, 8(%[w]);"             /* Store address of the test instr. */  \
+         "li t2, 1;"                                                           \
+         "sd t2, 16(%[w]);"            /* Flag that rd is valid. */            \
+         ".endif;"                                                             \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "li t2, 1;"                                                           \
+         "sd t2, 24(%[w]);"            /* Flag that rs1 is valid. */           \
+         ".endif;"                                                             \
+         ".if \"" #rs2 "\" != \"unused\";"                                     \
+         "li t2, 1;"                                                           \
+         "sd t2, 32(%[w]);"            /* Flag that rs2 is valid. */           \
+         ".endif;"                                                             \
+         ".if \"" #rd "\" != \"unused\";"                                      \
+         "ld " #rd ", 40(%[w]);"       /* Reload rd. */                        \
+         ".endif;"                                                             \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "ld " #rs1 ", 48(%[w]);"      /* Reload rs1. */                       \
+         ".endif;"                                                             \
+         ".if \"" #rs2 "\" != \"unused\";"                                     \
+         "ld " #rs2 ", 56(%[w]);"      /* Reload rs2. */                       \
+         ".endif;"                                                             \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      if (w[3] != 0) { /* If rs1 is valid. */                                  \
+         printf("  inputs: %s=%s", #rs1, rs1_val);                             \
+         if (w[4] != 0) /* If rs2 is valid. */                                 \
+            printf(", %s=%s", #rs2, rs2_val);                                  \
+         printf("\n");                                                         \
+      }                                                                        \
+      if (w[2] != 0) /* If rd is valid. */                                     \
+         printf("  output: %s=" DEST_FMT_##rd "\n", #rd,                       \
+                DEST_DIFF_##rd(w[0], w[1]));                                   \
+      printf("  target: reached\n");                                           \
+   }
+
+#define TESTINST_0_0_J_RANGE(length, instruction, offset)                      \
+   JMP_RANGE(length, instruction, "0", "0", offset, unused, unused, unused)
+
+#define TESTINST_0_1_JR_RANGE(length, instruction, rs1_val, offset, rs1)       \
+   JMP_RANGE(length, instruction, rs1_val, "0", offset, unused, rs1, unused)
+
+#define TESTINST_1_0_JAL_RANGE(length, instruction, offset, rd)                \
+   JMP_RANGE(length, instruction, "0", "0", offset, rd, unused, unused)
+
+#define TESTINST_1_1_JALR_RANGE(length, instruction, rs1_val, offset, rd, rs1) \
+   JMP_RANGE(length, instruction, rs1_val, "0", offset, rd, rs1, unused)
+
+#define TESTINST_0_1_BxxZ_RANGE(length, instruction, rs1_val, offset, rs1)     \
+   JMP_RANGE(length, instruction, #rs1_val, "0", offset, unused, rs1, unused)
+
+#define TESTINST_0_2_Bxx_RANGE(length, instruction, rs1_val, rs2_val, offset,  \
+                               rs1, rs2)                                       \
+   JMP_RANGE(length, instruction, #rs1_val, #rs2_val, offset, unused, rs1, rs2)
+
+#define JMP_COND(length, instruction, rs1_val, rs2_val, rs1, rs2)              \
+   {                                                                           \
+      unsigned long w[3 /*out*/ + 2 /*spill*/] = {0, 0, 0, 0, 0};              \
+      /* w[0] = flag that the branch was taken                                 \
+         w[1] = flag that rs1 is valid                                         \
+         w[2] = flag that rs2 is valid                                         \
+         w[3] = spill slot for rs1                                             \
+         w[4] = spill slot for rs2                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "li t2, 1;"                                                           \
+         "sd t2, 0(%[w]);"             /* Set result to "taken". */            \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "sd " #rs1 ", 24(%[w]);"      /* Spill rs1. */                        \
+         "la " #rs1 ", " rs1_val ";"   /* Load the first input. */             \
+         ".endif;"                                                             \
+         ".if \"" #rs2 "\" != \"unused\";"                                     \
+         "sd " #rs2 ", 32(%[w]);"      /* Spill rs2. */                        \
+         "la " #rs2 ", " rs2_val ";"   /* Load the second input. */            \
+         ".endif;"                                                             \
+         ASMINST_##length(instruction) ";"                                     \
+         "li t2, 0;"                                                           \
+         "sd t2, 0(%[w]);"             /* Set result to "not taken". */        \
+         "1:;"                                                                 \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "li t2, 1;"                                                           \
+         "sd t2, 8(%[w]);"             /* Flag that rs1 is valid. */           \
+         ".endif;"                                                             \
+         ".if \"" #rs2 "\" != \"unused\";"                                     \
+         "li t2, 1;"                                                           \
+         "sd t2, 16(%[w]);"            /* Flag that rs2 is valid. */           \
+         ".endif;"                                                             \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "ld " #rs1 ", 24(%[w]);"      /* Reload rs1. */                       \
+         ".endif;"                                                             \
+         ".if \"" #rs2 "\" != \"unused\";"                                     \
+         "ld " #rs2 ", 32(%[w]);"      /* Reload rs2. */                       \
+         ".endif;"                                                             \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      if (w[1] != 0) { /* If rs1 is valid. */                                  \
+         printf("  inputs: %s=%s", #rs1, rs1_val);                             \
+         if (w[2] != 0) /* If rs2 is valid. */                                 \
+            printf(", %s=%s", #rs2, rs2_val);                                  \
+         printf("\n");                                                         \
+      }                                                                        \
+      printf("  branch: %s\n", w[0] ? "taken" : "not taken");                  \
+   }
+
+#define TESTINST_0_1_BxxZ_COND(length, instruction, rs1_val, rs1)              \
+   JMP_COND(length, instruction, #rs1_val, "0", rs1, unused)
+
+#define TESTINST_0_2_Bxx_COND(length, instruction, rs1_val, rs2_val, rs1, rs2) \
+   JMP_COND(length, instruction, #rs1_val, #rs2_val, rs1, rs2)
+
+#define TYPED_X_X(length, instruction, rs1_val, fcsr_val, rd, rs1, dpre, spre) \
+   {                                                                           \
+      unsigned long w[2 /*out*/ + 2 /*in*/ + 3 /*spill*/] = {                  \
+         0, 0, (unsigned long)rs1_val, (unsigned long)fcsr_val, 0, 0, 0};      \
+      /* w[0] = output rd value                                                \
+         w[1] = output fcsr value                                              \
+         w[2] = input rs1 value                                                \
+         w[3] = input fcsr value                                               \
+         w[4] = spill slot for rd                                              \
+         w[5] = spill slot for fcsr                                            \
+         w[6] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         dpre "sd " #rd ", 32(%[w]);"  /* Spill rd. */                         \
+         "frcsr t2;"                                                           \
+         "sd t2, 40(%[w]);"            /* Spill fcsr. */                       \
+         spre "sd " #rs1 ", 48(%[w]);" /* Spill rs1. */                        \
+         "ld t2, 24(%[w]);"                                                    \
+         "fscsr t2;"                   /* Load fcsr. */                        \
+         spre "ld " #rs1 ", 16(%[w]);" /* Load the first input. */             \
+         ASMINST_##length(instruction) ";"                                     \
+         dpre "sd " #rd ", 0(%[w]);"   /* Save result of the operation. */     \
+         "frcsr t2;"                                                           \
+         "sd t2, 8(%[w]);"             /* Save fcsr. */                        \
+         "ld t2, 40(%[w]);"                                                    \
+         "fscsr t2;"                   /* Reload fcsr. */                      \
+         dpre "ld " #rd ", 32(%[w]);"  /* Reload rd. */                        \
+         spre "ld " #rs1 ", 48(%[w]);" /* Reload rs1. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, fcsr=0x%08lx\n", #rs1,                    \
+             (unsigned long)rs1_val, (unsigned long)fcsr_val);                 \
+      printf("  output: %s=0x%016lx, fcsr=0x%08lx\n", #rd, w[0], w[1]);        \
+   }
+
+#define TESTINST_1_1_F(length, instruction, rs1_val, fcsr_val, rd, rs1)        \
+    TYPED_X_X(length, instruction, rs1_val, fcsr_val, rd, rs1, "f", "f")
+
+#define TESTINST_1_1_IF(length, instruction, rs1_val, fcsr_val, rd, rs1)       \
+    TYPED_X_X(length, instruction, rs1_val, fcsr_val, rd, rs1, "", "f")
+
+#define TESTINST_1_1_FI(length, instruction, rs1_val, fcsr_val, rd, rs1)       \
+    TYPED_X_X(length, instruction, rs1_val, fcsr_val, rd, rs1, "f", "")
+
+#define TYPED_X_FF(length, instruction, rs1_val, rs2_val, fcsr_val, rd, rs1,   \
+                   rs2, dpre)                                                  \
+   {                                                                           \
+      unsigned long w[2 /*out*/ + 3 /*in*/ + 4 /*spill*/] = {                  \
+         0, 0, (unsigned long)rs1_val, (unsigned long)rs2_val,                 \
+         (unsigned long)fcsr_val, 0, 0, 0, 0};                                 \
+      /* w[0] = output rd value                                                \
+         w[1] = output fcsr value                                              \
+         w[2] = input rs1 value                                                \
+         w[3] = input rs2 value                                                \
+         w[4] = input fcsr value                                               \
+         w[5] = spill slot for rd                                              \
+         w[6] = spill slot for fcsr                                            \
+         w[7] = spill slot for rs1                                             \
+         w[8] = spill slot for rs2                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         dpre "sd " #rd ", 40(%[w]);"  /* Spill rd. */                         \
+         "frcsr t2;"                                                           \
+         "sd t2, 48(%[w]);"            /* Spill fcsr. */                       \
+         "fsd " #rs1 ", 56(%[w]);"     /* Spill rs1. */                        \
+         "fsd " #rs2 ", 64(%[w]);"     /* Spill rs2. */                        \
+         "ld t2, 32(%[w]);"                                                    \
+         "fscsr t2;"                   /* Load fcsr. */                        \
+         "fld " #rs1 ", 16(%[w]);"     /* Load the first input. */             \
+         "fld " #rs2 ", 24(%[w]);"     /* Load the second input. */            \
+         ASMINST_##length(instruction) ";"                                     \
+         dpre "sd " #rd ", 0(%[w]);"   /* Save result of the operation. */     \
+         "frcsr t2;"                                                           \
+         "sd t2, 8(%[w]);"             /* Save fcsr. */                        \
+         "ld t2, 48(%[w]);"                                                    \
+         "fscsr t2;"                   /* Reload fcsr. */                      \
+         dpre "ld " #rd ", 40(%[w]);"  /* Reload rd. */                        \
+         "fld " #rs1 ", 56(%[w]);"     /* Reload rs1. */                       \
+         "fld " #rs2 ", 64(%[w]);"     /* Reload rs2. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, %s=0x%016lx, fcsr=0x%08lx\n", #rs1,       \
+             (unsigned long)rs1_val, #rs2, (unsigned long)rs2_val,             \
+             (unsigned long)fcsr_val);                                         \
+      printf("  output: %s=0x%016lx, fcsr=0x%08lx\n", #rd, w[0], w[1]);        \
+   }
+
+#define TESTINST_1_2_F(length, instruction, rs1_val, rs2_val, fcsr_val, rd,    \
+                       rs1, rs2)                                               \
+    TYPED_X_FF(length, instruction, rs1_val, rs2_val, fcsr_val, rd, rs1, rs2,  \
+               "f")
+
+#define TESTINST_1_2_FCMP(length, instruction, rs1_val, rs2_val, fcsr_val, rd, \
+                          rs1, rs2)                                            \
+    TYPED_X_FF(length, instruction, rs1_val, rs2_val, fcsr_val, rd, rs1, rs2,  \
+               "")
+
+#define TESTINST_1_3_F(length, instruction, rs1_val, rs2_val, rs3_val,         \
+                       fcsr_val, rd, rs1, rs2, rs3)                            \
+   {                                                                           \
+      unsigned long w[2 /*out*/ + 4 /*in*/ + 5 /*spill*/] = {                  \
+         0, 0, (unsigned long)rs1_val, (unsigned long)rs2_val,                 \
+         (unsigned long)rs3_val, (unsigned long)fcsr_val, 0, 0, 0, 0, 0};      \
+      /* w[0] = output rd value                                                \
+         w[1] = output fcsr value                                              \
+         w[2] = input rs1 value                                                \
+         w[3] = input rs2 value                                                \
+         w[4] = input rs3 value                                                \
+         w[5] = input fcsr value                                               \
+         w[6] = spill slot for rd                                              \
+         w[7] = spill slot for fcsr                                            \
+         w[8] = spill slot for rs1                                             \
+         w[9] = spill slot for rs2                                             \
+         w[10] = spill slot for rs3                                            \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         "fsd " #rd ", 48(%[w]);"      /* Spill rd. */                         \
+         "frcsr t2;"                                                           \
+         "sd t2, 56(%[w]);"            /* Spill fcsr. */                       \
+         "fsd " #rs1 ", 64(%[w]);"     /* Spill rs1. */                        \
+         "fsd " #rs2 ", 72(%[w]);"     /* Spill rs2. */                        \
+         "fsd " #rs3 ", 80(%[w]);"     /* Spill rs3. */                        \
+         "ld t2, 40(%[w]);"                                                    \
+         "fscsr t2;"                   /* Load fcsr. */                        \
+         "fld " #rs1 ", 16(%[w]);"     /* Load the first input. */             \
+         "fld " #rs2 ", 24(%[w]);"     /* Load the second input. */            \
+         "fld " #rs3 ", 32(%[w]);"     /* Load the third input. */             \
+         ASMINST_##length(instruction) ";"                                     \
+         "fsd " #rd ", 0(%[w]);"       /* Save result of the operation. */     \
+         "frcsr t2;"                                                           \
+         "sd t2, 8(%[w]);"             /* Save fcsr. */                        \
+         "ld t2, 56(%[w]);"                                                    \
+         "fscsr t2;"                   /* Reload fcsr. */                      \
+         "fld " #rd ", 48(%[w]);"      /* Reload rd. */                        \
+         "fld " #rs1 ", 64(%[w]);"     /* Reload rs1. */                       \
+         "fld " #rs2 ", 72(%[w]);"     /* Reload rs2. */                       \
+         "fld " #rs3 ", 80(%[w]);"     /* Reload rs2. */                       \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, %s=0x%016lx, %s=0x%016lx, "               \
+             "fcsr=0x%08lx\n", #rs1, (unsigned long)rs1_val, #rs2,             \
+             (unsigned long)rs2_val, #rs3, (unsigned long)rs3_val,             \
+             (unsigned long)fcsr_val);                                         \
+      printf("  output: %s=0x%016lx, fcsr=0x%08lx\n", #rd, w[0], w[1]);        \
+   }
+
+#define TESTINST_1_1_CSR(length, instruction, csr_val, rs1_val, rd, csr, rs1)  \
+   {                                                                           \
+      unsigned long w[2 /*out*/ + 2 /*in*/ + 3 /*spill*/] = {                  \
+         0, 0, (unsigned long)csr_val, (unsigned long)rs1_val, 0, 0, 0};       \
+      /* w[0] = output rd value                                                \
+         w[1] = output csr value                                               \
+         w[2] = input csr value                                                \
+         w[3] = input rs1 value                                                \
+         w[4] = spill slot for rd                                              \
+         w[5] = spill slot for csr                                             \
+         w[6] = spill slot for rs1                                             \
+       */                                                                      \
+      register unsigned long* t1 asm("t1") = w;                                \
+      __asm__ __volatile__(                                                    \
+         ".if \"" #rd "\" != \"unused\";"                                      \
+         "sd " #rd ", 32(%[w]);"       /* Spill rd. */                         \
+         ".endif;"                                                             \
+         "csrr t2, " #csr ";"                                                  \
+         "sd t2, 40(%[w]);"            /* Spill csr. */                        \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "sd " #rs1 ", 48(%[w]);"      /* Spill rs1. */                        \
+         ".endif;"                                                             \
+         "ld t2, 16(%[w]);"                                                    \
+         "csrw " #csr ", t2;"          /* Load csr. */                         \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "ld " #rs1 ", 24(%[w]);"      /* Load the first input. */             \
+         ".endif;"                                                             \
+         ASMINST_##length(instruction) ";"                                     \
+         ".if \"" #rd "\" != \"unused\";"                                      \
+         "sd " #rd ", 0(%[w]);"        /* Save result of the operation. */     \
+         ".endif;"                                                             \
+         "csrr t2, " #csr ";"                                                  \
+         "sd t2, 8(%[w]);"             /* Save csr. */                         \
+         "ld t2, 40(%[w]);"                                                    \
+         "csrw " #csr ", t2;"          /* Reload csr. */                       \
+         ".if \"" #rd "\" != \"unused\";"                                      \
+         "ld " #rd ", 32(%[w]);"       /* Reload rd. */                        \
+         ".endif;"                                                             \
+         ".if \"" #rs1 "\" != \"unused\";"                                     \
+         "ld " #rs1 ", 48(%[w]);"      /* Reload rs1. */                       \
+         ".endif;"                                                             \
+         :                                                                     \
+         : [w] "r"(t1)                                                         \
+         : "t2", "memory");                                                    \
+      printf("%s ::\n", instruction);                                          \
+      printf("  inputs: %s=0x%016lx, %s=0x%016lx\n", #rs1,                     \
+             (unsigned long)rs1_val, #csr, (unsigned long)csr_val);            \
+      printf("  output: %s=0x%016lx, %s=0x%016lx\n", #rd, w[0], #csr, w[1]);   \
+   }
+
+/* clang-format on */
--- a/tests/arch_test.c
+++ b/tests/arch_test.c
@@ -34,6 +34,7 @@
    "mips32",
    "mips64",
    "nanomips",
+   "riscv64",
    NULL
 };
 
@@ -79,6 +80,10 @@
 
 #elif defined(VGP_nanomips_linux)
    if ( 0 == strcmp( arch, "nanomips" ) ) return True;
+
+#elif defined(VGP_riscv64_linux)
+   if ( 0 == strcmp( arch, "riscv64" ) ) return True;
+
 #else
 #  error Unknown platform
 #endif   // VGP_*
--- /dev/null
+++ b/include/vki/vki-riscv64-linux.h
@@ -0,0 +1,633 @@
+
+/*--------------------------------------------------------------------*/
+/*--- riscv64/Linux-specific kernel interface. vki-riscv64-linux.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2020-2022 Petr Pavlu
+      petr.pavlu@dagobah.cz
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, see <http://www.gnu.org/licenses/>.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef __VKI_RISCV64_LINUX_H
+#define __VKI_RISCV64_LINUX_H
+
+// riscv64 is little-endian.
+#define VKI_LITTLE_ENDIAN  1
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/int-ll64.h
+//----------------------------------------------------------------------
+
+typedef unsigned char __vki_u8;
+
+typedef __signed__ short __vki_s16;
+typedef unsigned short __vki_u16;
+
+typedef __signed__ int __vki_s32;
+typedef unsigned int __vki_u32;
+
+typedef __signed__ long long __vki_s64;
+typedef unsigned long long __vki_u64;
+
+typedef unsigned short vki_u16;
+
+typedef unsigned int vki_u32;
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/arch/riscv/include/asm/page.h
+//----------------------------------------------------------------------
+
+#define VKI_PAGE_SHIFT	(12)
+#define VKI_PAGE_SIZE	(1UL << VKI_PAGE_SHIFT)
+#define VKI_MAX_PAGE_SHIFT	VKI_PAGE_SHIFT
+#define VKI_MAX_PAGE_SIZE	VKI_PAGE_SIZE
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/asm-generic/shmparam.h
+//----------------------------------------------------------------------
+
+#define VKI_SHMLBA VKI_PAGE_SIZE	/* attach addr a multiple of this */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/signal-defs.h
+//----------------------------------------------------------------------
+
+#define VKI_SIG_BLOCK          0	/* for blocking signals */
+#define VKI_SIG_UNBLOCK        1	/* for unblocking signals */
+#define VKI_SIG_SETMASK        2	/* for setting the signal mask */
+
+typedef void __vki_signalfn_t(int);
+typedef __vki_signalfn_t __user *__vki_sighandler_t;
+
+#define VKI_SIG_DFL	((__vki_sighandler_t)0)	/* default signal handling */
+#define VKI_SIG_IGN	((__vki_sighandler_t)1)	/* ignore signal */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/signal.h
+//----------------------------------------------------------------------
+
+#define _VKI_NSIG	64
+#define _VKI_NSIG_BPW	64
+#define _VKI_NSIG_WORDS	(_VKI_NSIG / _VKI_NSIG_BPW)
+
+typedef struct {
+	unsigned long sig[_VKI_NSIG_WORDS];
+} vki_sigset_t;
+
+#define VKI_SIGHUP		 1
+#define VKI_SIGINT		 2
+#define VKI_SIGQUIT		 3
+#define VKI_SIGILL		 4
+#define VKI_SIGTRAP		 5
+#define VKI_SIGABRT		 6
+#define VKI_SIGBUS		 7
+#define VKI_SIGFPE		 8
+#define VKI_SIGKILL		 9
+#define VKI_SIGUSR1		10
+#define VKI_SIGSEGV		11
+#define VKI_SIGUSR2		12
+#define VKI_SIGPIPE		13
+#define VKI_SIGALRM		14
+#define VKI_SIGTERM		15
+#define VKI_SIGSTKFLT		16
+#define VKI_SIGCHLD		17
+#define VKI_SIGCONT		18
+#define VKI_SIGSTOP		19
+#define VKI_SIGTSTP		20
+#define VKI_SIGTTIN		21
+#define VKI_SIGTTOU		22
+#define VKI_SIGURG		23
+#define VKI_SIGXCPU		24
+#define VKI_SIGXFSZ		25
+#define VKI_SIGVTALRM		26
+#define VKI_SIGPROF		27
+#define VKI_SIGWINCH		28
+#define VKI_SIGIO		29
+#define VKI_SIGPWR		30
+#define VKI_SIGSYS		31
+#define	VKI_SIGUNUSED		31
+
+#define VKI_SIGRTMIN		32
+#define VKI_SIGRTMAX		_VKI_NSIG
+
+#define VKI_SA_NOCLDSTOP	0x00000001
+#define VKI_SA_NOCLDWAIT	0x00000002
+#define VKI_SA_SIGINFO		0x00000004
+#define VKI_SA_ONSTACK		0x08000000
+#define VKI_SA_RESTART		0x10000000
+#define VKI_SA_NODEFER		0x40000000
+#define VKI_SA_RESETHAND	0x80000000
+
+#define VKI_SA_NOMASK	VKI_SA_NODEFER
+#define VKI_SA_ONESHOT	VKI_SA_RESETHAND
+
+#define VKI_MINSIGSTKSZ	2048
+
+struct vki_sigaction_base {
+	__vki_sighandler_t ksa_handler;
+	unsigned long sa_flags;
+	vki_sigset_t sa_mask;
+};
+
+/* On Linux we use the same type for passing sigactions to
+   and from the kernel.  Hence: */
+typedef  struct vki_sigaction_base  vki_sigaction_toK_t;
+typedef  struct vki_sigaction_base  vki_sigaction_fromK_t;
+
+typedef struct vki_sigaltstack {
+	void __user *ss_sp;
+	int ss_flags;
+	vki_size_t ss_size;
+} vki_stack_t;
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/linux/signal.h
+//----------------------------------------------------------------------
+
+#define VKI_SS_ONSTACK	1
+#define VKI_SS_DISABLE	2
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/mman-common.h
+//----------------------------------------------------------------------
+
+#define VKI_PROT_READ	0x1		/* page can be read */
+#define VKI_PROT_WRITE	0x2		/* page can be written */
+#define VKI_PROT_EXEC	0x4		/* page can be executed */
+#define VKI_PROT_NONE	0x0		/* page can not be accessed */
+#define VKI_PROT_GROWSDOWN	0x01000000	/* mprotect flag: extend change to start of growsdown vma */
+#define VKI_PROT_GROWSUP	0x02000000	/* mprotect flag: extend change to end of growsup vma */
+
+#define VKI_MAP_FIXED	0x10		/* Interpret addr exactly */
+#define VKI_MAP_ANONYMOUS	0x20	/* don't use a file */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/mman.h
+//----------------------------------------------------------------------
+
+#define VKI_MAP_NORESERVE       0x4000  /* don't check for reservations */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/linux/mman.h
+//----------------------------------------------------------------------
+
+#define VKI_MAP_SHARED	0x01		/* Share changes */
+#define VKI_MAP_PRIVATE	0x02		/* Changes are private */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/fcntl.h
+//----------------------------------------------------------------------
+
+#define VKI_O_ACCMODE	     03
+#define VKI_O_RDONLY	     00
+#define VKI_O_WRONLY	     01
+#define VKI_O_RDWR	     02
+#define VKI_O_CREAT	   0100	/* not fcntl */
+#define VKI_O_EXCL	   0200	/* not fcntl */
+#define VKI_O_TRUNC	  01000	/* not fcntl */
+#define VKI_O_APPEND	  02000
+#define VKI_O_NONBLOCK	  04000
+#define VKI_O_LARGEFILE	0100000
+
+#define VKI_F_DUPFD		0	/* dup */
+#define VKI_F_GETFD		1	/* get close_on_exec */
+#define VKI_F_SETFD		2	/* set/clear close_on_exec */
+#define VKI_F_GETFL		3	/* get file->f_flags */
+#define VKI_F_SETFL		4	/* set file->f_flags */
+#define VKI_F_GETLK		5
+#define VKI_F_SETLK		6
+#define VKI_F_SETLKW		7
+
+#define VKI_F_SETOWN		8	/*  for sockets. */
+#define VKI_F_GETOWN		9	/*  for sockets. */
+#define VKI_F_SETSIG		10	/*  for sockets. */
+#define VKI_F_GETSIG		11	/*  for sockets. */
+
+#define VKI_F_SETOWN_EX		15
+#define VKI_F_GETOWN_EX		16
+
+#define VKI_F_OFD_GETLK		36
+#define VKI_F_OFD_SETLK		37
+#define VKI_F_OFD_SETLKW	38
+
+struct vki_f_owner_ex {
+	int	type;
+	__vki_kernel_pid_t	pid;
+};
+
+#define VKI_FD_CLOEXEC	1	/* actually anything with low bit set goes */
+
+#define VKI_F_LINUX_SPECIFIC_BASE	1024
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/linux/fcntl.h
+//----------------------------------------------------------------------
+
+#define VKI_AT_FDCWD		-100
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/resource.h
+//----------------------------------------------------------------------
+
+#define VKI_RLIMIT_DATA		2	/* max data size */
+#define VKI_RLIMIT_STACK	3	/* max stack size */
+#define VKI_RLIMIT_CORE		4	/* max core file size */
+#define VKI_RLIMIT_NOFILE	7	/* max number of open files */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/socket.h
+//----------------------------------------------------------------------
+
+#define VKI_SOL_SOCKET	1
+
+#define VKI_SO_TYPE	3
+
+#define VKI_SO_ATTACH_FILTER	26
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/sockios.h
+//----------------------------------------------------------------------
+
+#define VKI_SIOCSPGRP		0x8902
+#define VKI_SIOCGPGRP		0x8904
+#define VKI_SIOCATMARK		0x8905
+#define VKI_SIOCGSTAMP		0x8906		/* Get stamp (timeval) */
+#define VKI_SIOCGSTAMPNS	0x8907		/* Get stamp (timespec) */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/stat.h
+//----------------------------------------------------------------------
+
+struct vki_stat {
+	unsigned long	st_dev;		/* Device.  */
+	unsigned long	st_ino;		/* File serial number.  */
+	unsigned int	st_mode;	/* File mode.  */
+	unsigned int	st_nlink;	/* Link count.  */
+	unsigned int	st_uid;		/* User ID of the file's owner.  */
+	unsigned int	st_gid;		/* Group ID of the file's group. */
+	unsigned long	st_rdev;	/* Device number, if device.  */
+	unsigned long	__pad1;
+	long		st_size;	/* Size of file, in bytes.  */
+	int		st_blksize;	/* Optimal block size for I/O.  */
+	int		__pad2;
+	long		st_blocks;	/* Number 512-byte blocks allocated. */
+	long		st_atime;	/* Time of last access.  */
+	unsigned long	st_atime_nsec;
+	long		st_mtime;	/* Time of last modification.  */
+	unsigned long	st_mtime_nsec;
+	long		st_ctime;	/* Time of last status change.  */
+	unsigned long	st_ctime_nsec;
+	unsigned int	__unused4;
+	unsigned int	__unused5;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/statfs.h
+//----------------------------------------------------------------------
+
+struct vki_statfs {
+	long f_type;
+	long f_bsize;
+	long f_blocks;
+	long f_bfree;
+	long f_bavail;
+	long f_files;
+	long f_ffree;
+	__vki_kernel_fsid_t f_fsid;
+	long f_namelen;
+	long f_frsize;
+	long f_flags;
+	long f_spare[4];
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/termios.h
+//----------------------------------------------------------------------
+
+struct vki_winsize {
+	unsigned short ws_row;
+	unsigned short ws_col;
+	unsigned short ws_xpixel;
+	unsigned short ws_ypixel;
+};
+
+#define VKI_NCC 8
+struct vki_termio {
+	unsigned short c_iflag;		/* input mode flags */
+	unsigned short c_oflag;		/* output mode flags */
+	unsigned short c_cflag;		/* control mode flags */
+	unsigned short c_lflag;		/* local mode flags */
+	unsigned char c_line;		/* line discipline */
+	unsigned char c_cc[VKI_NCC];	/* control characters */
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/termbits.h
+//----------------------------------------------------------------------
+
+typedef unsigned char	vki_cc_t;
+typedef unsigned int	vki_tcflag_t;
+
+#define VKI_NCCS 19
+struct vki_termios {
+	vki_tcflag_t c_iflag;		/* input mode flags */
+	vki_tcflag_t c_oflag;		/* output mode flags */
+	vki_tcflag_t c_cflag;		/* control mode flags */
+	vki_tcflag_t c_lflag;		/* local mode flags */
+	vki_cc_t c_line;		/* line discipline */
+	vki_cc_t c_cc[VKI_NCCS];	/* control characters */
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/ioctl.h
+//----------------------------------------------------------------------
+
+#define _VKI_IOC_NRBITS		8
+#define _VKI_IOC_TYPEBITS	8
+#define _VKI_IOC_SIZEBITS	14
+#define _VKI_IOC_DIRBITS	2
+
+#define _VKI_IOC_SIZEMASK	((1 << _VKI_IOC_SIZEBITS)-1)
+#define _VKI_IOC_DIRMASK	((1 << _VKI_IOC_DIRBITS)-1)
+
+#define _VKI_IOC_NRSHIFT	0
+#define _VKI_IOC_TYPESHIFT	(_VKI_IOC_NRSHIFT+_VKI_IOC_NRBITS)
+#define _VKI_IOC_SIZESHIFT	(_VKI_IOC_TYPESHIFT+_VKI_IOC_TYPEBITS)
+#define _VKI_IOC_DIRSHIFT	(_VKI_IOC_SIZESHIFT+_VKI_IOC_SIZEBITS)
+
+#define _VKI_IOC_NONE	0U
+#define _VKI_IOC_WRITE	1U
+#define _VKI_IOC_READ	2U
+
+#define _VKI_IOC(dir,type,nr,size) \
+	(((dir)  << _VKI_IOC_DIRSHIFT) | \
+	 ((type) << _VKI_IOC_TYPESHIFT) | \
+	 ((nr)   << _VKI_IOC_NRSHIFT) | \
+	 ((size) << _VKI_IOC_SIZESHIFT))
+
+#define _VKI_IO(type,nr)	_VKI_IOC(_VKI_IOC_NONE,(type),(nr),0)
+#define _VKI_IOR(type,nr,size)	_VKI_IOC(_VKI_IOC_READ,(type),(nr),sizeof(size))
+#define _VKI_IOW(type,nr,size)	_VKI_IOC(_VKI_IOC_WRITE,(type),(nr),sizeof(size))
+#define _VKI_IOWR(type,nr,size)	_VKI_IOC(_VKI_IOC_READ|_VKI_IOC_WRITE,(type),(nr),sizeof(size))
+
+#define _VKI_IOC_DIR(nr)		(((nr) >> _VKI_IOC_DIRSHIFT) & _VKI_IOC_DIRMASK)
+#define _VKI_IOC_SIZE(nr)		(((nr) >> _VKI_IOC_SIZESHIFT) & _VKI_IOC_SIZEMASK)
+
+//----------------------------------------------------------------------
+// From linux-3.10.5/include/uapi/asm-generic/ioctls.h
+//----------------------------------------------------------------------
+
+#define VKI_TCGETS	0x5401
+#define VKI_TCSETS	0x5402
+#define VKI_TCSETSW	0x5403
+#define VKI_TCSETSF	0x5404
+#define VKI_TCGETA	0x5405
+#define VKI_TCSETA	0x5406
+#define VKI_TCSETAW	0x5407
+#define VKI_TCSETAF	0x5408
+#define VKI_TCSBRK	0x5409
+#define VKI_TCXONC	0x540A
+#define VKI_TCFLSH	0x540B
+#define VKI_TIOCSCTTY	0x540E
+#define VKI_TIOCGPGRP	0x540F
+#define VKI_TIOCSPGRP	0x5410
+#define VKI_TIOCOUTQ	0x5411
+#define VKI_TIOCGWINSZ	0x5413
+#define VKI_TIOCSWINSZ	0x5414
+#define VKI_TIOCMGET	0x5415
+#define VKI_TIOCMBIS	0x5416
+#define VKI_TIOCMBIC	0x5417
+#define VKI_TIOCMSET	0x5418
+#define VKI_FIONREAD	0x541B
+#define VKI_TIOCLINUX	0x541C
+#define VKI_TIOCGSERIAL	0x541E
+#define VKI_TIOCSSERIAL	0x541F
+#define VKI_FIONBIO	0x5421
+#define VKI_TIOCNOTTY	0x5422
+#define VKI_TCSBRKP	0x5425	/* Needed for POSIX tcsendbreak() */
+#define VKI_TIOCGPTN	_VKI_IOR('T',0x30, unsigned int) /* Get Pty Number (of pty-mux device) */
+#define VKI_TIOCSPTLCK	_VKI_IOW('T',0x31, int) /* Lock/unlock Pty */
+
+#define VKI_FIONCLEX    0x5450
+#define VKI_FIOCLEX     0x5451
+#define VKI_FIOASYNC	0x5452
+#define VKI_TIOCSERGETLSR   0x5459 /* Get line status register */
+
+#define VKI_TIOCGICOUNT	0x545D	/* read serial port inline interrupt counts */
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/poll.h
+//----------------------------------------------------------------------
+
+#define VKI_POLLIN		0x0001
+
+struct vki_pollfd {
+	int fd;
+	short events;
+	short revents;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/arch/riscv/include/uapi/asm/ptrace.h
+//----------------------------------------------------------------------
+
+struct vki_user_regs_struct {
+	unsigned long pc;
+	unsigned long ra;
+	unsigned long sp;
+	unsigned long gp;
+	unsigned long tp;
+	unsigned long t0;
+	unsigned long t1;
+	unsigned long t2;
+	unsigned long s0;
+	unsigned long s1;
+	unsigned long a0;
+	unsigned long a1;
+	unsigned long a2;
+	unsigned long a3;
+	unsigned long a4;
+	unsigned long a5;
+	unsigned long a6;
+	unsigned long a7;
+	unsigned long s2;
+	unsigned long s3;
+	unsigned long s4;
+	unsigned long s5;
+	unsigned long s6;
+	unsigned long s7;
+	unsigned long s8;
+	unsigned long s9;
+	unsigned long s10;
+	unsigned long s11;
+	unsigned long t3;
+	unsigned long t4;
+	unsigned long t5;
+	unsigned long t6;
+};
+
+struct __vki_riscv_f_ext_state {
+	__vki_u32 f[32];
+	__vki_u32 fcsr;
+};
+
+struct __vki_riscv_d_ext_state {
+	__vki_u64 f[32];
+	__vki_u32 fcsr;
+};
+
+struct __vki_riscv_q_ext_state {
+	__vki_u64 f[64] __attribute__((aligned(16)));
+	__vki_u32 fcsr;
+	__vki_u32 reserved[3];
+};
+
+union __vki_riscv_fp_state {
+	struct __vki_riscv_f_ext_state f;
+	struct __vki_riscv_d_ext_state d;
+	struct __vki_riscv_q_ext_state q;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/arch/riscv/include/uapi/asm/sigcontext.h
+//----------------------------------------------------------------------
+
+struct vki_sigcontext {
+	struct vki_user_regs_struct sc_regs;
+	union __vki_riscv_fp_state sc_fpregs;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/arch/riscv/include/uapi/asm/elf.h
+//----------------------------------------------------------------------
+
+typedef unsigned long vki_elf_greg_t;
+typedef struct vki_user_regs_struct vki_elf_gregset_t;
+#define VKI_ELF_NGREG (sizeof (struct vki_elf_gregset_t) / sizeof(vki_elf_greg_t))
+
+typedef union __vki_riscv_fp_state vki_elf_fpregset_t;
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/arch/riscv/include/uapi/asm/ucontext.h
+//----------------------------------------------------------------------
+
+struct vki_ucontext {
+	unsigned long		uc_flags;
+	struct vki_ucontext	*uc_link;
+	vki_stack_t		uc_stack;
+	vki_sigset_t		uc_sigmask;
+	__vki_u8		__unused[1024 / 8 - sizeof(vki_sigset_t)];
+	struct vki_sigcontext	uc_mcontext;
+};
+
+/* TODO Get rid of this. */
+typedef char vki_modify_ldt_t;
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/ipcbuf.h
+//----------------------------------------------------------------------
+
+struct vki_ipc64_perm {
+	__vki_kernel_key_t	key;
+	__vki_kernel_uid32_t	uid;
+	__vki_kernel_gid32_t	gid;
+	__vki_kernel_uid32_t	cuid;
+	__vki_kernel_gid32_t	cgid;
+	__vki_kernel_mode_t	mode;
+        unsigned char           __pad1[4 - sizeof(__vki_kernel_mode_t)];
+	unsigned short		seq;
+	unsigned short		__pad2;
+	unsigned long		__unused1;
+	unsigned long		__unused2;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/sembuf.h
+//----------------------------------------------------------------------
+
+struct vki_semid64_ds {
+	struct vki_ipc64_perm sem_perm;		/* permissions .. see ipc.h */
+	__vki_kernel_time_t	sem_otime;		/* last semop time */
+	__vki_kernel_time_t	sem_ctime;		/* last change time */
+	unsigned long	sem_nsems;		/* no. of semaphores in array */
+	unsigned long	__unused3;
+	unsigned long	__unused4;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/msgbuf.h
+//----------------------------------------------------------------------
+
+struct vki_msqid64_ds {
+	struct vki_ipc64_perm msg_perm;
+	__vki_kernel_time_t msg_stime;	/* last msgsnd time */
+	__vki_kernel_time_t msg_rtime;	/* last msgrcv time */
+	__vki_kernel_time_t msg_ctime;	/* last change time */
+	unsigned long  msg_cbytes;	/* current number of bytes on queue */
+	unsigned long  msg_qnum;	/* number of messages in queue */
+	unsigned long  msg_qbytes;	/* max number of bytes on queue */
+	__vki_kernel_pid_t msg_lspid;	/* pid of last msgsnd */
+	__vki_kernel_pid_t msg_lrpid;	/* last receive pid */
+	unsigned long  __unused4;
+	unsigned long  __unused5;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/shmbuf.h
+//----------------------------------------------------------------------
+
+struct vki_shmid64_ds {
+	struct vki_ipc64_perm	shm_perm;	/* operation perms */
+	vki_size_t		shm_segsz;	/* size of segment (bytes) */
+	__vki_kernel_time_t	shm_atime;	/* last attach time */
+	__vki_kernel_time_t	shm_dtime;	/* last detach time */
+	__vki_kernel_time_t	shm_ctime;	/* last change time */
+	__vki_kernel_pid_t	shm_cpid;	/* pid of creator */
+	__vki_kernel_pid_t	shm_lpid;	/* pid of last operator */
+	unsigned long		shm_nattch;	/* no. of current attaches */
+	unsigned long		__unused4;
+	unsigned long		__unused5;
+};
+
+struct vki_shminfo64 {
+	unsigned long	shmmax;
+	unsigned long	shmmin;
+	unsigned long	shmmni;
+	unsigned long	shmseg;
+	unsigned long	shmall;
+	unsigned long	__unused1;
+	unsigned long	__unused2;
+	unsigned long	__unused3;
+	unsigned long	__unused4;
+};
+
+//----------------------------------------------------------------------
+// From linux-5.10.4/include/uapi/asm-generic/errno.h
+//----------------------------------------------------------------------
+
+#define	VKI_ENOSYS		38	/* Invalid system call number */
+#define	VKI_EOVERFLOW		75	/* Value too large for defined data type */
+
+#endif // __VKI_RISCV64_LINUX_H
+
+/*--------------------------------------------------------------------*/
+/*--- end                                      vki-riscv64-linux.h ---*/
+/*--------------------------------------------------------------------*/
